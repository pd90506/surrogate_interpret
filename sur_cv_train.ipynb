{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=5, bottleneck_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            shortcut_layers = []\n",
    "            shortcut_layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.ReLU())  # Using ReLU for simplicity; you can choose other activations as needed\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.ReLU())\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, hidden_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            self.layers.append(nn.Sequential(*shortcut_layers))\n",
    "\n",
    "        self.output_layer= nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x) # shortcut\n",
    "        return self.output_layer(x)\n",
    "\n",
    "def pairwise_cosine_similarity(Q, K):\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) #[N, P, L]\n",
    "    # denominator = torch.sqrt((Q**2).sum(-1).unsqueeze(-1) * (K**2).sum(-1).unsqueeze(-2))\n",
    "    denominator = (K**2).sum(-1).unsqueeze(-2)\n",
    "    attention_weights = attention_scores / (denominator + 1e-5)\n",
    "    return attention_weights\n",
    "\n",
    "class SimplifiedAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SimplifiedAttention, self).__init__()\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, q, k, v, reduce=False):\n",
    "        assert len(q.shape) in (2, 3), \"The query tensor must be 2 or 3 dimensional.\"\n",
    "        if len(q.shape) == 2:\n",
    "            # Q = self.query(q).unsqueeze(1) # [N, P, d], P = 1\n",
    "            Q = q.unsqueeze(1)\n",
    "        else:\n",
    "            # Q = self.query(q) # [N, P, d] , P: prediction length\n",
    "            Q = q\n",
    "        # K = self.key(k) # [N, L, d]\n",
    "        K = k\n",
    "        # V = self.value(v) # [N, L, d]\n",
    "        V = v\n",
    "        \n",
    "        # Compute the attention scores [N, P, L]\n",
    "        # attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))\n",
    "        # attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_weights = pairwise_cosine_similarity(Q, K)\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        # attention_weights = F.softmax(attention_scores, dim=-1) # [N, P, L]\n",
    "        # attention_weights = F.normalize(attention_scores, p=2, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values using the attention weights\n",
    "        if reduce:\n",
    "            attention_outputs = torch.matmul(attention_weights, V) # [N, P, d]\n",
    "        else:\n",
    "            # attention_outputs = torch.einsum('bij,bjk->bijk', attention_weights, V) # [N, P, L] x [N, L, d] --> [N, P, L, d]\n",
    "            # shape = attention_outputs.shape\n",
    "            attention_outputs = V.unsqueeze(1).expand(-1, Q.shape[1], -1, -1) # [N, P, L, d]\n",
    "            # attention_outputs = V # [N, L, d]\n",
    "\n",
    "        return attention_outputs, attention_weights  # Return both weights and outputs\n",
    "\n",
    "\n",
    "class SurrogateInterpretation(nn.Module):\n",
    "    def __init__(self, pred_model, classifier_head, input_embed, hidden_size) -> None:\n",
    "        \"\"\"\n",
    "        pred_model: prediction model\n",
    "        classifier_head: last fully connected layer \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.pred_model = pred_model\n",
    "\n",
    "        # Input embedding, it doesn't necessarily be an embedding per se. \n",
    "        # It used to convert the input to a form of list of token tensors.\n",
    "        self.input_embed = input_embed\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = classifier_head\n",
    "        # Transform function to non-linearly transform patch embedding to the representation space\n",
    "        self.transform_func = MLP(input_dim=hidden_size,\n",
    "                                  hidden_dim=hidden_size,\n",
    "                                  output_dim=hidden_size,\n",
    "                                  num_blocks=5,\n",
    "                                  bottleneck_dim=64)\n",
    "        self.attention = SimplifiedAttention(embed_size=hidden_size)\n",
    "\n",
    "        # freeze parameters of the prediction model.\n",
    "        if True:\n",
    "            self.freeze_params()\n",
    "        \n",
    "        self.sim_loss_func = nn.MSELoss()\n",
    "        self.cls_loss_func = nn.CrossEntropyLoss()\n",
    "        # self.cls_loss_func = nn.NLLLoss()\n",
    "        self.kl_loss = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "        self.cossim_loss_func = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    def freeze_params(self,):\n",
    "        for name, param in self.pred_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "            # print(f\"freezed {name}\")\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = False   \n",
    "        for param in self.input_embed.parameters():\n",
    "            param.requires_grad = False   \n",
    "        return \n",
    "    \n",
    "    def compute_loss(self, pred_out, pseudo_label_out, pred, pseudo_label):\n",
    "        # assert len(last_cls_hidden_state.shape) in (2, 3), \"The last hidden state should be of shape [N, L, d] or [N, d]\"\n",
    "        # if len(last_cls_hidden_state.shape) == 2:\n",
    "        #     last_cls_hidden_state = last_cls_hidden_state.unsqueeze(1) # convert to [N, 1, d]\n",
    "\n",
    "        # assert len(pseudo_label.shape) in (2, 3), \"The last hidden state should be of shape [N, L, d] or [N, d]\"\n",
    "        # if len(pseudo_label.shape) == 2:\n",
    "        #     pseudo_label = pseudo_label.unsqueeze(1) # convert to [N, 1, d]\n",
    "        \n",
    "        # sim_loss = self.sim_loss_func(pred_out, pseudo_label_out)\n",
    "        cls_loss = self.cls_loss_func(pred, pseudo_label)\n",
    "        # cls_loss = self.cls_loss_func(torch.log(pred), pseudo_label)\n",
    "        # kl_loss = self.kl_loss(F.log_softmax(pred_out, dim=-1), F.log_softmax(pseudo_label_out, dim=-1))\n",
    "        cos_sim = - self.cossim_loss_func(pred_out, pseudo_label_out).mean()\n",
    "\n",
    "        loss = cls_loss # + 1 * cos_sim\n",
    "\n",
    "        return {'loss':loss, \n",
    "                'cls_loss': cls_loss,\n",
    "                'cos_sim': cos_sim}\n",
    "        \n",
    "    \n",
    "    def forward(self, pixel_values, labels=None):\n",
    "        outputs = self.pred_model(pixel_values=pixel_values, output_hidden_states=True) \n",
    "        last_cls_hidden_state = outputs['hidden_states'][-1][:,0,:] # [N, d] the last hidden state of the cls token\n",
    "        patch_embeddings = self.input_embed(pixel_values=pixel_values) # [N, L, d]\n",
    "        \n",
    "        \n",
    "        patch_reprs = self.transform_func(patch_embeddings)\n",
    "        attention_output_split, attention_weights = self.attention(\n",
    "            last_cls_hidden_state,\n",
    "            patch_reprs,\n",
    "            patch_reprs,\n",
    "        ) # attention_weight [N, P, L], attention_output [N, P, d], \n",
    "        # attention_output = attention_output_split.sum(dim=-2) # dim=-2 for reducing the output as the sum of all interpretable features\n",
    "        attention_output = attention_weights.unsqueeze(-1) * attention_output_split # [N, P, L, d]\n",
    "        pred = self.classifier(attention_output.sum(-2))  #[N, P, out]\n",
    "        # pred_split = torch.softmax(pred_split_logit, dim=-1)\n",
    "        # pred_split = attention_weights.unsqueeze(-1) * pred_split_logit # [N, P, L, out] dim=-2 for reducing the output as the sum of all interpretable outputs\n",
    "        # pred = pred_split.mean(-2) # [N, P, out] its a normalized probability output, last dim sum to 1\n",
    "        print(\"max_pred\", pred[0][0].max())\n",
    "        print(\"min_pred\", pred[0][0].min())\n",
    "\n",
    "        pseudo_label_out = torch.softmax(self.classifier(last_cls_hidden_state),dim=-1)\n",
    "        pseudo_label = pseudo_label_out.argmax(-1)\n",
    "        pseudo_label_out = pseudo_label_out.contiguous().view(-1, pseudo_label_out.shape[-1])\n",
    "        pseudo_label = pseudo_label.contiguous().view( pseudo_label.shape[-1])\n",
    "        # print(last_cls_hidden_state.shape)\n",
    "        # print(pseudo_label)\n",
    "        # TODO fix\n",
    "        # attention_output = torch.sum(patch_reprs, dim=1, keepdim=True)\n",
    "        # TODO fix\n",
    "\n",
    "        # pred = self.classifier(attention_output) # [N, L, out]\n",
    "        # pred = torch.softmax(pred, dim=-1) # [N, L, out]\n",
    "        # pred = torch.matmul(attention_weights, pred)  # [N, P, out]\n",
    "        # pred = self.classifier(attention_output) # [N, P, out]\n",
    "\n",
    "        pred = pred.contiguous().view(-1, pred.shape[-1])\n",
    "        # print(pred.shape)\n",
    "\n",
    "        loss_dict = self.compute_loss(torch.softmax(pred, dim=-1), pseudo_label_out, pred, pseudo_label)\n",
    "        # loss_dict = self.compute_loss(attention_output.contiguous().view(-1, attention_output.shape[-1]), \n",
    "        #                               last_cls_hidden_state.contiguous().view(-1, last_cls_hidden_state.shape[-1]), \n",
    "        #                               pred, \n",
    "        #                               pseudo_label)\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        pred_labels = pred.argmax(-1).view(-1)\n",
    "        correct = (pred_labels == pseudo_label).sum()\n",
    "        accuracy = correct / len(pred_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            pred_accuracy = (pseudo_label == labels).sum() / len(labels)\n",
    "            outputs['pred_acc'] = pred_accuracy\n",
    "\n",
    "        outputs['patch_reprs'] = patch_reprs\n",
    "        outputs['attention_output'] = attention_output_split.sum(-2)\n",
    "        outputs['attention_weights'] = attention_weights\n",
    "        outputs['last_hidden_state'] = last_cls_hidden_state\n",
    "        outputs['loss'] = loss\n",
    "        outputs['cossim_loss'] = loss_dict['cos_sim']\n",
    "        outputs['cls_loss'] = loss_dict['cls_loss']\n",
    "        outputs['acc'] = accuracy\n",
    "        outputs['attention_output_split'] = attention_output_split\n",
    "        # outputs['pred_split'] = pred_split\n",
    "        outputs['pred'] = pred\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "dataset = dataset['train']\n",
    "splits = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([256, 3, 224, 224])\n",
      "labels torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=256, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_embed = pred_model.get_input_embeddings()\n",
    "# classifier_head = pred_model.classifier\n",
    "# hidden_size = config.hidden_size\n",
    "# model = SurrogateInterpretation(pred_model=pred_model, classifier_head=classifier_head, input_embed=input_embed, hidden_size=hidden_size)\n",
    "# model.to(device)\n",
    "# # outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_pred tensor(13.8239, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.7613, device='mps:0', grad_fn=<MinBackward1>)\n",
      "attention_output shape:  torch.Size([1, 1, 768])\n",
      "attention_weights shape:  torch.Size([1, 1, 196])\n",
      "last_hidden_state shape:  torch.Size([1, 768])\n",
      "patch_reprs shape:  torch.Size([1, 196, 768])\n",
      "max_pred tensor(10.1694, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.4074, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 15.852861404418945, acc: 0.296875, pred_acc: 0.7890625cossim_loss: -0.3158619999885559, cls_loss: 15.852861404418945\n",
      "max_pred tensor(130.4956, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-43.4597, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 509.9730529785156, acc: 0.0078125, pred_acc: 0.75390625cossim_loss: -0.009016415104269981, cls_loss: 509.9730529785156\n",
      "max_pred tensor(2106.9907, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-553.8331, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 986.6251220703125, acc: 0.0, pred_acc: 0.8046875cossim_loss: -1.7550009943079203e-06, cls_loss: 986.6251220703125\n",
      "max_pred tensor(406.2214, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-155.9266, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 290.8799743652344, acc: 0.00390625, pred_acc: 0.74609375cossim_loss: -0.007603500969707966, cls_loss: 290.8799743652344\n",
      "max_pred tensor(24.1880, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-27.2766, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 27.707286834716797, acc: 0.12109375, pred_acc: 0.76171875cossim_loss: -0.1329873651266098, cls_loss: 27.707286834716797\n",
      "max_pred tensor(147.6813, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-153.4105, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 97.56153869628906, acc: 0.04296875, pred_acc: 0.75cossim_loss: -0.046935997903347015, cls_loss: 97.56153869628906\n",
      "max_pred tensor(19.1726, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-28.4000, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 91.3385238647461, acc: 0.0234375, pred_acc: 0.78515625cossim_loss: -0.0273500457406044, cls_loss: 91.3385238647461\n",
      "max_pred tensor(109.6204, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-227.8250, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 61.76842498779297, acc: 0.02734375, pred_acc: 0.7890625cossim_loss: -0.027748489752411842, cls_loss: 61.76842498779297\n",
      "max_pred tensor(35.3329, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-142.3547, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 49.73036193847656, acc: 0.0546875, pred_acc: 0.7734375cossim_loss: -0.06073938310146332, cls_loss: 49.73036193847656\n",
      "max_pred tensor(104.5958, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-579.9550, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 38.81455612182617, acc: 0.05859375, pred_acc: 0.74609375cossim_loss: -0.06530582904815674, cls_loss: 38.81455612182617\n",
      "max_pred tensor(82.1631, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-481.3923, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 35.940555572509766, acc: 0.1171875, pred_acc: 0.72265625cossim_loss: -0.13228370249271393, cls_loss: 35.940555572509766\n",
      "max_pred tensor(58.0868, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-164.6252, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 41.68474197387695, acc: 0.15234375, pred_acc: 0.76171875cossim_loss: -0.169125497341156, cls_loss: 41.68474197387695\n",
      "max_pred tensor(22.5615, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-60.9503, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 41.03424072265625, acc: 0.15625, pred_acc: 0.7734375cossim_loss: -0.1754952073097229, cls_loss: 41.03424072265625\n",
      "max_pred tensor(31.1033, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-159.3538, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 34.131080627441406, acc: 0.1171875, pred_acc: 0.76953125cossim_loss: -0.1362471580505371, cls_loss: 34.131080627441406\n",
      "max_pred tensor(37.4311, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-133.8934, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 24.285776138305664, acc: 0.15625, pred_acc: 0.78125cossim_loss: -0.18498535454273224, cls_loss: 24.285776138305664\n",
      "max_pred tensor(28.6309, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-138.3597, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 19.20348358154297, acc: 0.13671875, pred_acc: 0.8046875cossim_loss: -0.15448161959648132, cls_loss: 19.20348358154297\n",
      "max_pred tensor(84.2092, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-185.7619, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 20.290603637695312, acc: 0.18359375, pred_acc: 0.75cossim_loss: -0.2137587070465088, cls_loss: 20.290603637695312\n",
      "max_pred tensor(37.4070, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-93.5824, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 14.931220054626465, acc: 0.16796875, pred_acc: 0.7890625cossim_loss: -0.19928891956806183, cls_loss: 14.931220054626465\n",
      "max_pred tensor(64.6823, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-186.8665, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 14.692874908447266, acc: 0.19140625, pred_acc: 0.78125cossim_loss: -0.21218401193618774, cls_loss: 14.692874908447266\n",
      "max_pred tensor(59.9880, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-95.5296, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 12.311287879943848, acc: 0.203125, pred_acc: 0.75cossim_loss: -0.23193059861660004, cls_loss: 12.311287879943848\n",
      "max_pred tensor(44.9683, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-40.3768, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 10.144569396972656, acc: 0.33984375, pred_acc: 0.8046875cossim_loss: -0.3711843490600586, cls_loss: 10.144569396972656\n",
      "max_pred tensor(87.2257, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-334.1446, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 8.241114616394043, acc: 0.40234375, pred_acc: 0.796875cossim_loss: -0.43963637948036194, cls_loss: 8.241114616394043\n",
      "max_pred tensor(77.2226, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.9825, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 6.61777400970459, acc: 0.4140625, pred_acc: 0.78515625cossim_loss: -0.43884050846099854, cls_loss: 6.61777400970459\n",
      "max_pred tensor(30.3539, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-125.6102, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 5.950340270996094, acc: 0.5078125, pred_acc: 0.75cossim_loss: -0.5420404076576233, cls_loss: 5.950340270996094\n",
      "max_pred tensor(29.8919, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-98.7237, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 5.034749984741211, acc: 0.5078125, pred_acc: 0.7265625cossim_loss: -0.5363159775733948, cls_loss: 5.034749984741211\n",
      "max_pred tensor(32.4846, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-55.0589, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 4.021222114562988, acc: 0.515625, pred_acc: 0.77734375cossim_loss: -0.538581907749176, cls_loss: 4.021222114562988\n",
      "max_pred tensor(12.7912, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-9.3606, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 3.9035987854003906, acc: 0.5390625, pred_acc: 0.75390625cossim_loss: -0.5709536671638489, cls_loss: 3.9035987854003906\n",
      "max_pred tensor(27.6295, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-30.2166, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 3.7687735557556152, acc: 0.6171875, pred_acc: 0.7734375cossim_loss: -0.6377564072608948, cls_loss: 3.7687735557556152\n",
      "max_pred tensor(32.2185, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-38.5526, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 3.872671127319336, acc: 0.58984375, pred_acc: 0.7578125cossim_loss: -0.6173387765884399, cls_loss: 3.872671127319336\n",
      "max_pred tensor(21.2026, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.8553, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 2.8482112884521484, acc: 0.59765625, pred_acc: 0.73828125cossim_loss: -0.6218147873878479, cls_loss: 2.8482112884521484\n",
      "max_pred tensor(24.1008, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-25.4396, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 2.451719284057617, acc: 0.63671875, pred_acc: 0.75cossim_loss: -0.6658381223678589, cls_loss: 2.451719284057617\n",
      "max_pred tensor(13.0864, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-23.8428, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.8209927082061768, acc: 0.6875, pred_acc: 0.81640625cossim_loss: -0.7213765382766724, cls_loss: 1.8209927082061768\n",
      "max_pred tensor(24.3588, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-54.9307, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.8686643838882446, acc: 0.67578125, pred_acc: 0.7734375cossim_loss: -0.7046537399291992, cls_loss: 1.8686643838882446\n",
      "max_pred tensor(36.9146, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-26.1975, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 2.300814390182495, acc: 0.6640625, pred_acc: 0.7421875cossim_loss: -0.6936166286468506, cls_loss: 2.300814390182495\n",
      "max_pred tensor(27.1840, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.0774, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.6517788171768188, acc: 0.69140625, pred_acc: 0.78515625cossim_loss: -0.7247050404548645, cls_loss: 1.6517788171768188\n",
      "max_pred tensor(25.1826, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-27.4803, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.7098362445831299, acc: 0.72265625, pred_acc: 0.78125cossim_loss: -0.7472753524780273, cls_loss: 1.7098362445831299\n",
      "max_pred tensor(63.8671, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-29.0047, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.412784457206726, acc: 0.75, pred_acc: 0.7734375cossim_loss: -0.7811051607131958, cls_loss: 1.412784457206726\n",
      "max_pred tensor(27.8969, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-30.3967, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1215472221374512, acc: 0.75, pred_acc: 0.796875cossim_loss: -0.7816466689109802, cls_loss: 1.1215472221374512\n",
      "max_pred tensor(27.5893, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.2374, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.4516733884811401, acc: 0.7421875, pred_acc: 0.7578125cossim_loss: -0.7758474349975586, cls_loss: 1.4516733884811401\n",
      "max_pred tensor(19.4620, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-23.9982, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.8552979230880737, acc: 0.68359375, pred_acc: 0.7578125cossim_loss: -0.7116825580596924, cls_loss: 1.8552979230880737\n",
      "max_pred tensor(27.1608, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-47.2451, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.2355635166168213, acc: 0.73828125, pred_acc: 0.79296875cossim_loss: -0.7588358521461487, cls_loss: 1.2355635166168213\n",
      "max_pred tensor(18.5897, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-34.5436, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.7114059925079346, acc: 0.7265625, pred_acc: 0.75cossim_loss: -0.761443018913269, cls_loss: 1.7114059925079346\n",
      "max_pred tensor(48.0757, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-30.8817, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.5534498691558838, acc: 0.71875, pred_acc: 0.76171875cossim_loss: -0.7482766509056091, cls_loss: 1.5534498691558838\n",
      "max_pred tensor(29.1602, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.7861, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.2049236297607422, acc: 0.75390625, pred_acc: 0.80078125cossim_loss: -0.793441116809845, cls_loss: 1.2049236297607422\n",
      "max_pred tensor(18.9613, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.3258, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.189335584640503, acc: 0.75, pred_acc: 0.7734375cossim_loss: -0.7798523306846619, cls_loss: 1.189335584640503\n",
      "max_pred tensor(26.3933, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-21.9970, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.0522124767303467, acc: 0.75, pred_acc: 0.75cossim_loss: -0.7851511240005493, cls_loss: 1.0522124767303467\n",
      "max_pred tensor(23.6838, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-11.2964, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1165471076965332, acc: 0.74609375, pred_acc: 0.78125cossim_loss: -0.7850834131240845, cls_loss: 1.1165471076965332\n",
      "max_pred tensor(35.3084, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-40.2952, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.6107902526855469, acc: 0.69140625, pred_acc: 0.74609375cossim_loss: -0.7365685701370239, cls_loss: 1.6107902526855469\n",
      "max_pred tensor(20.1648, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.2549, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.3219925165176392, acc: 0.72265625, pred_acc: 0.78125cossim_loss: -0.7483347058296204, cls_loss: 1.3219925165176392\n",
      "max_pred tensor(28.1347, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-54.7750, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.2840957641601562, acc: 0.7421875, pred_acc: 0.7578125cossim_loss: -0.7726236581802368, cls_loss: 1.2840957641601562\n",
      "max_pred tensor(24.1610, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.6362, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.0377219915390015, acc: 0.765625, pred_acc: 0.7734375cossim_loss: -0.7929182648658752, cls_loss: 1.0377219915390015\n",
      "max_pred tensor(34.9048, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-34.0433, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.045569658279419, acc: 0.79296875, pred_acc: 0.80078125cossim_loss: -0.8205772638320923, cls_loss: 1.045569658279419\n",
      "max_pred tensor(36.7613, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.1827, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9262828826904297, acc: 0.80078125, pred_acc: 0.7734375cossim_loss: -0.8289573192596436, cls_loss: 0.9262828826904297\n",
      "max_pred tensor(45.1086, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.7842, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1133787631988525, acc: 0.77734375, pred_acc: 0.75cossim_loss: -0.8073277473449707, cls_loss: 1.1133787631988525\n",
      "max_pred tensor(28.4290, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-27.5905, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.158172369003296, acc: 0.75390625, pred_acc: 0.82421875cossim_loss: -0.791588544845581, cls_loss: 1.158172369003296\n",
      "max_pred tensor(38.5914, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-47.3409, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1148282289505005, acc: 0.76953125, pred_acc: 0.73046875cossim_loss: -0.7982890605926514, cls_loss: 1.1148282289505005\n",
      "max_pred tensor(47.0277, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-36.1961, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7688612937927246, acc: 0.79296875, pred_acc: 0.77734375cossim_loss: -0.8177763223648071, cls_loss: 0.7688612937927246\n",
      "max_pred tensor(45.8707, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-30.4989, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9143540859222412, acc: 0.76953125, pred_acc: 0.7890625cossim_loss: -0.8107035160064697, cls_loss: 0.9143540859222412\n",
      "max_pred tensor(11.8722, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-24.0736, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.2789868116378784, acc: 0.7734375, pred_acc: 0.78515625cossim_loss: -0.8011903762817383, cls_loss: 1.2789868116378784\n",
      "max_pred tensor(17.2981, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.9149, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9386937618255615, acc: 0.78125, pred_acc: 0.8125cossim_loss: -0.8223412036895752, cls_loss: 0.9386937618255615\n",
      "max_pred tensor(22.4566, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.3602, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8767518997192383, acc: 0.7734375, pred_acc: 0.71875cossim_loss: -0.8060701489448547, cls_loss: 0.8767518997192383\n",
      "max_pred tensor(12.9815, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.1091, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9372586607933044, acc: 0.81640625, pred_acc: 0.7578125cossim_loss: -0.8342156410217285, cls_loss: 0.9372586607933044\n",
      "max_pred tensor(26.1925, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-31.2449, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.875210165977478, acc: 0.7890625, pred_acc: 0.75390625cossim_loss: -0.8189859986305237, cls_loss: 0.875210165977478\n",
      "max_pred tensor(62.7024, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-30.5389, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7185962796211243, acc: 0.828125, pred_acc: 0.796875cossim_loss: -0.8486872911453247, cls_loss: 0.7185962796211243\n",
      "max_pred tensor(31.3197, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.5419, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8365960121154785, acc: 0.828125, pred_acc: 0.7578125cossim_loss: -0.8493878245353699, cls_loss: 0.8365960121154785\n",
      "max_pred tensor(27.5269, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.8893, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8928775191307068, acc: 0.7890625, pred_acc: 0.7578125cossim_loss: -0.8021847009658813, cls_loss: 0.8928775191307068\n",
      "max_pred tensor(45.4002, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-28.7067, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.903965175151825, acc: 0.7890625, pred_acc: 0.76953125cossim_loss: -0.8237425684928894, cls_loss: 0.903965175151825\n",
      "max_pred tensor(22.1466, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.2628, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.163524866104126, acc: 0.76171875, pred_acc: 0.74609375cossim_loss: -0.7850331664085388, cls_loss: 1.163524866104126\n",
      "max_pred tensor(23.6246, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.8879, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9758892059326172, acc: 0.77734375, pred_acc: 0.7890625cossim_loss: -0.7998678684234619, cls_loss: 0.9758892059326172\n",
      "max_pred tensor(19.5354, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.0744, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8106536865234375, acc: 0.80078125, pred_acc: 0.76953125cossim_loss: -0.8326272964477539, cls_loss: 0.8106536865234375\n",
      "max_pred tensor(37.5210, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.5784, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.0372450351715088, acc: 0.76171875, pred_acc: 0.74609375cossim_loss: -0.788698673248291, cls_loss: 1.0372450351715088\n",
      "max_pred tensor(15.5534, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.1760, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.768425703048706, acc: 0.80859375, pred_acc: 0.76171875cossim_loss: -0.837334930896759, cls_loss: 0.768425703048706\n",
      "max_pred tensor(31.1186, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.7687, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9821720123291016, acc: 0.76953125, pred_acc: 0.77734375cossim_loss: -0.8004533052444458, cls_loss: 0.9821720123291016\n",
      "max_pred tensor(31.9786, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-25.1774, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9425330758094788, acc: 0.76953125, pred_acc: 0.73046875cossim_loss: -0.8095782995223999, cls_loss: 0.9425330758094788\n",
      "max_pred tensor(17.6371, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.5254, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7470208406448364, acc: 0.80859375, pred_acc: 0.78515625cossim_loss: -0.8433466553688049, cls_loss: 0.7470208406448364\n",
      "max_pred tensor(34.4263, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-29.4511, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.01133394241333, acc: 0.76171875, pred_acc: 0.74609375cossim_loss: -0.7948585152626038, cls_loss: 1.01133394241333\n",
      "max_pred tensor(26.7359, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.3941, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.0575058460235596, acc: 0.7578125, pred_acc: 0.70703125cossim_loss: -0.7915777564048767, cls_loss: 1.0575058460235596\n",
      "max_pred tensor(22.9056, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.0321, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8721098899841309, acc: 0.7890625, pred_acc: 0.75390625cossim_loss: -0.8242505788803101, cls_loss: 0.8721098899841309\n",
      "max_pred tensor(27.4865, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.9782, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.0970790386199951, acc: 0.7578125, pred_acc: 0.74609375cossim_loss: -0.7777906656265259, cls_loss: 1.0970790386199951\n",
      "max_pred tensor(14.8464, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.7440, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1595901250839233, acc: 0.75, pred_acc: 0.78125cossim_loss: -0.7789793014526367, cls_loss: 1.1595901250839233\n",
      "max_pred tensor(21.5809, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.3033, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9259375333786011, acc: 0.78515625, pred_acc: 0.72265625cossim_loss: -0.8142188787460327, cls_loss: 0.9259375333786011\n",
      "max_pred tensor(29.9077, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.1093, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1911975145339966, acc: 0.734375, pred_acc: 0.75cossim_loss: -0.7660391926765442, cls_loss: 1.1911975145339966\n",
      "max_pred tensor(39.5078, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-21.1183, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6689029932022095, acc: 0.8203125, pred_acc: 0.7421875cossim_loss: -0.850839376449585, cls_loss: 0.6689029932022095\n",
      "max_pred tensor(24.5921, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.9107, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9737832546234131, acc: 0.7890625, pred_acc: 0.765625cossim_loss: -0.8099327087402344, cls_loss: 0.9737832546234131\n",
      "max_pred tensor(27.6068, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.2784, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.009759545326233, acc: 0.75390625, pred_acc: 0.72265625cossim_loss: -0.7949444055557251, cls_loss: 1.009759545326233\n",
      "max_pred tensor(49.0100, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-34.1432, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8661283254623413, acc: 0.78125, pred_acc: 0.7734375cossim_loss: -0.8207212686538696, cls_loss: 0.8661283254623413\n",
      "max_pred tensor(19.5718, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-29.1568, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8656171560287476, acc: 0.8125, pred_acc: 0.75cossim_loss: -0.8347461223602295, cls_loss: 0.8656171560287476\n",
      "max_pred tensor(47.0701, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-27.2265, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8836023211479187, acc: 0.80078125, pred_acc: 0.765625cossim_loss: -0.825344979763031, cls_loss: 0.8836023211479187\n",
      "max_pred tensor(12.9776, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-8.6757, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7258968949317932, acc: 0.78515625, pred_acc: 0.7890625cossim_loss: -0.8294556140899658, cls_loss: 0.7258968949317932\n",
      "max_pred tensor(17.9248, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.9774, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7555537819862366, acc: 0.8203125, pred_acc: 0.73828125cossim_loss: -0.8446484804153442, cls_loss: 0.7555537819862366\n",
      "max_pred tensor(54.4324, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-35.5418, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7583498954772949, acc: 0.79296875, pred_acc: 0.7578125cossim_loss: -0.8362273573875427, cls_loss: 0.7583498954772949\n",
      "max_pred tensor(37.8514, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.6591, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7404014468193054, acc: 0.7890625, pred_acc: 0.7890625cossim_loss: -0.8245884776115417, cls_loss: 0.7404014468193054\n",
      "max_pred tensor(28.4750, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-25.0019, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7609377503395081, acc: 0.80859375, pred_acc: 0.734375cossim_loss: -0.8357469439506531, cls_loss: 0.7609377503395081\n",
      "max_pred tensor(17.4229, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.1134, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9784371852874756, acc: 0.765625, pred_acc: 0.81640625cossim_loss: -0.8045077919960022, cls_loss: 0.9784371852874756\n",
      "max_pred tensor(34.3935, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-25.3728, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1436333656311035, acc: 0.78125, pred_acc: 0.671875cossim_loss: -0.8029844760894775, cls_loss: 1.1436333656311035\n",
      "max_pred tensor(29.0595, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.1421, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8208667039871216, acc: 0.7734375, pred_acc: 0.79296875cossim_loss: -0.8180369734764099, cls_loss: 0.8208667039871216\n",
      "max_pred tensor(18.3949, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.2290, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.979559600353241, acc: 0.75, pred_acc: 0.73828125cossim_loss: -0.8012301921844482, cls_loss: 0.979559600353241\n",
      "max_pred tensor(33.0779, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.8148, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7434778213500977, acc: 0.81640625, pred_acc: 0.76953125cossim_loss: -0.8339642286300659, cls_loss: 0.7434778213500977\n",
      "max_pred tensor(33.6827, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.2453, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7952868938446045, acc: 0.8046875, pred_acc: 0.75cossim_loss: -0.832230806350708, cls_loss: 0.7952868938446045\n",
      "max_pred tensor(33.1238, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.3749, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6366684436798096, acc: 0.8359375, pred_acc: 0.796875cossim_loss: -0.8718338012695312, cls_loss: 0.6366684436798096\n",
      "max_pred tensor(15.3931, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.3100, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.0499082803726196, acc: 0.7578125, pred_acc: 0.734375cossim_loss: -0.7984501123428345, cls_loss: 1.0499082803726196\n",
      "max_pred tensor(19.8927, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.0604, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7577449083328247, acc: 0.8203125, pred_acc: 0.734375cossim_loss: -0.8404154181480408, cls_loss: 0.7577449083328247\n",
      "max_pred tensor(28.4100, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-32.9050, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9723049998283386, acc: 0.78125, pred_acc: 0.7109375cossim_loss: -0.818497896194458, cls_loss: 0.9723049998283386\n",
      "max_pred tensor(18.4272, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.4338, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9420037269592285, acc: 0.75390625, pred_acc: 0.73828125cossim_loss: -0.7947705984115601, cls_loss: 0.9420037269592285\n",
      "max_pred tensor(41.4526, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.3107, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8629096746444702, acc: 0.78125, pred_acc: 0.71875cossim_loss: -0.8248691558837891, cls_loss: 0.8629096746444702\n",
      "max_pred tensor(32.8004, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-35.3819, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7212852239608765, acc: 0.82421875, pred_acc: 0.74609375cossim_loss: -0.8535515069961548, cls_loss: 0.7212852239608765\n",
      "max_pred tensor(16.0456, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-11.2323, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8146798610687256, acc: 0.7890625, pred_acc: 0.75cossim_loss: -0.8189939856529236, cls_loss: 0.8146798610687256\n",
      "max_pred tensor(41.6409, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-28.9914, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7447021007537842, acc: 0.80859375, pred_acc: 0.76171875cossim_loss: -0.8326317071914673, cls_loss: 0.7447021007537842\n",
      "max_pred tensor(29.5408, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.2075, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6177303791046143, acc: 0.80859375, pred_acc: 0.76953125cossim_loss: -0.8518980741500854, cls_loss: 0.6177303791046143\n",
      "max_pred tensor(24.1854, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.6691, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6887103319168091, acc: 0.81640625, pred_acc: 0.75390625cossim_loss: -0.8445119857788086, cls_loss: 0.6887103319168091\n",
      "max_pred tensor(30.4950, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-20.7748, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6138653755187988, acc: 0.83984375, pred_acc: 0.73828125cossim_loss: -0.8613678812980652, cls_loss: 0.6138653755187988\n",
      "max_pred tensor(20.0106, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.8424, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.42561793327331543, acc: 0.8828125, pred_acc: 0.796875cossim_loss: -0.9048880934715271, cls_loss: 0.42561793327331543\n",
      "max_pred tensor(19.0620, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.8644, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6449902057647705, acc: 0.828125, pred_acc: 0.80078125cossim_loss: -0.8547687530517578, cls_loss: 0.6449902057647705\n",
      "max_pred tensor(25.6697, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.3392, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7475791573524475, acc: 0.7890625, pred_acc: 0.79296875cossim_loss: -0.8256213068962097, cls_loss: 0.7475791573524475\n",
      "max_pred tensor(37.6246, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-24.7642, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8409945368766785, acc: 0.81640625, pred_acc: 0.77734375cossim_loss: -0.8325519561767578, cls_loss: 0.8409945368766785\n",
      "max_pred tensor(37.3000, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.6883, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6979089975357056, acc: 0.80078125, pred_acc: 0.74609375cossim_loss: -0.8486642837524414, cls_loss: 0.6979089975357056\n",
      "max_pred tensor(16.3797, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-11.6177, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8774699568748474, acc: 0.77734375, pred_acc: 0.80078125cossim_loss: -0.8164104223251343, cls_loss: 0.8774699568748474\n",
      "max_pred tensor(34.0313, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-23.0891, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6818139553070068, acc: 0.8359375, pred_acc: 0.74609375cossim_loss: -0.858187198638916, cls_loss: 0.6818139553070068\n",
      "max_pred tensor(30.6460, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.6834, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.76199871301651, acc: 0.78125, pred_acc: 0.77734375cossim_loss: -0.8132032752037048, cls_loss: 0.76199871301651\n",
      "max_pred tensor(22.5347, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.6759, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8673135638237, acc: 0.7578125, pred_acc: 0.74609375cossim_loss: -0.7950816750526428, cls_loss: 0.8673135638237\n",
      "max_pred tensor(32.5292, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.9403, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7777868509292603, acc: 0.8046875, pred_acc: 0.79296875cossim_loss: -0.8425988554954529, cls_loss: 0.7777868509292603\n",
      "max_pred tensor(31.0973, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.0045, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6688517332077026, acc: 0.8125, pred_acc: 0.73828125cossim_loss: -0.8507608771324158, cls_loss: 0.6688517332077026\n",
      "max_pred tensor(39.1930, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.2142, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7788028717041016, acc: 0.79296875, pred_acc: 0.7265625cossim_loss: -0.831538200378418, cls_loss: 0.7788028717041016\n",
      "max_pred tensor(47.5997, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-25.9914, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 1.1375112533569336, acc: 0.73046875, pred_acc: 0.71875cossim_loss: -0.7674064636230469, cls_loss: 1.1375112533569336\n",
      "max_pred tensor(58.8165, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-28.7120, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6352176666259766, acc: 0.81640625, pred_acc: 0.8046875cossim_loss: -0.8490821123123169, cls_loss: 0.6352176666259766\n",
      "max_pred tensor(16.5085, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.6978, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7843843698501587, acc: 0.78515625, pred_acc: 0.74609375cossim_loss: -0.826922595500946, cls_loss: 0.7843843698501587\n",
      "max_pred tensor(53.5875, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-46.1393, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6806678175926208, acc: 0.81640625, pred_acc: 0.8359375cossim_loss: -0.8468419909477234, cls_loss: 0.6806678175926208\n",
      "max_pred tensor(36.5321, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-11.6863, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7317856550216675, acc: 0.82421875, pred_acc: 0.76953125cossim_loss: -0.8479096293449402, cls_loss: 0.7317856550216675\n",
      "max_pred tensor(42.3111, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.9846, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7434234619140625, acc: 0.796875, pred_acc: 0.75cossim_loss: -0.83716881275177, cls_loss: 0.7434234619140625\n",
      "max_pred tensor(39.5204, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-18.1784, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.9676980972290039, acc: 0.7421875, pred_acc: 0.7421875cossim_loss: -0.7860335111618042, cls_loss: 0.9676980972290039\n",
      "max_pred tensor(20.3153, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.2367, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6543036699295044, acc: 0.83984375, pred_acc: 0.79296875cossim_loss: -0.8571516275405884, cls_loss: 0.6543036699295044\n",
      "max_pred tensor(35.8102, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.1318, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6850831508636475, acc: 0.8359375, pred_acc: 0.78125cossim_loss: -0.8581973314285278, cls_loss: 0.6850831508636475\n",
      "max_pred tensor(64.8643, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-40.1076, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7727271318435669, acc: 0.8203125, pred_acc: 0.7890625cossim_loss: -0.8520060777664185, cls_loss: 0.7727271318435669\n",
      "max_pred tensor(36.6844, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.8803, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6182726621627808, acc: 0.82421875, pred_acc: 0.78125cossim_loss: -0.8551051616668701, cls_loss: 0.6182726621627808\n",
      "max_pred tensor(34.2513, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.2034, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6653937101364136, acc: 0.8125, pred_acc: 0.7734375cossim_loss: -0.8487461805343628, cls_loss: 0.6653937101364136\n",
      "max_pred tensor(28.0406, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.8113, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6503714323043823, acc: 0.8203125, pred_acc: 0.76953125cossim_loss: -0.8543066382408142, cls_loss: 0.6503714323043823\n",
      "max_pred tensor(22.4462, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.7114, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7479231357574463, acc: 0.8203125, pred_acc: 0.7421875cossim_loss: -0.8449468612670898, cls_loss: 0.7479231357574463\n",
      "max_pred tensor(14.3777, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.5090, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5772076845169067, acc: 0.84765625, pred_acc: 0.77734375cossim_loss: -0.8724671006202698, cls_loss: 0.5772076845169067\n",
      "max_pred tensor(12.1706, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-11.8868, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8165572881698608, acc: 0.796875, pred_acc: 0.78515625cossim_loss: -0.8394896388053894, cls_loss: 0.8165572881698608\n",
      "max_pred tensor(18.6726, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-8.2459, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5848150849342346, acc: 0.8203125, pred_acc: 0.79296875cossim_loss: -0.8594449758529663, cls_loss: 0.5848150849342346\n",
      "max_pred tensor(34.2990, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.6122, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8442370891571045, acc: 0.81640625, pred_acc: 0.734375cossim_loss: -0.8428257703781128, cls_loss: 0.8442370891571045\n",
      "max_pred tensor(29.0084, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-19.3630, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.675525426864624, acc: 0.81640625, pred_acc: 0.7890625cossim_loss: -0.8558638095855713, cls_loss: 0.675525426864624\n",
      "max_pred tensor(28.1884, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.4083, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7349700927734375, acc: 0.8046875, pred_acc: 0.7890625cossim_loss: -0.8371663093566895, cls_loss: 0.7349700927734375\n",
      "max_pred tensor(36.0550, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-21.7295, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8613936901092529, acc: 0.80859375, pred_acc: 0.796875cossim_loss: -0.8389855623245239, cls_loss: 0.8613936901092529\n",
      "max_pred tensor(23.7370, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.5316, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6105279326438904, acc: 0.8125, pred_acc: 0.78515625cossim_loss: -0.8493236303329468, cls_loss: 0.6105279326438904\n",
      "max_pred tensor(24.3583, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.6971, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5410181283950806, acc: 0.84375, pred_acc: 0.74609375cossim_loss: -0.8772662878036499, cls_loss: 0.5410181283950806\n",
      "max_pred tensor(12.7922, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-11.3028, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.774562656879425, acc: 0.78515625, pred_acc: 0.7890625cossim_loss: -0.8125267624855042, cls_loss: 0.774562656879425\n",
      "max_pred tensor(22.4825, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.7829, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7421765327453613, acc: 0.79296875, pred_acc: 0.765625cossim_loss: -0.8259780406951904, cls_loss: 0.7421765327453613\n",
      "max_pred tensor(19.0246, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.1227, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7168148756027222, acc: 0.80859375, pred_acc: 0.80859375cossim_loss: -0.8369641900062561, cls_loss: 0.7168148756027222\n",
      "max_pred tensor(11.3925, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-10.7414, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8156266212463379, acc: 0.7890625, pred_acc: 0.734375cossim_loss: -0.8211420178413391, cls_loss: 0.8156266212463379\n",
      "max_pred tensor(30.2120, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.8223, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7395845651626587, acc: 0.796875, pred_acc: 0.75390625cossim_loss: -0.8287237882614136, cls_loss: 0.7395845651626587\n",
      "max_pred tensor(39.3241, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.9211, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8942559957504272, acc: 0.79296875, pred_acc: 0.75390625cossim_loss: -0.8130759000778198, cls_loss: 0.8942559957504272\n",
      "max_pred tensor(28.0516, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.0811, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6237165927886963, acc: 0.81640625, pred_acc: 0.78515625cossim_loss: -0.8527455925941467, cls_loss: 0.6237165927886963\n",
      "max_pred tensor(38.1080, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-22.7876, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7617536783218384, acc: 0.83984375, pred_acc: 0.7578125cossim_loss: -0.847842812538147, cls_loss: 0.7617536783218384\n",
      "max_pred tensor(23.0006, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.1147, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8262985348701477, acc: 0.78125, pred_acc: 0.74609375cossim_loss: -0.8185508847236633, cls_loss: 0.8262985348701477\n",
      "max_pred tensor(37.8810, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-18.0561, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5867226123809814, acc: 0.828125, pred_acc: 0.76171875cossim_loss: -0.8600375652313232, cls_loss: 0.5867226123809814\n",
      "max_pred tensor(24.1967, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.3743, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8191187381744385, acc: 0.796875, pred_acc: 0.7421875cossim_loss: -0.8232084512710571, cls_loss: 0.8191187381744385\n",
      "max_pred tensor(28.5887, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-15.2397, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.86715167760849, acc: 0.78125, pred_acc: 0.7578125cossim_loss: -0.8139030933380127, cls_loss: 0.86715167760849\n",
      "max_pred tensor(32.0927, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-14.4398, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5797667503356934, acc: 0.83984375, pred_acc: 0.78515625cossim_loss: -0.865720272064209, cls_loss: 0.5797667503356934\n",
      "max_pred tensor(37.1759, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-21.9806, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7487671375274658, acc: 0.80078125, pred_acc: 0.796875cossim_loss: -0.8334112167358398, cls_loss: 0.7487671375274658\n",
      "max_pred tensor(16.7017, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.4165, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6742472648620605, acc: 0.81640625, pred_acc: 0.78125cossim_loss: -0.8471372127532959, cls_loss: 0.6742472648620605\n",
      "max_pred tensor(32.3344, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-24.3087, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6895533800125122, acc: 0.81640625, pred_acc: 0.7578125cossim_loss: -0.846869170665741, cls_loss: 0.6895533800125122\n",
      "max_pred tensor(28.6405, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.0254, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8705555200576782, acc: 0.796875, pred_acc: 0.73828125cossim_loss: -0.8279380798339844, cls_loss: 0.8705555200576782\n",
      "max_pred tensor(19.1022, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-12.2979, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6494030952453613, acc: 0.83203125, pred_acc: 0.7890625cossim_loss: -0.8579865097999573, cls_loss: 0.6494030952453613\n",
      "max_pred tensor(35.9309, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-25.8630, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5974241495132446, acc: 0.8203125, pred_acc: 0.77734375cossim_loss: -0.8600102663040161, cls_loss: 0.5974241495132446\n",
      "max_pred tensor(20.4294, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-16.1848, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6361408829689026, acc: 0.83203125, pred_acc: 0.78515625cossim_loss: -0.8619146943092346, cls_loss: 0.6361408829689026\n",
      "max_pred tensor(33.7341, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-23.4697, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.8766583204269409, acc: 0.75, pred_acc: 0.69921875cossim_loss: -0.7999491095542908, cls_loss: 0.8766583204269409\n",
      "max_pred tensor(34.6916, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-18.1022, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.6643951535224915, acc: 0.8125, pred_acc: 0.796875cossim_loss: -0.8549684286117554, cls_loss: 0.6643951535224915\n",
      "max_pred tensor(26.5084, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-13.5232, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.7652450799942017, acc: 0.8125, pred_acc: 0.75390625cossim_loss: -0.8309006094932556, cls_loss: 0.7652450799942017\n",
      "max_pred tensor(17.2512, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-18.0182, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.726162314414978, acc: 0.8125, pred_acc: 0.7421875cossim_loss: -0.8485127687454224, cls_loss: 0.726162314414978\n",
      "max_pred tensor(43.3304, device='mps:0', grad_fn=<MaxBackward1>)\n",
      "min_pred tensor(-17.8799, device='mps:0', grad_fn=<MinBackward1>)\n",
      "loss: 0.5523342490196228, acc: 0.859375, pred_acc: 0.8203125cossim_loss: -0.8828401565551758, cls_loss: 0.5523342490196228\n",
      "max_pred "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[233], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 19\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[227], line 158\u001b[0m, in \u001b[0;36mSurrogateInterpretation.forward\u001b[0;34m(self, pixel_values, labels)\u001b[0m\n\u001b[1;32m    154\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(attention_output\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m#[N, P, out]\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# pred_split = torch.softmax(pred_split_logit, dim=-1)\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# pred_split = attention_weights.unsqueeze(-1) * pred_split_logit # [N, P, L, out] dim=-2 for reducing the output as the sum of all interpretable outputs\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# pred = pred_split.mean(-2) # [N, P, out] its a normalized probability output, last dim sum to 1\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmax())\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmin())\n\u001b[1;32m    161\u001b[0m pseudo_label_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(last_cls_hidden_state),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/_tensor.py:461\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    458\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    459\u001b[0m     )\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/_tensor_str.py:677\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    676\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/_tensor_str.py:597\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    595\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    596\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 597\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    600\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/_tensor_str.py:349\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    347\u001b[0m     )\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 349\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    350\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/torch/_tensor_str.py:137\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     nonzero_finite_vals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_select\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfinite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtensor_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mne\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nonzero_finite_vals\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# no valid number, do nothing\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_embed = pred_model.get_input_embeddings()\n",
    "classifier_head = pred_model.classifier\n",
    "hidden_size = config.hidden_size\n",
    "model = SurrogateInterpretation(pred_model=pred_model, classifier_head=classifier_head, input_embed=input_embed, hidden_size=hidden_size)\n",
    "model.to(device)\n",
    "outputs = model(**inputs)\n",
    "print(\"attention_output shape: \", outputs['attention_output'].shape)\n",
    "print(\"attention_weights shape: \", outputs['attention_weights'].shape)\n",
    "print(\"last_hidden_state shape: \", outputs['last_hidden_state'].shape)\n",
    "print(\"patch_reprs shape: \", outputs['patch_reprs'].shape)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        label = data['labels'].to(device)\n",
    "        outputs = model(pixel_values, label)\n",
    "        loss = outputs['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss.item()}, acc: {outputs['acc'].item()}, pred_acc: {outputs['pred_acc'].item()}\"\n",
    "              f\"cossim_loss: {outputs['cossim_loss'].item()}, cls_loss: {outputs['cls_loss'].item()}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], device='mps:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pred[24][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad == True:\n",
    "#         print(name, param.requires_grad)\n",
    "model_name = 'vit_sur_loss=cls+cos_att=cossim2.pt'\n",
    "torch.save(model.state_dict(), f'model/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_output shape:  tensor([[[ -2.4139,   1.2525,  -5.8068,  ...,  -1.3713,   3.6165,  -0.0333]],\n",
      "\n",
      "        [[  0.8080,  -2.6431,  -1.7622,  ...,   7.8847,  -0.8463,  10.1360]],\n",
      "\n",
      "        [[  3.2632,  -9.8401,   4.9660,  ..., -11.6845,  -3.7304,  -4.1211]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.3884,  -4.7305,  -2.3395,  ...,  -4.6104,  -0.1576,   2.3779]],\n",
      "\n",
      "        [[  6.6678,  -0.7116,   0.7123,  ...,  -1.5447,  -0.6401,  -5.3296]],\n",
      "\n",
      "        [[ -0.0722,  -1.1478,   5.5667,  ...,   2.7927,  -1.6272,  -2.9185]]],\n",
      "       device='mps:0', grad_fn=<UnsafeViewBackward0>)\n",
      "attention_weights: tensor([[[ 0.0062, -0.0446, -0.0248,  ..., -0.0851, -0.0127, -0.0037]],\n",
      "\n",
      "        [[-0.0486,  0.0095, -0.0574,  ..., -0.0123, -0.0571, -0.0509]],\n",
      "\n",
      "        [[ 0.0132, -0.0574,  0.0335,  ..., -0.0291, -0.0285,  0.0066]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0173,  0.0006, -0.0141,  ..., -0.0051, -0.0056, -0.1184]],\n",
      "\n",
      "        [[-0.0358,  0.0102, -0.0101,  ...,  0.0503, -0.0032,  0.0283]],\n",
      "\n",
      "        [[-0.0095,  0.0219,  0.0039,  ..., -0.0193, -0.0322,  0.0029]]],\n",
      "       device='mps:0', grad_fn=<DivBackward0>)\n",
      "last_hidden_state shape:  tensor([[ -6.5898,  11.2904,  -6.6615,  ...,   1.2074,  -0.7898,  -3.9372],\n",
      "        [ -1.4653,  -5.8697,   9.1209,  ...,  -5.6890,   0.4717,  14.9723],\n",
      "        [  2.6811,  -1.9956,   5.2226,  ..., -10.0995,  10.2021,  12.7760],\n",
      "        ...,\n",
      "        [  0.2299,  -8.5136,   1.1077,  ...,  -9.6072,  -4.1624,  -0.3767],\n",
      "        [  5.3960,  -2.6165,  -6.0973,  ...,   1.1822, -14.7051,   7.1953],\n",
      "        [ -0.4192,  -1.4929,   2.8651,  ...,  -0.9941,  -5.1000,  -2.9890]],\n",
      "       device='mps:0')\n",
      "pred:  tensor([295, 886, 597, 385,  10, 182, 801, 967, 636, 388, 252, 327, 406, 119,\n",
      "        621, 878, 960, 824, 926, 202,  11, 746, 250, 990,  39, 426, 572, 813,\n",
      "        747, 824,  91, 643, 890, 697, 753,  83, 179,  76, 354,  97, 761, 349,\n",
      "        731, 466, 118, 858, 796, 721, 900, 194, 607, 188, 750, 522, 135, 510,\n",
      "        169, 397, 648, 997, 950,  44, 258, 210,  27, 895,  93, 745, 232,  87,\n",
      "        904, 940, 195, 338,  41, 315, 586, 907,  64, 802, 624, 267,  96, 847,\n",
      "        831, 855, 243, 398,  28, 197, 435, 317, 299, 466, 831,  95, 326, 642,\n",
      "        898,  86, 159, 562, 306, 728, 941, 546, 488, 311, 391, 784, 203, 602,\n",
      "        128, 313, 898, 632, 387, 326, 650, 530,  99, 957, 533, 813, 416,  26,\n",
      "        206, 951, 336, 128, 864,  13, 334, 848, 823, 277, 322, 535, 692, 723,\n",
      "        693, 698, 693, 894, 754, 156, 201, 141, 997, 594, 479, 398, 646, 537,\n",
      "        871, 372, 236, 912, 254,  81, 767, 699, 166, 476,  94, 669, 594, 238,\n",
      "        678, 364, 880, 423, 761, 775,  84, 259, 639, 269, 725, 473, 995, 468,\n",
      "        332, 565, 239, 980, 180, 868, 167, 897, 150, 500, 580,  57, 661, 900,\n",
      "        722, 564, 214, 312], device='mps:0')\n",
      "labels:  tensor([295, 886, 597, 386,  10, 182, 801, 967, 414, 388, 196, 327, 677, 119,\n",
      "        621, 878, 415, 824, 926, 154,  11, 746, 250, 990,  39, 426, 572, 618,\n",
      "        747, 474,  91, 643, 890, 697, 753,  83, 179,  76, 354,  97, 761, 349,\n",
      "        731, 466, 119, 858, 796, 721, 900, 194, 607, 188, 750, 522, 135, 510,\n",
      "        169, 397, 648, 997, 950,  44, 258, 210,  27, 895,  93, 745, 852,  87,\n",
      "        799, 941, 253, 338,  46, 315, 847, 892,  64, 802, 454, 267, 382, 847,\n",
      "        831, 855, 243, 443,  28, 197, 435, 317, 299, 895, 831,  95, 326, 722,\n",
      "        653,  86, 159, 682, 306, 728, 941, 546, 488, 311, 391, 784, 203, 602,\n",
      "        128, 313, 898, 632, 387, 326, 650, 811,  99, 957, 533, 813, 416,  25,\n",
      "        206, 951, 336, 128, 864,  13, 334, 848, 823, 287, 322, 535, 737, 723,\n",
      "        693, 538, 693, 493, 485, 156, 201, 141, 997, 793, 817, 398, 646, 537,\n",
      "        871, 372, 236, 912, 254, 187, 767, 699, 167, 476, 127, 669, 594, 240,\n",
      "        515, 364, 880, 423, 761, 830,  84, 259, 638, 269, 505, 473, 995, 468,\n",
      "        647, 565, 239, 980, 179, 868, 167, 897, 703, 500, 580,  57, 661, 270,\n",
      "        722, 564, 156, 312])\n",
      "interp:  tensor([295, 886, 597, 385,  10, 182, 639, 967, 748, 388, 252, 327, 406, 119,\n",
      "        880, 878, 960, 824, 926, 204,  11, 746, 250, 990,  39, 426, 572, 813,\n",
      "        747, 735,  91, 643, 890, 697, 753,  83, 179,  76, 354,  97, 761, 350,\n",
      "        731, 466, 121, 858, 796, 721, 900, 194, 607, 188, 750, 522, 135, 510,\n",
      "        169, 397, 648, 997, 950,  44, 258, 210,  27, 895,  93, 745, 232,  87,\n",
      "        904, 940, 253, 338,  41, 315, 586, 420,  64, 802, 727, 267, 364, 847,\n",
      "        831, 855, 243, 747,  28, 197, 435, 317, 299, 705, 831,  95, 326, 420,\n",
      "        898,  86, 159, 562, 306, 728, 941, 546, 488, 311, 391, 784, 203, 702,\n",
      "        128, 313, 898, 409, 387, 326, 513, 530,  99, 957, 533, 813, 416,  35,\n",
      "        206, 951, 336, 128, 864,  13, 334, 848, 823, 271, 322, 535, 692, 723,\n",
      "        693, 743, 693, 894, 754, 156, 201, 141, 997, 594, 817, 398, 646, 537,\n",
      "        871, 372, 236, 912, 254,  81, 767, 699, 167, 476,  22, 669, 594, 238,\n",
      "        515, 364, 880, 423, 761, 462,  84, 231, 639, 269, 773, 473, 995, 468,\n",
      "        332, 565, 239, 980, 242, 659, 167, 897, 150, 500, 580,  57, 661, 682,\n",
      "        830, 564, 214, 312], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"attention_output shape: \", outputs['attention_output'])\n",
    "print(\"attention_weights:\", outputs['attention_weights'])\n",
    "print(\"last_hidden_state shape: \", outputs['last_hidden_state'])\n",
    "logit = model.classifier(outputs['last_hidden_state'])\n",
    "pred = logit.argmax(-1)\n",
    "print('pred: ', pred)\n",
    "int_logit = model.classifier(outputs['attention_output'])\n",
    "interp = int_logit.argmax(-1)\n",
    "print('labels: ', data['labels'])\n",
    "print('interp: ', interp.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0062, -0.0446, -0.0248,  ..., -0.0851, -0.0127, -0.0037]],\n",
       "\n",
       "        [[-0.0486,  0.0095, -0.0574,  ..., -0.0123, -0.0571, -0.0509]],\n",
       "\n",
       "        [[ 0.0132, -0.0574,  0.0335,  ..., -0.0291, -0.0285,  0.0066]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0173,  0.0006, -0.0141,  ..., -0.0051, -0.0056, -0.1184]],\n",
       "\n",
       "        [[-0.0358,  0.0102, -0.0101,  ...,  0.0503, -0.0032,  0.0283]],\n",
       "\n",
       "        [[-0.0095,  0.0219,  0.0039,  ..., -0.0193, -0.0322,  0.0029]]],\n",
       "       device='mps:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['attention_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"sur_model01\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 500/13500 [01:45<43:35,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0818, 'grad_norm': 0.0, 'learning_rate': 1.925925925925926e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1000/13500 [03:28<45:42,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0758, 'grad_norm': 0.0, 'learning_rate': 1.851851851851852e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1500/13500 [05:12<42:13,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0668, 'grad_norm': 0.0, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2000/13500 [06:57<40:17,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0879, 'grad_norm': 0.0, 'learning_rate': 1.7037037037037038e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 2500/13500 [08:41<37:57,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0409, 'grad_norm': 0.0, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 3000/13500 [10:25<36:15,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0952, 'grad_norm': 0.0, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 3501/13500 [12:10<33:33,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.1246, 'grad_norm': 0.0, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 4000/13500 [13:54<32:42,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.038, 'grad_norm': 0.0, 'learning_rate': 1.4074074074074075e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4500/13500 [15:38<29:02,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0617, 'grad_norm': 0.0, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 35.93 GB, other allocations: 2.95 MB, max allowed: 36.27 GB). Tried to allocate 882.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:2049\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2049\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2053\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:2412\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2410\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2412\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:3444\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3442\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3443\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m-> 3444\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3447\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:125\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    128\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:84\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     81\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     87\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 35.93 GB, other allocations: 2.95 MB, max allowed: 36.27 GB). Tried to allocate 882.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=dataset['train'][:512]['image'], return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][:512]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
