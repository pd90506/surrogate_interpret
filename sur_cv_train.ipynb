{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/panda/anaconda3/envs/ts/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=5, bottleneck_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            shortcut_layers = []\n",
    "            shortcut_layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.ReLU())  # Using ReLU for simplicity; you can choose other activations as needed\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.ReLU())\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, hidden_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            self.layers.append(nn.Sequential(*shortcut_layers))\n",
    "\n",
    "        self.output_layer= nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.layers:\n",
    "            x = x + layer(x) # shortcut\n",
    "        return self.output_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_cosine_similarity(Q, K):\n",
    "    attention_scores = torch.matmul(Q, K.transpose(-2, -1)) #[N, P, L]\n",
    "    # denominator = torch.sqrt((Q**2).sum(-1).unsqueeze(-1) * (K**2).sum(-1).unsqueeze(-2))\n",
    "    denominator = (K**2).sum(-1).unsqueeze(-2)\n",
    "    attention_weights = attention_scores / (denominator + 1e-5)\n",
    "    return attention_weights\n",
    "\n",
    "class SimplifiedAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SimplifiedAttention, self).__init__()\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        self.value = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, q, k, v, reduce=True):\n",
    "        assert len(q.shape) in (2, 3), \"The query tensor must be 2 or 3 dimensional.\"\n",
    "        if len(q.shape) == 2:\n",
    "            # Q = self.query(q).unsqueeze(1) # [N, P, d], P = 1\n",
    "            Q = q.unsqueeze(1)\n",
    "        else:\n",
    "            # Q = self.query(q) # [N, P, d] , P: prediction length\n",
    "            Q = q\n",
    "        # K = self.key(k) # [N, L, d]\n",
    "        K = k\n",
    "        # V = self.value(v) # [N, L, d]\n",
    "        V = v\n",
    "        \n",
    "        # Compute the attention scores [N, P, L]\n",
    "        # attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))\n",
    "        # attention_scores = torch.matmul(Q, K.transpose(-2, -1))\n",
    "        attention_weights = pairwise_cosine_similarity(Q, K)\n",
    "        \n",
    "        # Apply softmax to get the attention weights\n",
    "        # attention_weights = F.softmax(attention_scores, dim=-1) # [N, P, L]\n",
    "        # attention_weights = F.normalize(attention_scores, p=2, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values using the attention weights\n",
    "        if reduce:\n",
    "            attention_outputs = torch.matmul(attention_weights, V) # [N, P, d]\n",
    "        else:\n",
    "            attention_outputs = torch.einsum('bij,bjk->bijk', attention_weights, V) # [N, P, L] x [N, L, d] --> [N, P, L, d]\n",
    "            # attention_outputs = V # [N, L, d]\n",
    "\n",
    "        return attention_outputs, attention_weights  # Return both weights and outputs\n",
    "\n",
    "# import collections\n",
    "# class SurrogatePatchEmbeddings(nn.Module):\n",
    "#     \"\"\"\n",
    "#     This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into\n",
    "#     patches of shape `(batch_size, seq_length, patch_height, patch_width)` to be consumed by a\n",
    "#     surrogate model.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__()\n",
    "#         image_size, patch_size = config.image_size, config.patch_size\n",
    "#         num_channels, hidden_size = config.num_channels, config.hidden_size\n",
    "\n",
    "#         image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n",
    "#         patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n",
    "#         num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n",
    "#         self.image_size = image_size\n",
    "#         self.patch_size = patch_size\n",
    "#         self.num_channels = num_channels\n",
    "#         self.num_patches = num_patches\n",
    "\n",
    "#         self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "#     def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n",
    "#         batch_size, num_channels, height, width = pixel_values.shape\n",
    "#         if num_channels != self.num_channels:\n",
    "#             raise ValueError(\n",
    "#                 \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n",
    "#                 f\" Expected {self.num_channels} but got {num_channels}.\"\n",
    "#             )\n",
    "#         if not interpolate_pos_encoding:\n",
    "#             if height != self.image_size[0] or width != self.image_size[1]:\n",
    "#                 raise ValueError(\n",
    "#                     f\"Input image size ({height}*{width}) doesn't match model\"\n",
    "#                     f\" ({self.image_size[0]}*{self.image_size[1]}).\"\n",
    "#                 )\n",
    "#         embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n",
    "#         return embeddings\n",
    "\n",
    "class SurrogateInterpretation(nn.Module):\n",
    "    def __init__(self, pred_model, classifier_head, input_embed, hidden_size) -> None:\n",
    "        \"\"\"\n",
    "        pred_model: prediction model\n",
    "        classifier_head: last fully connected layer \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pred_model = pred_model\n",
    "\n",
    "        # Input embedding, it doesn't necessarily be an embedding per se. \n",
    "        # It used to convert the input to a form of list of token tensors.\n",
    "        self.input_embed = input_embed\n",
    "\n",
    "        # Classifier head\n",
    "        self.classifier = classifier_head\n",
    "        # Transform function to non-linearly transform patch embedding to the representation space\n",
    "        self.transform_func = MLP(input_dim=hidden_size,\n",
    "                                  hidden_dim=hidden_size,\n",
    "                                  output_dim=hidden_size,\n",
    "                                  num_blocks=5,\n",
    "                                  bottleneck_dim=64)\n",
    "        self.attention = SimplifiedAttention(embed_size=hidden_size)\n",
    "\n",
    "        # freeze parameters of the prediction model.\n",
    "        if True:\n",
    "            self.freeze_params()\n",
    "        \n",
    "        self.sim_loss_func = nn.MSELoss()\n",
    "        self.cls_loss_func = nn.CrossEntropyLoss()\n",
    "        self.kl_loss = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "        self.cossim_loss_func = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    def freeze_params(self,):\n",
    "        for name, param in self.pred_model.named_parameters():\n",
    "            param.requires_grad = False \n",
    "            # print(f\"freezed {name}\")\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = False   \n",
    "        for param in self.input_embed.parameters():\n",
    "            param.requires_grad = False   \n",
    "        return \n",
    "    \n",
    "    def compute_loss(self, pred_out, pseudo_label_out, pred, pseudo_label):\n",
    "        # assert len(last_cls_hidden_state.shape) in (2, 3), \"The last hidden state should be of shape [N, L, d] or [N, d]\"\n",
    "        # if len(last_cls_hidden_state.shape) == 2:\n",
    "        #     last_cls_hidden_state = last_cls_hidden_state.unsqueeze(1) # convert to [N, 1, d]\n",
    "\n",
    "        # assert len(pseudo_label.shape) in (2, 3), \"The last hidden state should be of shape [N, L, d] or [N, d]\"\n",
    "        # if len(pseudo_label.shape) == 2:\n",
    "        #     pseudo_label = pseudo_label.unsqueeze(1) # convert to [N, 1, d]\n",
    "        \n",
    "        # sim_loss = self.sim_loss_func(pred_out, pseudo_label_out)\n",
    "        cls_loss = self.cls_loss_func(pred, pseudo_label)\n",
    "        # kl_loss = self.kl_loss(F.log_softmax(pred_out, dim=-1), F.log_softmax(pseudo_label_out, dim=-1))\n",
    "        cos_sim = - self.cossim_loss_func(pred_out, pseudo_label_out).mean()\n",
    "\n",
    "        loss = cls_loss + 1 * cos_sim\n",
    "\n",
    "        return {'loss':loss, \n",
    "                'cls_loss': cls_loss,\n",
    "                'cos_sim': cos_sim}\n",
    "        \n",
    "    \n",
    "    def forward(self, pixel_values, labels=None, reduce_attention=True):\n",
    "        outputs = self.pred_model(pixel_values=pixel_values, output_hidden_states=True) \n",
    "        last_cls_hidden_state = outputs['hidden_states'][-1][:,0,:] # [N, d] the last hidden state of the cls token\n",
    "        patch_embeddings = self.input_embed(pixel_values=pixel_values) # [N, L, d]\n",
    "        \n",
    "        \n",
    "        patch_reprs = self.transform_func(patch_embeddings)\n",
    "        attention_output, attention_weights = self.attention(\n",
    "            last_cls_hidden_state,\n",
    "            patch_reprs,\n",
    "            patch_reprs,\n",
    "            reduce=reduce_attention\n",
    "        ) # attention_weight [N, P, L], attention_output [N, P, d], \n",
    "\n",
    "        pseudo_label_out = self.classifier(last_cls_hidden_state)\n",
    "        pseudo_label = pseudo_label_out.argmax(-1)\n",
    "        pseudo_label_out = pseudo_label_out.contiguous().view(-1, pseudo_label_out.shape[-1])\n",
    "        pseudo_label = pseudo_label.contiguous().view( pseudo_label.shape[-1])\n",
    "        # print(last_cls_hidden_state.shape)\n",
    "        # print(pseudo_label)\n",
    "        # TODO fix\n",
    "        # attention_output = torch.sum(patch_reprs, dim=1, keepdim=True)\n",
    "        # TODO fix\n",
    "\n",
    "        # pred = self.classifier(attention_output) # [N, L, out]\n",
    "        # pred = torch.softmax(pred, dim=-1) # [N, L, out]\n",
    "        # pred = torch.matmul(attention_weights, pred)  # [N, P, out]\n",
    "        pred = self.classifier(attention_output) # [N, P, out]\n",
    "\n",
    "        pred = pred.contiguous().view(-1, pred.shape[-1])\n",
    "        # print(pred.shape)\n",
    "\n",
    "        loss_dict = self.compute_loss(pred, pseudo_label_out, pred, pseudo_label)\n",
    "        # loss_dict = self.compute_loss(attention_output.contiguous().view(-1, attention_output.shape[-1]), \n",
    "        #                               last_cls_hidden_state.contiguous().view(-1, last_cls_hidden_state.shape[-1]), \n",
    "        #                               pred, \n",
    "        #                               pseudo_label)\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        pred_labels = pred.argmax(-1).view(-1)\n",
    "        correct = (pred_labels == pseudo_label).sum()\n",
    "        accuracy = correct / len(pred_labels)\n",
    "\n",
    "        if labels is not None:\n",
    "            pred_accuracy = (pseudo_label == labels).sum() / len(labels)\n",
    "            outputs['pred_acc'] = pred_accuracy\n",
    "\n",
    "        outputs['patch_reprs'] = patch_reprs\n",
    "        outputs['attention_output'] = attention_output\n",
    "        outputs['attention_weights'] = attention_weights\n",
    "        outputs['last_hidden_state'] = last_cls_hidden_state\n",
    "        outputs['loss'] = loss\n",
    "        outputs['cossim_loss'] = loss_dict['cos_sim']\n",
    "        outputs['cls_loss'] = loss_dict['cls_loss']\n",
    "        outputs['acc'] = accuracy\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "dataset = dataset['train']\n",
    "splits = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([256, 3, 224, 224])\n",
      "labels torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=256, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_embed = pred_model.get_input_embeddings()\n",
    "# classifier_head = pred_model.classifier\n",
    "# hidden_size = config.hidden_size\n",
    "# model = SurrogateInterpretation(pred_model=pred_model, classifier_head=classifier_head, input_embed=input_embed, hidden_size=hidden_size)\n",
    "# model.to(device)\n",
    "# # outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_output shape:  torch.Size([1, 1, 768])\n",
      "attention_weights shape:  torch.Size([1, 1, 196])\n",
      "last_hidden_state shape:  torch.Size([1, 768])\n",
      "patch_reprs shape:  torch.Size([1, 196, 768])\n",
      "loss: 12.071322441101074, acc: 0.26171875, pred_acc: 0.765625cossim_loss: -0.24208229780197144, cls_loss: 12.31340503692627\n",
      "loss: 124.10237121582031, acc: 0.04296875, pred_acc: 0.78515625cossim_loss: -0.17761749029159546, cls_loss: 124.27999114990234\n",
      "loss: 242.25257873535156, acc: 0.0078125, pred_acc: 0.78515625cossim_loss: -0.1607733964920044, cls_loss: 242.41334533691406\n",
      "loss: 104.09986114501953, acc: 0.03515625, pred_acc: 0.75cossim_loss: -0.22822889685630798, cls_loss: 104.32808685302734\n",
      "loss: 55.381561279296875, acc: 0.06640625, pred_acc: 0.765625cossim_loss: -0.20862434804439545, cls_loss: 55.590187072753906\n",
      "loss: 75.34963989257812, acc: 0.07421875, pred_acc: 0.75cossim_loss: -0.2037229835987091, cls_loss: 75.55335998535156\n",
      "loss: 26.76494598388672, acc: 0.09375, pred_acc: 0.81640625cossim_loss: -0.2590194344520569, cls_loss: 27.02396583557129\n",
      "loss: 31.013845443725586, acc: 0.15625, pred_acc: 0.7109375cossim_loss: -0.24085280299186707, cls_loss: 31.254697799682617\n",
      "loss: 35.29372024536133, acc: 0.09375, pred_acc: 0.80859375cossim_loss: -0.21983560919761658, cls_loss: 35.51355743408203\n",
      "loss: 26.346460342407227, acc: 0.1015625, pred_acc: 0.7421875cossim_loss: -0.23132605850696564, cls_loss: 26.57778549194336\n",
      "loss: 23.41161346435547, acc: 0.1875, pred_acc: 0.765625cossim_loss: -0.24908123910427094, cls_loss: 23.660694122314453\n",
      "loss: 10.740873336791992, acc: 0.28125, pred_acc: 0.80078125cossim_loss: -0.282747745513916, cls_loss: 11.02362060546875\n",
      "loss: 8.925604820251465, acc: 0.33203125, pred_acc: 0.7109375cossim_loss: -0.32189303636550903, cls_loss: 9.24749755859375\n",
      "loss: 12.265054702758789, acc: 0.3046875, pred_acc: 0.7890625cossim_loss: -0.30737823247909546, cls_loss: 12.572432518005371\n",
      "loss: 14.594900131225586, acc: 0.296875, pred_acc: 0.76953125cossim_loss: -0.30665576457977295, cls_loss: 14.901556015014648\n",
      "loss: 10.222646713256836, acc: 0.28515625, pred_acc: 0.76953125cossim_loss: -0.31731387972831726, cls_loss: 10.539960861206055\n",
      "loss: 7.518924713134766, acc: 0.3984375, pred_acc: 0.8125cossim_loss: -0.3337060809135437, cls_loss: 7.852630615234375\n",
      "loss: 6.545637130737305, acc: 0.49609375, pred_acc: 0.76953125cossim_loss: -0.3589141368865967, cls_loss: 6.9045515060424805\n",
      "loss: 4.28728723526001, acc: 0.50390625, pred_acc: 0.73046875cossim_loss: -0.3734925389289856, cls_loss: 4.66077995300293\n",
      "loss: 2.8345394134521484, acc: 0.58984375, pred_acc: 0.734375cossim_loss: -0.3819040060043335, cls_loss: 3.2164433002471924\n",
      "loss: 2.253023862838745, acc: 0.640625, pred_acc: 0.78125cossim_loss: -0.4032943844795227, cls_loss: 2.656318187713623\n",
      "loss: 2.5506720542907715, acc: 0.65625, pred_acc: 0.765625cossim_loss: -0.4098758101463318, cls_loss: 2.960547924041748\n",
      "loss: 3.6674485206604004, acc: 0.5703125, pred_acc: 0.7578125cossim_loss: -0.42254602909088135, cls_loss: 4.089994430541992\n",
      "loss: 2.854759454727173, acc: 0.64453125, pred_acc: 0.75cossim_loss: -0.42708492279052734, cls_loss: 3.2818443775177\n",
      "loss: 1.6690349578857422, acc: 0.7109375, pred_acc: 0.79296875cossim_loss: -0.429481565952301, cls_loss: 2.0985164642333984\n",
      "loss: 1.4027211666107178, acc: 0.734375, pred_acc: 0.7578125cossim_loss: -0.4401586055755615, cls_loss: 1.8428797721862793\n",
      "loss: 1.2340925931930542, acc: 0.671875, pred_acc: 0.79296875cossim_loss: -0.4646735191345215, cls_loss: 1.6987661123275757\n",
      "loss: 0.9784141778945923, acc: 0.734375, pred_acc: 0.78515625cossim_loss: -0.4609163999557495, cls_loss: 1.4393305778503418\n",
      "loss: 1.1360552310943604, acc: 0.76171875, pred_acc: 0.76953125cossim_loss: -0.46182578802108765, cls_loss: 1.5978809595108032\n",
      "loss: 1.0160362720489502, acc: 0.74609375, pred_acc: 0.70703125cossim_loss: -0.4849528968334198, cls_loss: 1.5009891986846924\n",
      "loss: 0.6419386863708496, acc: 0.78125, pred_acc: 0.7890625cossim_loss: -0.4767155349254608, cls_loss: 1.1186542510986328\n",
      "loss: 1.2023159265518188, acc: 0.734375, pred_acc: 0.77734375cossim_loss: -0.48691096901893616, cls_loss: 1.6892268657684326\n",
      "loss: 0.8258763551712036, acc: 0.7421875, pred_acc: 0.765625cossim_loss: -0.4969543516635895, cls_loss: 1.3228306770324707\n",
      "loss: 0.9262164235115051, acc: 0.74609375, pred_acc: 0.7578125cossim_loss: -0.4898013472557068, cls_loss: 1.416017770767212\n",
      "loss: 0.6097109913825989, acc: 0.77734375, pred_acc: 0.7734375cossim_loss: -0.4970943331718445, cls_loss: 1.1068053245544434\n",
      "loss: 0.821778416633606, acc: 0.734375, pred_acc: 0.765625cossim_loss: -0.5031261444091797, cls_loss: 1.3249045610427856\n",
      "loss: 0.8015871644020081, acc: 0.76171875, pred_acc: 0.75390625cossim_loss: -0.5081107020378113, cls_loss: 1.3096978664398193\n",
      "loss: 0.7085093259811401, acc: 0.75390625, pred_acc: 0.7890625cossim_loss: -0.49445685744285583, cls_loss: 1.2029662132263184\n",
      "loss: 0.46912407875061035, acc: 0.77734375, pred_acc: 0.76953125cossim_loss: -0.5065851211547852, cls_loss: 0.9757091999053955\n",
      "loss: 0.9236917495727539, acc: 0.7578125, pred_acc: 0.76953125cossim_loss: -0.5168335437774658, cls_loss: 1.4405252933502197\n",
      "loss: 0.4252206087112427, acc: 0.7890625, pred_acc: 0.796875cossim_loss: -0.5062739849090576, cls_loss: 0.9314945936203003\n",
      "loss: 0.4402027130126953, acc: 0.7890625, pred_acc: 0.734375cossim_loss: -0.539548397064209, cls_loss: 0.9797511100769043\n",
      "loss: 0.7633956074714661, acc: 0.73046875, pred_acc: 0.75cossim_loss: -0.5199982523918152, cls_loss: 1.2833938598632812\n",
      "loss: 0.2865455746650696, acc: 0.796875, pred_acc: 0.78515625cossim_loss: -0.5276625752449036, cls_loss: 0.8142081499099731\n",
      "loss: 0.371502161026001, acc: 0.77734375, pred_acc: 0.7421875cossim_loss: -0.5305019617080688, cls_loss: 0.9020041227340698\n",
      "loss: 0.37675923109054565, acc: 0.76953125, pred_acc: 0.7578125cossim_loss: -0.5309369564056396, cls_loss: 0.9076961874961853\n",
      "loss: 0.155656635761261, acc: 0.796875, pred_acc: 0.703125cossim_loss: -0.535851001739502, cls_loss: 0.6915076375007629\n",
      "loss: 0.21054792404174805, acc: 0.8203125, pred_acc: 0.7578125cossim_loss: -0.5300618410110474, cls_loss: 0.7406097650527954\n",
      "loss: 0.26552116870880127, acc: 0.7734375, pred_acc: 0.75390625cossim_loss: -0.5243102312088013, cls_loss: 0.7898313999176025\n",
      "loss: 0.32724708318710327, acc: 0.7890625, pred_acc: 0.77734375cossim_loss: -0.5404109358787537, cls_loss: 0.8676580190658569\n",
      "loss: 0.18827742338180542, acc: 0.81640625, pred_acc: 0.7578125cossim_loss: -0.5326927900314331, cls_loss: 0.7209702134132385\n",
      "loss: 0.2822558283805847, acc: 0.828125, pred_acc: 0.73828125cossim_loss: -0.5524348020553589, cls_loss: 0.8346906304359436\n",
      "loss: 0.4561491012573242, acc: 0.7734375, pred_acc: 0.7734375cossim_loss: -0.5380822420120239, cls_loss: 0.9942313432693481\n",
      "loss: 0.17518293857574463, acc: 0.8046875, pred_acc: 0.828125cossim_loss: -0.5413603782653809, cls_loss: 0.7165433168411255\n",
      "loss: 0.23403668403625488, acc: 0.80859375, pred_acc: 0.765625cossim_loss: -0.5512095093727112, cls_loss: 0.7852461934089661\n",
      "loss: 0.37426555156707764, acc: 0.78515625, pred_acc: 0.78125cossim_loss: -0.5359716415405273, cls_loss: 0.910237193107605\n",
      "loss: 0.12087756395339966, acc: 0.84375, pred_acc: 0.81640625cossim_loss: -0.540043294429779, cls_loss: 0.6609208583831787\n",
      "loss: 0.23671674728393555, acc: 0.82421875, pred_acc: 0.80078125cossim_loss: -0.5530682802200317, cls_loss: 0.7897850275039673\n",
      "loss: 0.4202505350112915, acc: 0.78125, pred_acc: 0.80078125cossim_loss: -0.5418398380279541, cls_loss: 0.9620903730392456\n",
      "loss: 0.3032567501068115, acc: 0.8125, pred_acc: 0.78515625cossim_loss: -0.5528334379196167, cls_loss: 0.8560901880264282\n",
      "loss: 0.2811136245727539, acc: 0.8046875, pred_acc: 0.80078125cossim_loss: -0.5376620888710022, cls_loss: 0.8187757134437561\n",
      "loss: 0.4423311948776245, acc: 0.796875, pred_acc: 0.76171875cossim_loss: -0.5412120819091797, cls_loss: 0.9835432767868042\n",
      "loss: 0.25056886672973633, acc: 0.81640625, pred_acc: 0.76171875cossim_loss: -0.5409396886825562, cls_loss: 0.7915085554122925\n",
      "loss: 0.3611801862716675, acc: 0.77734375, pred_acc: 0.7578125cossim_loss: -0.5520787239074707, cls_loss: 0.9132589101791382\n",
      "loss: 0.28245943784713745, acc: 0.81640625, pred_acc: 0.78515625cossim_loss: -0.5368985533714294, cls_loss: 0.8193579912185669\n",
      "loss: 0.25639522075653076, acc: 0.78125, pred_acc: 0.7578125cossim_loss: -0.5470797419548035, cls_loss: 0.8034749627113342\n",
      "loss: 0.14433443546295166, acc: 0.8046875, pred_acc: 0.77734375cossim_loss: -0.5535191297531128, cls_loss: 0.6978535652160645\n",
      "loss: 0.27011269330978394, acc: 0.77734375, pred_acc: 0.765625cossim_loss: -0.5510337948799133, cls_loss: 0.8211464881896973\n",
      "loss: 0.19999366998672485, acc: 0.83203125, pred_acc: 0.74609375cossim_loss: -0.5510498285293579, cls_loss: 0.7510434985160828\n",
      "loss: 0.16961783170700073, acc: 0.80078125, pred_acc: 0.80078125cossim_loss: -0.5503209829330444, cls_loss: 0.7199388146400452\n",
      "loss: 0.20761680603027344, acc: 0.796875, pred_acc: 0.75390625cossim_loss: -0.5430799722671509, cls_loss: 0.7506967782974243\n",
      "loss: 0.367276132106781, acc: 0.796875, pred_acc: 0.7421875cossim_loss: -0.5518642663955688, cls_loss: 0.9191403985023499\n",
      "loss: 0.111533522605896, acc: 0.81640625, pred_acc: 0.7734375cossim_loss: -0.5504800081253052, cls_loss: 0.6620135307312012\n",
      "loss: 0.189380943775177, acc: 0.80859375, pred_acc: 0.7578125cossim_loss: -0.5539938807487488, cls_loss: 0.7433748245239258\n",
      "loss: 0.3232279419898987, acc: 0.80859375, pred_acc: 0.734375cossim_loss: -0.5605326294898987, cls_loss: 0.8837605714797974\n",
      "loss: 0.35828351974487305, acc: 0.76953125, pred_acc: 0.71875cossim_loss: -0.5541743040084839, cls_loss: 0.9124578237533569\n",
      "loss: 0.10249799489974976, acc: 0.84765625, pred_acc: 0.75cossim_loss: -0.5477539896965027, cls_loss: 0.6502519845962524\n",
      "loss: 0.047887980937957764, acc: 0.828125, pred_acc: 0.74609375cossim_loss: -0.5430852770805359, cls_loss: 0.5909732580184937\n",
      "loss: 0.06087195873260498, acc: 0.82421875, pred_acc: 0.78125cossim_loss: -0.5599315166473389, cls_loss: 0.6208034753799438\n",
      "loss: 0.2794874310493469, acc: 0.79296875, pred_acc: 0.7734375cossim_loss: -0.5475960969924927, cls_loss: 0.8270835280418396\n",
      "loss: 0.11524307727813721, acc: 0.8046875, pred_acc: 0.796875cossim_loss: -0.5512583255767822, cls_loss: 0.6665014028549194\n",
      "loss: 0.31948554515838623, acc: 0.765625, pred_acc: 0.77734375cossim_loss: -0.5553789734840393, cls_loss: 0.8748645186424255\n",
      "loss: 0.17593050003051758, acc: 0.8125, pred_acc: 0.765625cossim_loss: -0.5505403280258179, cls_loss: 0.7264708280563354\n",
      "loss: 0.2565279006958008, acc: 0.8125, pred_acc: 0.76953125cossim_loss: -0.5604459047317505, cls_loss: 0.8169738054275513\n",
      "loss: 0.20652353763580322, acc: 0.8046875, pred_acc: 0.8046875cossim_loss: -0.5572946071624756, cls_loss: 0.7638181447982788\n",
      "loss: 0.11853116750717163, acc: 0.80078125, pred_acc: 0.7421875cossim_loss: -0.5545451641082764, cls_loss: 0.673076331615448\n",
      "loss: 0.11373889446258545, acc: 0.8046875, pred_acc: 0.7578125cossim_loss: -0.5655566453933716, cls_loss: 0.679295539855957\n",
      "loss: -0.02687925100326538, acc: 0.82421875, pred_acc: 0.82421875cossim_loss: -0.5619484782218933, cls_loss: 0.5350692272186279\n",
      "loss: 0.13572824001312256, acc: 0.8046875, pred_acc: 0.75cossim_loss: -0.5598698854446411, cls_loss: 0.6955981254577637\n",
      "loss: 0.08307290077209473, acc: 0.82421875, pred_acc: 0.76953125cossim_loss: -0.5594099760055542, cls_loss: 0.6424828767776489\n",
      "loss: 0.15145766735076904, acc: 0.8125, pred_acc: 0.796875cossim_loss: -0.5549407601356506, cls_loss: 0.7063984274864197\n",
      "loss: 0.1744009256362915, acc: 0.796875, pred_acc: 0.7734375cossim_loss: -0.5562541484832764, cls_loss: 0.7306550741195679\n",
      "loss: 0.180342435836792, acc: 0.8359375, pred_acc: 0.734375cossim_loss: -0.5626994371414185, cls_loss: 0.7430418729782104\n",
      "loss: 0.09678572416305542, acc: 0.859375, pred_acc: 0.76953125cossim_loss: -0.5645743608474731, cls_loss: 0.6613600850105286\n",
      "loss: 0.22735190391540527, acc: 0.8125, pred_acc: 0.73828125cossim_loss: -0.5746214389801025, cls_loss: 0.8019733428955078\n",
      "loss: 0.20158737897872925, acc: 0.796875, pred_acc: 0.79296875cossim_loss: -0.5684379935264587, cls_loss: 0.770025372505188\n",
      "loss: 0.0048912763595581055, acc: 0.8203125, pred_acc: 0.79296875cossim_loss: -0.5573995113372803, cls_loss: 0.5622907876968384\n",
      "loss: -0.014631330966949463, acc: 0.84765625, pred_acc: 0.796875cossim_loss: -0.569942831993103, cls_loss: 0.5553115010261536\n",
      "loss: 0.03942835330963135, acc: 0.8515625, pred_acc: 0.75390625cossim_loss: -0.5731443762779236, cls_loss: 0.6125727295875549\n",
      "loss: -0.054784178733825684, acc: 0.84375, pred_acc: 0.73828125cossim_loss: -0.5587396621704102, cls_loss: 0.5039554834365845\n",
      "loss: 0.11854028701782227, acc: 0.85546875, pred_acc: 0.74609375cossim_loss: -0.5700534582138062, cls_loss: 0.6885937452316284\n",
      "loss: 0.1983330249786377, acc: 0.8359375, pred_acc: 0.75390625cossim_loss: -0.5706127882003784, cls_loss: 0.7689458131790161\n",
      "loss: 0.11035054922103882, acc: 0.8125, pred_acc: 0.80859375cossim_loss: -0.56783527135849, cls_loss: 0.6781858205795288\n",
      "loss: 0.19272834062576294, acc: 0.796875, pred_acc: 0.69921875cossim_loss: -0.5652706623077393, cls_loss: 0.7579990029335022\n",
      "loss: 0.25983524322509766, acc: 0.76953125, pred_acc: 0.77734375cossim_loss: -0.5763015747070312, cls_loss: 0.8361368179321289\n",
      "loss: 0.16523444652557373, acc: 0.82421875, pred_acc: 0.70703125cossim_loss: -0.5718561410903931, cls_loss: 0.7370905876159668\n",
      "loss: 0.22234266996383667, acc: 0.7734375, pred_acc: 0.7734375cossim_loss: -0.5783345103263855, cls_loss: 0.8006771802902222\n",
      "loss: 0.16631406545639038, acc: 0.8359375, pred_acc: 0.79296875cossim_loss: -0.565967321395874, cls_loss: 0.7322813868522644\n",
      "loss: 0.02479851245880127, acc: 0.828125, pred_acc: 0.78515625cossim_loss: -0.5628553628921509, cls_loss: 0.5876538753509521\n",
      "loss: -0.049515604972839355, acc: 0.84375, pred_acc: 0.76171875cossim_loss: -0.5555589199066162, cls_loss: 0.5060433149337769\n",
      "loss: 0.08824360370635986, acc: 0.81640625, pred_acc: 0.7734375cossim_loss: -0.5740135908126831, cls_loss: 0.662257194519043\n",
      "loss: 0.12938785552978516, acc: 0.8046875, pred_acc: 0.73828125cossim_loss: -0.5724411010742188, cls_loss: 0.7018289566040039\n",
      "loss: 0.23463785648345947, acc: 0.8046875, pred_acc: 0.73828125cossim_loss: -0.572835385799408, cls_loss: 0.8074732422828674\n",
      "loss: 0.2456296682357788, acc: 0.7890625, pred_acc: 0.7734375cossim_loss: -0.5687295198440552, cls_loss: 0.814359188079834\n",
      "loss: 0.20703351497650146, acc: 0.80859375, pred_acc: 0.7421875cossim_loss: -0.5696371793746948, cls_loss: 0.7766706943511963\n",
      "loss: 0.017161071300506592, acc: 0.83203125, pred_acc: 0.73828125cossim_loss: -0.5788276195526123, cls_loss: 0.5959886908531189\n",
      "loss: 0.3589945435523987, acc: 0.77734375, pred_acc: 0.68359375cossim_loss: -0.5831517577171326, cls_loss: 0.9421463012695312\n",
      "loss: 0.1850820779800415, acc: 0.7890625, pred_acc: 0.76171875cossim_loss: -0.5789741277694702, cls_loss: 0.7640562057495117\n",
      "loss: 0.04442024230957031, acc: 0.83984375, pred_acc: 0.75cossim_loss: -0.5759413242340088, cls_loss: 0.6203615665435791\n",
      "loss: -0.021602094173431396, acc: 0.84765625, pred_acc: 0.77734375cossim_loss: -0.566795289516449, cls_loss: 0.5451931953430176\n",
      "loss: 0.2412005066871643, acc: 0.80078125, pred_acc: 0.7578125cossim_loss: -0.5801330804824829, cls_loss: 0.8213335871696472\n",
      "loss: 0.03495579957962036, acc: 0.8203125, pred_acc: 0.7421875cossim_loss: -0.5729578733444214, cls_loss: 0.6079136729240417\n",
      "loss: 0.19231224060058594, acc: 0.80859375, pred_acc: 0.78125cossim_loss: -0.5834710001945496, cls_loss: 0.7757832407951355\n",
      "loss: 0.2236771583557129, acc: 0.79296875, pred_acc: 0.7421875cossim_loss: -0.5756478309631348, cls_loss: 0.7993249893188477\n",
      "loss: 0.0238039493560791, acc: 0.8046875, pred_acc: 0.7578125cossim_loss: -0.5705944299697876, cls_loss: 0.5943983793258667\n",
      "loss: 0.029029369354248047, acc: 0.83984375, pred_acc: 0.75390625cossim_loss: -0.5855149030685425, cls_loss: 0.6145442724227905\n",
      "loss: 0.10301017761230469, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.5860884189605713, cls_loss: 0.689098596572876\n",
      "loss: 0.11002177000045776, acc: 0.8125, pred_acc: 0.75cossim_loss: -0.5812714695930481, cls_loss: 0.6912932395935059\n",
      "loss: 0.13511991500854492, acc: 0.8046875, pred_acc: 0.74609375cossim_loss: -0.5837142467498779, cls_loss: 0.7188341617584229\n",
      "loss: 0.02938377857208252, acc: 0.828125, pred_acc: 0.80859375cossim_loss: -0.5859944820404053, cls_loss: 0.6153782606124878\n",
      "loss: 0.32198482751846313, acc: 0.78515625, pred_acc: 0.734375cossim_loss: -0.5781925320625305, cls_loss: 0.9001773595809937\n",
      "loss: 0.1311413049697876, acc: 0.80859375, pred_acc: 0.79296875cossim_loss: -0.5773239135742188, cls_loss: 0.7084652185440063\n",
      "loss: 0.0809485912322998, acc: 0.80859375, pred_acc: 0.76171875cossim_loss: -0.5729405879974365, cls_loss: 0.6538891792297363\n",
      "loss: 0.1314699649810791, acc: 0.80859375, pred_acc: 0.72265625cossim_loss: -0.5722517967224121, cls_loss: 0.7037217617034912\n",
      "loss: 0.16557985544204712, acc: 0.78515625, pred_acc: 0.765625cossim_loss: -0.5850507020950317, cls_loss: 0.7506305575370789\n",
      "loss: -0.0027840733528137207, acc: 0.80078125, pred_acc: 0.80078125cossim_loss: -0.5758847594261169, cls_loss: 0.5731006860733032\n",
      "loss: 0.046973466873168945, acc: 0.8203125, pred_acc: 0.80078125cossim_loss: -0.5721884965896606, cls_loss: 0.6191619634628296\n",
      "loss: -0.030364394187927246, acc: 0.8359375, pred_acc: 0.76171875cossim_loss: -0.5851988792419434, cls_loss: 0.5548344850540161\n",
      "loss: -0.10980552434921265, acc: 0.83984375, pred_acc: 0.765625cossim_loss: -0.5853948593139648, cls_loss: 0.4755893349647522\n",
      "loss: -0.06801873445510864, acc: 0.87890625, pred_acc: 0.765625cossim_loss: -0.5895890593528748, cls_loss: 0.5215703248977661\n",
      "loss: -0.06131184101104736, acc: 0.8515625, pred_acc: 0.828125cossim_loss: -0.5865359306335449, cls_loss: 0.5252240896224976\n",
      "loss: 0.18418437242507935, acc: 0.84375, pred_acc: 0.765625cossim_loss: -0.5801251530647278, cls_loss: 0.7643095254898071\n",
      "loss: 0.11670637130737305, acc: 0.81640625, pred_acc: 0.73828125cossim_loss: -0.5834043025970459, cls_loss: 0.700110673904419\n",
      "loss: 0.057579636573791504, acc: 0.8203125, pred_acc: 0.7578125cossim_loss: -0.5786612629890442, cls_loss: 0.6362408995628357\n",
      "loss: 0.04812943935394287, acc: 0.83203125, pred_acc: 0.78125cossim_loss: -0.5732311010360718, cls_loss: 0.6213605403900146\n",
      "loss: 0.1572321653366089, acc: 0.80859375, pred_acc: 0.8046875cossim_loss: -0.5877420902252197, cls_loss: 0.7449742555618286\n",
      "loss: 0.13455188274383545, acc: 0.80859375, pred_acc: 0.796875cossim_loss: -0.5959776639938354, cls_loss: 0.7305295467376709\n",
      "loss: 0.0396651029586792, acc: 0.84765625, pred_acc: 0.78125cossim_loss: -0.581794261932373, cls_loss: 0.6214593648910522\n",
      "loss: 0.06758439540863037, acc: 0.82421875, pred_acc: 0.76953125cossim_loss: -0.5798823833465576, cls_loss: 0.647466778755188\n",
      "loss: 0.109616219997406, acc: 0.83984375, pred_acc: 0.7890625cossim_loss: -0.588779628276825, cls_loss: 0.698395848274231\n",
      "loss: -0.11867415904998779, acc: 0.859375, pred_acc: 0.78515625cossim_loss: -0.5849436521530151, cls_loss: 0.46626949310302734\n",
      "loss: 0.2224191427230835, acc: 0.79296875, pred_acc: 0.80078125cossim_loss: -0.5932934880256653, cls_loss: 0.8157126307487488\n",
      "loss: 0.15631103515625, acc: 0.80859375, pred_acc: 0.765625cossim_loss: -0.5882683992385864, cls_loss: 0.7445794343948364\n",
      "loss: -0.0170443058013916, acc: 0.83203125, pred_acc: 0.7734375cossim_loss: -0.5812164545059204, cls_loss: 0.5641721487045288\n",
      "loss: 0.2801365852355957, acc: 0.765625, pred_acc: 0.7890625cossim_loss: -0.582679033279419, cls_loss: 0.8628156185150146\n",
      "loss: -0.10731792449951172, acc: 0.84765625, pred_acc: 0.80078125cossim_loss: -0.5796136856079102, cls_loss: 0.47229576110839844\n",
      "loss: -0.10461971163749695, acc: 0.8515625, pred_acc: 0.8046875cossim_loss: -0.5836106538772583, cls_loss: 0.47899094223976135\n",
      "loss: 0.09873509407043457, acc: 0.80078125, pred_acc: 0.73828125cossim_loss: -0.591025710105896, cls_loss: 0.6897608041763306\n",
      "loss: 0.1955079436302185, acc: 0.78515625, pred_acc: 0.7578125cossim_loss: -0.5897471904754639, cls_loss: 0.7852551341056824\n",
      "loss: 0.11120754480361938, acc: 0.79296875, pred_acc: 0.78125cossim_loss: -0.5963204503059387, cls_loss: 0.7075279951095581\n",
      "loss: 0.21052861213684082, acc: 0.828125, pred_acc: 0.75cossim_loss: -0.586196780204773, cls_loss: 0.7967253923416138\n",
      "loss: 0.023206055164337158, acc: 0.82421875, pred_acc: 0.73828125cossim_loss: -0.6048812866210938, cls_loss: 0.6280873417854309\n",
      "loss: 0.11694252490997314, acc: 0.8125, pred_acc: 0.78125cossim_loss: -0.590030312538147, cls_loss: 0.7069728374481201\n",
      "loss: -0.07012492418289185, acc: 0.85546875, pred_acc: 0.78125cossim_loss: -0.5899614691734314, cls_loss: 0.5198365449905396\n",
      "loss: 0.01816403865814209, acc: 0.828125, pred_acc: 0.7421875cossim_loss: -0.5973114967346191, cls_loss: 0.6154755353927612\n",
      "loss: -0.12885263562202454, acc: 0.8515625, pred_acc: 0.74609375cossim_loss: -0.5872815847396851, cls_loss: 0.4584289491176605\n",
      "loss: 0.06828171014785767, acc: 0.8125, pred_acc: 0.7421875cossim_loss: -0.5877788662910461, cls_loss: 0.6560605764389038\n",
      "loss: -0.033738553524017334, acc: 0.8671875, pred_acc: 0.81640625cossim_loss: -0.5759457945823669, cls_loss: 0.5422072410583496\n",
      "loss: -0.047833919525146484, acc: 0.8359375, pred_acc: 0.74609375cossim_loss: -0.5906403064727783, cls_loss: 0.5428063869476318\n",
      "loss: 0.035835206508636475, acc: 0.828125, pred_acc: 0.75cossim_loss: -0.5878296494483948, cls_loss: 0.6236648559570312\n",
      "loss: 0.0523717999458313, acc: 0.81640625, pred_acc: 0.76171875cossim_loss: -0.5903638005256653, cls_loss: 0.6427356004714966\n",
      "loss: -0.024916231632232666, acc: 0.8359375, pred_acc: 0.7578125cossim_loss: -0.6019451022148132, cls_loss: 0.5770288705825806\n",
      "loss: 0.14226704835891724, acc: 0.82421875, pred_acc: 0.76953125cossim_loss: -0.5988540053367615, cls_loss: 0.7411210536956787\n",
      "loss: 0.15121948719024658, acc: 0.83203125, pred_acc: 0.7578125cossim_loss: -0.5893244743347168, cls_loss: 0.7405439615249634\n",
      "loss: 0.03310978412628174, acc: 0.81640625, pred_acc: 0.77734375cossim_loss: -0.6018040776252747, cls_loss: 0.6349138617515564\n",
      "loss: 0.13244765996932983, acc: 0.7950000166893005, pred_acc: 0.7300000190734863cossim_loss: -0.5935710072517395, cls_loss: 0.7260186672210693\n",
      "loss: -0.10393229126930237, acc: 0.85546875, pred_acc: 0.7890625cossim_loss: -0.6006571054458618, cls_loss: 0.49672481417655945\n",
      "loss: -0.055694401264190674, acc: 0.85546875, pred_acc: 0.77734375cossim_loss: -0.594679594039917, cls_loss: 0.5389851927757263\n",
      "loss: 0.36709094047546387, acc: 0.79296875, pred_acc: 0.75cossim_loss: -0.5970866084098816, cls_loss: 0.9641775488853455\n",
      "loss: 0.10998553037643433, acc: 0.8203125, pred_acc: 0.79296875cossim_loss: -0.5891408324241638, cls_loss: 0.6991263628005981\n",
      "loss: -0.010137081146240234, acc: 0.83984375, pred_acc: 0.76171875cossim_loss: -0.5963178873062134, cls_loss: 0.5861808061599731\n",
      "loss: 0.08731710910797119, acc: 0.80859375, pred_acc: 0.734375cossim_loss: -0.5883293151855469, cls_loss: 0.6756464242935181\n",
      "loss: -0.024155795574188232, acc: 0.83984375, pred_acc: 0.76171875cossim_loss: -0.5947902202606201, cls_loss: 0.5706344246864319\n",
      "loss: 0.34022384881973267, acc: 0.77734375, pred_acc: 0.765625cossim_loss: -0.5987304449081421, cls_loss: 0.9389542937278748\n",
      "loss: 0.097697913646698, acc: 0.80859375, pred_acc: 0.71875cossim_loss: -0.5999240875244141, cls_loss: 0.6976220011711121\n",
      "loss: 0.027799665927886963, acc: 0.85546875, pred_acc: 0.7734375cossim_loss: -0.5831276178359985, cls_loss: 0.6109272837638855\n",
      "loss: 0.01976597309112549, acc: 0.859375, pred_acc: 0.7578125cossim_loss: -0.5943410396575928, cls_loss: 0.6141070127487183\n",
      "loss: -0.1866995394229889, acc: 0.8984375, pred_acc: 0.75390625cossim_loss: -0.5980651378631592, cls_loss: 0.4113655984401703\n",
      "loss: 0.24045920372009277, acc: 0.78125, pred_acc: 0.7265625cossim_loss: -0.5987651348114014, cls_loss: 0.8392243385314941\n",
      "loss: 0.00288313627243042, acc: 0.8515625, pred_acc: 0.80859375cossim_loss: -0.5953200459480286, cls_loss: 0.598203182220459\n",
      "loss: 0.01820683479309082, acc: 0.80859375, pred_acc: 0.76953125cossim_loss: -0.6019353270530701, cls_loss: 0.6201421618461609\n",
      "loss: 0.04488837718963623, acc: 0.83984375, pred_acc: 0.7578125cossim_loss: -0.6079744100570679, cls_loss: 0.6528627872467041\n",
      "loss: 0.05554145574569702, acc: 0.80078125, pred_acc: 0.78515625cossim_loss: -0.6025059819221497, cls_loss: 0.6580474376678467\n",
      "loss: -0.009698212146759033, acc: 0.83984375, pred_acc: 0.75390625cossim_loss: -0.593044102191925, cls_loss: 0.583345890045166\n",
      "loss: -0.11774575710296631, acc: 0.875, pred_acc: 0.82421875cossim_loss: -0.5967592000961304, cls_loss: 0.47901344299316406\n",
      "loss: -0.0427132248878479, acc: 0.8203125, pred_acc: 0.75390625cossim_loss: -0.5886943340301514, cls_loss: 0.5459811091423035\n",
      "loss: 0.015525579452514648, acc: 0.82421875, pred_acc: 0.78125cossim_loss: -0.6034893989562988, cls_loss: 0.6190149784088135\n",
      "loss: -0.04206562042236328, acc: 0.83203125, pred_acc: 0.78125cossim_loss: -0.5899922847747803, cls_loss: 0.547926664352417\n",
      "loss: -0.04462563991546631, acc: 0.83203125, pred_acc: 0.796875cossim_loss: -0.5943740606307983, cls_loss: 0.549748420715332\n",
      "loss: 0.09061402082443237, acc: 0.81640625, pred_acc: 0.734375cossim_loss: -0.6066431403160095, cls_loss: 0.6972571611404419\n",
      "loss: -0.16951468586921692, acc: 0.87109375, pred_acc: 0.80078125cossim_loss: -0.6020795106887817, cls_loss: 0.4325648248195648\n",
      "loss: 0.03520810604095459, acc: 0.828125, pred_acc: 0.73046875cossim_loss: -0.6039422750473022, cls_loss: 0.6391503810882568\n",
      "loss: -0.08249890804290771, acc: 0.84765625, pred_acc: 0.83984375cossim_loss: -0.5932343006134033, cls_loss: 0.5107353925704956\n",
      "loss: 0.08647304773330688, acc: 0.828125, pred_acc: 0.79296875cossim_loss: -0.5984479188919067, cls_loss: 0.6849209666252136\n",
      "loss: 0.032162249088287354, acc: 0.8359375, pred_acc: 0.77734375cossim_loss: -0.5951756238937378, cls_loss: 0.6273378729820251\n",
      "loss: -0.0469202995300293, acc: 0.83203125, pred_acc: 0.75cossim_loss: -0.6073164939880371, cls_loss: 0.5603961944580078\n",
      "loss: 0.01188206672668457, acc: 0.8671875, pred_acc: 0.8046875cossim_loss: -0.5916738510131836, cls_loss: 0.6035559177398682\n",
      "loss: 0.17273658514022827, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.5980663895606995, cls_loss: 0.7708029747009277\n",
      "loss: 0.13455283641815186, acc: 0.8125, pred_acc: 0.7578125cossim_loss: -0.6007541418075562, cls_loss: 0.735306978225708\n",
      "loss: 0.06554055213928223, acc: 0.8203125, pred_acc: 0.7734375cossim_loss: -0.6054360866546631, cls_loss: 0.6709766387939453\n",
      "loss: 0.04122817516326904, acc: 0.8359375, pred_acc: 0.81640625cossim_loss: -0.5934581756591797, cls_loss: 0.6346863508224487\n",
      "loss: 0.12569022178649902, acc: 0.78515625, pred_acc: 0.7734375cossim_loss: -0.5983440279960632, cls_loss: 0.7240342497825623\n",
      "loss: -0.19160577654838562, acc: 0.8828125, pred_acc: 0.75390625cossim_loss: -0.6084041595458984, cls_loss: 0.4167983829975128\n",
      "loss: 0.024968445301055908, acc: 0.8671875, pred_acc: 0.80078125cossim_loss: -0.5975056886672974, cls_loss: 0.6224741339683533\n",
      "loss: 0.059593796730041504, acc: 0.828125, pred_acc: 0.78125cossim_loss: -0.5891896486282349, cls_loss: 0.6487834453582764\n",
      "loss: -0.07030308246612549, acc: 0.84375, pred_acc: 0.796875cossim_loss: -0.5913044214248657, cls_loss: 0.5210013389587402\n",
      "loss: 0.07187539339065552, acc: 0.7890625, pred_acc: 0.77734375cossim_loss: -0.6081202030181885, cls_loss: 0.679995596408844\n",
      "loss: -0.058778464794158936, acc: 0.87890625, pred_acc: 0.8046875cossim_loss: -0.6039193868637085, cls_loss: 0.5451409220695496\n",
      "loss: -0.0320853590965271, acc: 0.8359375, pred_acc: 0.79296875cossim_loss: -0.6026864051818848, cls_loss: 0.5706010460853577\n",
      "loss: 0.010326981544494629, acc: 0.83984375, pred_acc: 0.734375cossim_loss: -0.6085597276687622, cls_loss: 0.6188867092132568\n",
      "loss: 0.12303626537322998, acc: 0.83203125, pred_acc: 0.81640625cossim_loss: -0.5939633846282959, cls_loss: 0.7169996500015259\n",
      "loss: -0.042890191078186035, acc: 0.85546875, pred_acc: 0.8125cossim_loss: -0.6053425073623657, cls_loss: 0.5624523162841797\n",
      "loss: 0.16415560245513916, acc: 0.7734375, pred_acc: 0.76953125cossim_loss: -0.6058619022369385, cls_loss: 0.7700175046920776\n",
      "loss: 0.03974020481109619, acc: 0.83203125, pred_acc: 0.78125cossim_loss: -0.6021398305892944, cls_loss: 0.6418800354003906\n",
      "loss: 0.011989116668701172, acc: 0.8046875, pred_acc: 0.7421875cossim_loss: -0.6078434586524963, cls_loss: 0.6198325753211975\n",
      "loss: 0.027175486087799072, acc: 0.82421875, pred_acc: 0.7578125cossim_loss: -0.6098729968070984, cls_loss: 0.6370484828948975\n",
      "loss: 0.06583517789840698, acc: 0.83984375, pred_acc: 0.7578125cossim_loss: -0.6015726923942566, cls_loss: 0.6674078702926636\n",
      "loss: 0.10223698616027832, acc: 0.828125, pred_acc: 0.80859375cossim_loss: -0.6053189039230347, cls_loss: 0.707555890083313\n",
      "loss: 0.0801386833190918, acc: 0.81640625, pred_acc: 0.79296875cossim_loss: -0.6013147830963135, cls_loss: 0.6814534664154053\n",
      "loss: 0.14818882942199707, acc: 0.80859375, pred_acc: 0.765625cossim_loss: -0.6028655171394348, cls_loss: 0.7510543465614319\n",
      "loss: 0.034801602363586426, acc: 0.84375, pred_acc: 0.75390625cossim_loss: -0.6115883588790894, cls_loss: 0.6463899612426758\n",
      "loss: -0.031391680240631104, acc: 0.84765625, pred_acc: 0.8125cossim_loss: -0.6059596538543701, cls_loss: 0.574567973613739\n",
      "loss: 0.04519551992416382, acc: 0.84765625, pred_acc: 0.78125cossim_loss: -0.6007487773895264, cls_loss: 0.6459442973136902\n",
      "loss: 0.03217345476150513, acc: 0.81640625, pred_acc: 0.7265625cossim_loss: -0.6178550124168396, cls_loss: 0.6500284671783447\n",
      "loss: 0.00026661157608032227, acc: 0.83984375, pred_acc: 0.80859375cossim_loss: -0.6030962467193604, cls_loss: 0.6033628582954407\n",
      "loss: -0.0047203898429870605, acc: 0.84765625, pred_acc: 0.8125cossim_loss: -0.6017063856124878, cls_loss: 0.5969859957695007\n",
      "loss: -0.0790979266166687, acc: 0.87109375, pred_acc: 0.76953125cossim_loss: -0.6082451939582825, cls_loss: 0.5291472673416138\n",
      "loss: 0.15959584712982178, acc: 0.81640625, pred_acc: 0.74609375cossim_loss: -0.6012266874313354, cls_loss: 0.7608225345611572\n",
      "loss: 0.1840919852256775, acc: 0.796875, pred_acc: 0.71484375cossim_loss: -0.617157518863678, cls_loss: 0.8012495040893555\n",
      "loss: -0.027418971061706543, acc: 0.86328125, pred_acc: 0.73828125cossim_loss: -0.605385422706604, cls_loss: 0.5779664516448975\n",
      "loss: 0.10933923721313477, acc: 0.8203125, pred_acc: 0.734375cossim_loss: -0.613001823425293, cls_loss: 0.7223410606384277\n",
      "loss: 0.14718079566955566, acc: 0.828125, pred_acc: 0.75cossim_loss: -0.6087868213653564, cls_loss: 0.7559676170349121\n",
      "loss: -0.014009416103363037, acc: 0.83203125, pred_acc: 0.76171875cossim_loss: -0.6075677871704102, cls_loss: 0.5935583710670471\n",
      "loss: -0.05757498741149902, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.6128902435302734, cls_loss: 0.5553152561187744\n",
      "loss: -0.07344949245452881, acc: 0.8515625, pred_acc: 0.76171875cossim_loss: -0.6082277297973633, cls_loss: 0.5347782373428345\n",
      "loss: 0.05722755193710327, acc: 0.796875, pred_acc: 0.75cossim_loss: -0.612804114818573, cls_loss: 0.6700316667556763\n",
      "loss: -0.0039027929306030273, acc: 0.8203125, pred_acc: 0.7421875cossim_loss: -0.6067533493041992, cls_loss: 0.6028505563735962\n",
      "loss: 0.0711970329284668, acc: 0.8359375, pred_acc: 0.72265625cossim_loss: -0.6130689382553101, cls_loss: 0.6842659711837769\n",
      "loss: 0.06733763217926025, acc: 0.8046875, pred_acc: 0.77734375cossim_loss: -0.6075021028518677, cls_loss: 0.6748397350311279\n",
      "loss: 0.06510728597640991, acc: 0.8515625, pred_acc: 0.7578125cossim_loss: -0.6128227710723877, cls_loss: 0.6779300570487976\n",
      "loss: -0.02459561824798584, acc: 0.83203125, pred_acc: 0.734375cossim_loss: -0.6050233840942383, cls_loss: 0.5804277658462524\n",
      "loss: 0.1615721583366394, acc: 0.828125, pred_acc: 0.734375cossim_loss: -0.6240457892417908, cls_loss: 0.7856179475784302\n",
      "loss: -0.09727698564529419, acc: 0.84765625, pred_acc: 0.77734375cossim_loss: -0.6105071306228638, cls_loss: 0.5132301449775696\n",
      "loss: 0.010130524635314941, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.6048027276992798, cls_loss: 0.6149332523345947\n",
      "loss: 0.15423953533172607, acc: 0.79296875, pred_acc: 0.73046875cossim_loss: -0.6117783188819885, cls_loss: 0.7660178542137146\n",
      "loss: 0.17213451862335205, acc: 0.7890625, pred_acc: 0.77734375cossim_loss: -0.6062753200531006, cls_loss: 0.7784098386764526\n",
      "loss: 0.15155351161956787, acc: 0.7890625, pred_acc: 0.7421875cossim_loss: -0.6010404825210571, cls_loss: 0.752593994140625\n",
      "loss: -0.07540547847747803, acc: 0.8359375, pred_acc: 0.80078125cossim_loss: -0.6033587455749512, cls_loss: 0.5279532670974731\n",
      "loss: 0.014313817024230957, acc: 0.80078125, pred_acc: 0.74609375cossim_loss: -0.607887864112854, cls_loss: 0.622201681137085\n",
      "loss: -0.022064387798309326, acc: 0.83984375, pred_acc: 0.734375cossim_loss: -0.6123605966567993, cls_loss: 0.59029620885849\n",
      "loss: 0.11282545328140259, acc: 0.8359375, pred_acc: 0.71875cossim_loss: -0.6142438054084778, cls_loss: 0.7270692586898804\n",
      "loss: 0.231051504611969, acc: 0.76171875, pred_acc: 0.73828125cossim_loss: -0.6154505014419556, cls_loss: 0.8465020060539246\n",
      "loss: 0.04325723648071289, acc: 0.8046875, pred_acc: 0.73828125cossim_loss: -0.6127820014953613, cls_loss: 0.6560392379760742\n",
      "loss: 0.26768332719802856, acc: 0.78125, pred_acc: 0.7578125cossim_loss: -0.6091254949569702, cls_loss: 0.8768088221549988\n",
      "loss: 0.013459444046020508, acc: 0.796875, pred_acc: 0.7421875cossim_loss: -0.6114439964294434, cls_loss: 0.6249034404754639\n",
      "loss: -0.06688952445983887, acc: 0.8515625, pred_acc: 0.75390625cossim_loss: -0.6076734662055969, cls_loss: 0.5407839417457581\n",
      "loss: 0.05813115835189819, acc: 0.81640625, pred_acc: 0.7734375cossim_loss: -0.6067894101142883, cls_loss: 0.6649205684661865\n",
      "loss: -0.03811442852020264, acc: 0.84375, pred_acc: 0.79296875cossim_loss: -0.6067484617233276, cls_loss: 0.568634033203125\n",
      "loss: 0.029608488082885742, acc: 0.8203125, pred_acc: 0.8203125cossim_loss: -0.6029762029647827, cls_loss: 0.6325846910476685\n",
      "loss: -0.09395110607147217, acc: 0.86328125, pred_acc: 0.73828125cossim_loss: -0.6102173328399658, cls_loss: 0.5162662267684937\n",
      "loss: -0.14227595925331116, acc: 0.8671875, pred_acc: 0.78125cossim_loss: -0.6109272837638855, cls_loss: 0.46865132451057434\n",
      "loss: -0.00694507360458374, acc: 0.8203125, pred_acc: 0.7578125cossim_loss: -0.610792338848114, cls_loss: 0.6038472652435303\n",
      "loss: -0.014652907848358154, acc: 0.828125, pred_acc: 0.7578125cossim_loss: -0.6079279780387878, cls_loss: 0.5932750701904297\n",
      "loss: 0.022713541984558105, acc: 0.84375, pred_acc: 0.77734375cossim_loss: -0.6094909906387329, cls_loss: 0.632204532623291\n",
      "loss: 0.04329276084899902, acc: 0.8046875, pred_acc: 0.72265625cossim_loss: -0.6129661798477173, cls_loss: 0.6562589406967163\n",
      "loss: -0.012245476245880127, acc: 0.81640625, pred_acc: 0.7734375cossim_loss: -0.6143512725830078, cls_loss: 0.6021057963371277\n",
      "loss: 0.06405019760131836, acc: 0.828125, pred_acc: 0.76953125cossim_loss: -0.6098763942718506, cls_loss: 0.673926591873169\n",
      "loss: 0.0455174446105957, acc: 0.84375, pred_acc: 0.8203125cossim_loss: -0.6170759201049805, cls_loss: 0.6625933647155762\n",
      "loss: 0.1875990629196167, acc: 0.80078125, pred_acc: 0.7265625cossim_loss: -0.6121090650558472, cls_loss: 0.7997081279754639\n",
      "loss: 0.021183669567108154, acc: 0.83203125, pred_acc: 0.75cossim_loss: -0.610870897769928, cls_loss: 0.6320545673370361\n",
      "loss: 0.16746950149536133, acc: 0.8046875, pred_acc: 0.70703125cossim_loss: -0.616463303565979, cls_loss: 0.7839328050613403\n",
      "loss: 0.2605712413787842, acc: 0.76953125, pred_acc: 0.7734375cossim_loss: -0.6085349321365356, cls_loss: 0.8691061735153198\n",
      "loss: 0.0005929470062255859, acc: 0.82421875, pred_acc: 0.7578125cossim_loss: -0.6159305572509766, cls_loss: 0.6165235042572021\n",
      "loss: -0.04544103145599365, acc: 0.84375, pred_acc: 0.7734375cossim_loss: -0.6035661697387695, cls_loss: 0.5581251382827759\n",
      "loss: 0.09867656230926514, acc: 0.8125, pred_acc: 0.7265625cossim_loss: -0.6144310235977173, cls_loss: 0.7131075859069824\n",
      "loss: -0.037581682205200195, acc: 0.828125, pred_acc: 0.74609375cossim_loss: -0.6184564828872681, cls_loss: 0.5808748006820679\n",
      "loss: 0.08118820190429688, acc: 0.8125, pred_acc: 0.765625cossim_loss: -0.6152211427688599, cls_loss: 0.6964093446731567\n",
      "loss: -0.05723273754119873, acc: 0.8125, pred_acc: 0.78515625cossim_loss: -0.6076303720474243, cls_loss: 0.5503976345062256\n",
      "loss: 0.10037499666213989, acc: 0.82421875, pred_acc: 0.8046875cossim_loss: -0.6122636198997498, cls_loss: 0.7126386165618896\n",
      "loss: -0.17453330755233765, acc: 0.87109375, pred_acc: 0.78125cossim_loss: -0.6108478307723999, cls_loss: 0.43631452322006226\n",
      "loss: -0.029272854328155518, acc: 0.84765625, pred_acc: 0.78515625cossim_loss: -0.6126207709312439, cls_loss: 0.5833479166030884\n",
      "loss: 0.06201934814453125, acc: 0.81640625, pred_acc: 0.7734375cossim_loss: -0.6170517206192017, cls_loss: 0.6790710687637329\n",
      "loss: 0.010515689849853516, acc: 0.828125, pred_acc: 0.8046875cossim_loss: -0.6086382865905762, cls_loss: 0.6191539764404297\n",
      "loss: -0.021057307720184326, acc: 0.84375, pred_acc: 0.80078125cossim_loss: -0.6150557994842529, cls_loss: 0.5939984917640686\n",
      "loss: -0.04834204912185669, acc: 0.80859375, pred_acc: 0.76171875cossim_loss: -0.6172595620155334, cls_loss: 0.5689175128936768\n",
      "loss: -0.02470982074737549, acc: 0.8359375, pred_acc: 0.78515625cossim_loss: -0.6073538064956665, cls_loss: 0.582643985748291\n",
      "loss: -0.026515066623687744, acc: 0.84375, pred_acc: 0.7421875cossim_loss: -0.6108959913253784, cls_loss: 0.5843809247016907\n",
      "loss: 0.11232554912567139, acc: 0.80859375, pred_acc: 0.77734375cossim_loss: -0.6099638938903809, cls_loss: 0.7222894430160522\n",
      "loss: 0.11847704648971558, acc: 0.8046875, pred_acc: 0.75390625cossim_loss: -0.6080406308174133, cls_loss: 0.7265176773071289\n",
      "loss: 0.07011407613754272, acc: 0.8125, pred_acc: 0.79296875cossim_loss: -0.6098047494888306, cls_loss: 0.6799188256263733\n",
      "loss: 0.03986722230911255, acc: 0.83203125, pred_acc: 0.7734375cossim_loss: -0.6225709915161133, cls_loss: 0.6624382138252258\n",
      "loss: -0.010376453399658203, acc: 0.85546875, pred_acc: 0.75390625cossim_loss: -0.6135228872299194, cls_loss: 0.6031464338302612\n",
      "loss: 0.028620600700378418, acc: 0.828125, pred_acc: 0.75cossim_loss: -0.626015305519104, cls_loss: 0.6546359062194824\n",
      "loss: 0.05865389108657837, acc: 0.80078125, pred_acc: 0.7421875cossim_loss: -0.6267868876457214, cls_loss: 0.6854407787322998\n",
      "loss: 0.07978594303131104, acc: 0.8359375, pred_acc: 0.79296875cossim_loss: -0.6154296398162842, cls_loss: 0.6952155828475952\n",
      "loss: -0.03550601005554199, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.608758270740509, cls_loss: 0.573252260684967\n",
      "loss: 0.08140063285827637, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.6027714014053345, cls_loss: 0.6841720342636108\n",
      "loss: 0.006402552127838135, acc: 0.84375, pred_acc: 0.77734375cossim_loss: -0.6110249161720276, cls_loss: 0.6174274682998657\n",
      "loss: 0.0941588282585144, acc: 0.8125, pred_acc: 0.77734375cossim_loss: -0.6087703108787537, cls_loss: 0.7029291391372681\n",
      "loss: 0.09797382354736328, acc: 0.796875, pred_acc: 0.74609375cossim_loss: -0.6197304725646973, cls_loss: 0.7177042961120605\n",
      "loss: -0.09041053056716919, acc: 0.8359375, pred_acc: 0.73046875cossim_loss: -0.6133230328559875, cls_loss: 0.5229125022888184\n",
      "loss: 0.10267883539199829, acc: 0.80859375, pred_acc: 0.75390625cossim_loss: -0.6150259971618652, cls_loss: 0.7177048325538635\n",
      "loss: -0.1490257978439331, acc: 0.8828125, pred_acc: 0.7890625cossim_loss: -0.6045798659324646, cls_loss: 0.4555540680885315\n",
      "loss: 0.042031049728393555, acc: 0.84765625, pred_acc: 0.80078125cossim_loss: -0.6029026508331299, cls_loss: 0.6449337005615234\n",
      "loss: 0.05186736583709717, acc: 0.76953125, pred_acc: 0.7578125cossim_loss: -0.622097373008728, cls_loss: 0.6739647388458252\n",
      "loss: -0.0009242892265319824, acc: 0.85546875, pred_acc: 0.7890625cossim_loss: -0.6142207384109497, cls_loss: 0.6132964491844177\n",
      "loss: -0.07539945840835571, acc: 0.8359375, pred_acc: 0.77734375cossim_loss: -0.6144055724143982, cls_loss: 0.5390061140060425\n",
      "loss: -0.0016471147537231445, acc: 0.8203125, pred_acc: 0.765625cossim_loss: -0.625744104385376, cls_loss: 0.6240969896316528\n",
      "loss: 0.16028112173080444, acc: 0.78515625, pred_acc: 0.71484375cossim_loss: -0.6223318576812744, cls_loss: 0.7826129794120789\n",
      "loss: -0.22375911474227905, acc: 0.890625, pred_acc: 0.78125cossim_loss: -0.6141855716705322, cls_loss: 0.3904264569282532\n",
      "loss: 0.23157012462615967, acc: 0.8046875, pred_acc: 0.74609375cossim_loss: -0.6146149635314941, cls_loss: 0.8461850881576538\n",
      "loss: 0.23112016916275024, acc: 0.80078125, pred_acc: 0.76953125cossim_loss: -0.6127392649650574, cls_loss: 0.8438594341278076\n",
      "loss: -0.06897866725921631, acc: 0.83203125, pred_acc: 0.7734375cossim_loss: -0.6109005212783813, cls_loss: 0.541921854019165\n",
      "loss: 0.16780519485473633, acc: 0.7734375, pred_acc: 0.71875cossim_loss: -0.6150162220001221, cls_loss: 0.7828214168548584\n",
      "loss: -0.007290184497833252, acc: 0.8359375, pred_acc: 0.78125cossim_loss: -0.6136075258255005, cls_loss: 0.6063173413276672\n",
      "loss: -0.15737199783325195, acc: 0.8515625, pred_acc: 0.8046875cossim_loss: -0.6091940402984619, cls_loss: 0.45182204246520996\n",
      "loss: 0.09703147411346436, acc: 0.8203125, pred_acc: 0.76171875cossim_loss: -0.6111798286437988, cls_loss: 0.7082113027572632\n",
      "loss: 0.05509817600250244, acc: 0.8203125, pred_acc: 0.83203125cossim_loss: -0.6167382001876831, cls_loss: 0.6718363761901855\n",
      "loss: -0.03250187635421753, acc: 0.8125, pred_acc: 0.80078125cossim_loss: -0.6186830401420593, cls_loss: 0.5861811637878418\n",
      "loss: 0.10837340354919434, acc: 0.79296875, pred_acc: 0.75390625cossim_loss: -0.6128412485122681, cls_loss: 0.7212146520614624\n",
      "loss: 0.005424797534942627, acc: 0.859375, pred_acc: 0.7734375cossim_loss: -0.6199032664299011, cls_loss: 0.6253280639648438\n",
      "loss: -0.08868122100830078, acc: 0.85546875, pred_acc: 0.76953125cossim_loss: -0.6233347654342651, cls_loss: 0.5346535444259644\n",
      "loss: -0.023693084716796875, acc: 0.83203125, pred_acc: 0.7578125cossim_loss: -0.6205273866653442, cls_loss: 0.5968343019485474\n",
      "loss: 0.003396749496459961, acc: 0.84375, pred_acc: 0.703125cossim_loss: -0.6147991418838501, cls_loss: 0.6181958913803101\n",
      "loss: 0.055905938148498535, acc: 0.80859375, pred_acc: 0.73828125cossim_loss: -0.611737847328186, cls_loss: 0.6676437854766846\n",
      "loss: -0.11699345707893372, acc: 0.83984375, pred_acc: 0.78515625cossim_loss: -0.6115742921829224, cls_loss: 0.49458083510398865\n",
      "loss: -0.04236125946044922, acc: 0.83984375, pred_acc: 0.73828125cossim_loss: -0.6114140152931213, cls_loss: 0.5690527558326721\n",
      "loss: -0.18433311581611633, acc: 0.8828125, pred_acc: 0.80859375cossim_loss: -0.6174015998840332, cls_loss: 0.43306848406791687\n",
      "loss: -0.1701543629169464, acc: 0.87890625, pred_acc: 0.78125cossim_loss: -0.6160991191864014, cls_loss: 0.44594475626945496\n",
      "loss: -0.15062987804412842, acc: 0.8203125, pred_acc: 0.765625cossim_loss: -0.6205602288246155, cls_loss: 0.46993035078048706\n",
      "loss: 0.01080542802810669, acc: 0.8359375, pred_acc: 0.75390625cossim_loss: -0.6181098222732544, cls_loss: 0.6289152503013611\n",
      "loss: -0.1437813937664032, acc: 0.86328125, pred_acc: 0.765625cossim_loss: -0.6256415843963623, cls_loss: 0.4818601906299591\n",
      "loss: -0.05243504047393799, acc: 0.86328125, pred_acc: 0.76171875cossim_loss: -0.612888514995575, cls_loss: 0.560453474521637\n",
      "loss: -0.007624030113220215, acc: 0.86328125, pred_acc: 0.7578125cossim_loss: -0.6193884611129761, cls_loss: 0.6117644309997559\n",
      "loss: -0.04743152856826782, acc: 0.8359375, pred_acc: 0.76171875cossim_loss: -0.6268728971481323, cls_loss: 0.5794413685798645\n",
      "loss: 0.02018815279006958, acc: 0.82421875, pred_acc: 0.69921875cossim_loss: -0.6198493242263794, cls_loss: 0.640037477016449\n",
      "loss: 0.005825400352478027, acc: 0.796875, pred_acc: 0.79296875cossim_loss: -0.6185985803604126, cls_loss: 0.6244239807128906\n",
      "loss: 0.1039499044418335, acc: 0.8203125, pred_acc: 0.73046875cossim_loss: -0.6147111654281616, cls_loss: 0.7186610698699951\n",
      "loss: -0.18155211210250854, acc: 0.87109375, pred_acc: 0.81640625cossim_loss: -0.6201894283294678, cls_loss: 0.43863731622695923\n",
      "loss: -0.06313902139663696, acc: 0.86328125, pred_acc: 0.7734375cossim_loss: -0.6127088069915771, cls_loss: 0.5495697855949402\n",
      "loss: -0.007427632808685303, acc: 0.83984375, pred_acc: 0.76953125cossim_loss: -0.6257854104042053, cls_loss: 0.61835777759552\n",
      "loss: -0.23089641332626343, acc: 0.8600000143051147, pred_acc: 0.7799999713897705cossim_loss: -0.6130143404006958, cls_loss: 0.3821179270744324\n",
      "loss: 0.15663069486618042, acc: 0.82421875, pred_acc: 0.7265625cossim_loss: -0.6283054947853088, cls_loss: 0.7849361896514893\n",
      "loss: -0.18295791745185852, acc: 0.87890625, pred_acc: 0.77734375cossim_loss: -0.62429279088974, cls_loss: 0.44133487343788147\n",
      "loss: 0.10228312015533447, acc: 0.8125, pred_acc: 0.76171875cossim_loss: -0.6103454828262329, cls_loss: 0.7126286029815674\n",
      "loss: -0.10217219591140747, acc: 0.87109375, pred_acc: 0.8125cossim_loss: -0.6172800660133362, cls_loss: 0.5151078701019287\n",
      "loss: -0.1933678388595581, acc: 0.875, pred_acc: 0.7734375cossim_loss: -0.62107914686203, cls_loss: 0.4277113080024719\n",
      "loss: 0.10505509376525879, acc: 0.7890625, pred_acc: 0.75cossim_loss: -0.6265531778335571, cls_loss: 0.7316082715988159\n",
      "loss: -0.051207661628723145, acc: 0.84765625, pred_acc: 0.7734375cossim_loss: -0.619971513748169, cls_loss: 0.5687638521194458\n",
      "loss: -0.26996421813964844, acc: 0.8984375, pred_acc: 0.77734375cossim_loss: -0.618644118309021, cls_loss: 0.34867990016937256\n",
      "loss: 0.07543748617172241, acc: 0.8125, pred_acc: 0.73828125cossim_loss: -0.6320465207099915, cls_loss: 0.7074840068817139\n",
      "loss: 0.11836200952529907, acc: 0.80859375, pred_acc: 0.76171875cossim_loss: -0.624850869178772, cls_loss: 0.743212878704071\n",
      "loss: 0.19502854347229004, acc: 0.78515625, pred_acc: 0.73828125cossim_loss: -0.6211596727371216, cls_loss: 0.8161882162094116\n",
      "loss: -0.05336570739746094, acc: 0.8359375, pred_acc: 0.75390625cossim_loss: -0.6286110877990723, cls_loss: 0.5752453804016113\n",
      "loss: -0.21443134546279907, acc: 0.87109375, pred_acc: 0.81640625cossim_loss: -0.6232403516769409, cls_loss: 0.40880900621414185\n",
      "loss: -0.1533779501914978, acc: 0.890625, pred_acc: 0.7734375cossim_loss: -0.6206413507461548, cls_loss: 0.467263400554657\n",
      "loss: -0.09535020589828491, acc: 0.82421875, pred_acc: 0.7421875cossim_loss: -0.6276298761367798, cls_loss: 0.5322796702384949\n",
      "loss: -0.03323549032211304, acc: 0.86328125, pred_acc: 0.76171875cossim_loss: -0.6261149048805237, cls_loss: 0.5928794145584106\n",
      "loss: -0.027097463607788086, acc: 0.85546875, pred_acc: 0.7578125cossim_loss: -0.6256499290466309, cls_loss: 0.5985524654388428\n",
      "loss: -0.07206606864929199, acc: 0.875, pred_acc: 0.75390625cossim_loss: -0.6248776912689209, cls_loss: 0.5528116226196289\n",
      "loss: -0.02507716417312622, acc: 0.84375, pred_acc: 0.734375cossim_loss: -0.6207473278045654, cls_loss: 0.5956701636314392\n",
      "loss: 0.0019508004188537598, acc: 0.84375, pred_acc: 0.75390625cossim_loss: -0.6319476962089539, cls_loss: 0.6338984966278076\n",
      "loss: -0.11782485246658325, acc: 0.82421875, pred_acc: 0.7578125cossim_loss: -0.6209638118743896, cls_loss: 0.5031389594078064\n",
      "loss: 0.04088717699050903, acc: 0.8515625, pred_acc: 0.76953125cossim_loss: -0.6166673302650452, cls_loss: 0.6575545072555542\n",
      "loss: 0.019513309001922607, acc: 0.828125, pred_acc: 0.77734375cossim_loss: -0.6243042349815369, cls_loss: 0.6438175439834595\n",
      "loss: -0.0005279779434204102, acc: 0.8359375, pred_acc: 0.734375cossim_loss: -0.6202532052993774, cls_loss: 0.619725227355957\n",
      "loss: -0.01553565263748169, acc: 0.8359375, pred_acc: 0.80859375cossim_loss: -0.614690899848938, cls_loss: 0.5991552472114563\n",
      "loss: -0.06885218620300293, acc: 0.84375, pred_acc: 0.78515625cossim_loss: -0.6313581466674805, cls_loss: 0.5625059604644775\n",
      "loss: -0.03457003831863403, acc: 0.83203125, pred_acc: 0.765625cossim_loss: -0.6291652321815491, cls_loss: 0.594595193862915\n",
      "loss: -0.0673786997795105, acc: 0.8515625, pred_acc: 0.796875cossim_loss: -0.6163636445999146, cls_loss: 0.548984944820404\n",
      "loss: -0.1464405357837677, acc: 0.8515625, pred_acc: 0.77734375cossim_loss: -0.6303811073303223, cls_loss: 0.48394057154655457\n",
      "loss: -0.14694392681121826, acc: 0.8515625, pred_acc: 0.765625cossim_loss: -0.6262593865394592, cls_loss: 0.47931545972824097\n",
      "loss: 0.03538167476654053, acc: 0.8203125, pred_acc: 0.76171875cossim_loss: -0.6225041747093201, cls_loss: 0.6578858494758606\n",
      "loss: -0.12171030044555664, acc: 0.859375, pred_acc: 0.78515625cossim_loss: -0.619743287563324, cls_loss: 0.49803298711776733\n",
      "loss: -0.10536718368530273, acc: 0.84375, pred_acc: 0.765625cossim_loss: -0.6280341148376465, cls_loss: 0.5226669311523438\n",
      "loss: 0.09159541130065918, acc: 0.82421875, pred_acc: 0.76171875cossim_loss: -0.6268486380577087, cls_loss: 0.7184440493583679\n",
      "loss: -0.12175029516220093, acc: 0.87109375, pred_acc: 0.8046875cossim_loss: -0.6259176731109619, cls_loss: 0.504167377948761\n",
      "loss: -0.020080268383026123, acc: 0.828125, pred_acc: 0.75cossim_loss: -0.6352423429489136, cls_loss: 0.6151620745658875\n",
      "loss: -0.014971315860748291, acc: 0.85546875, pred_acc: 0.73828125cossim_loss: -0.6181823015213013, cls_loss: 0.603210985660553\n",
      "loss: -0.23230087757110596, acc: 0.87890625, pred_acc: 0.7890625cossim_loss: -0.6278334856033325, cls_loss: 0.39553260803222656\n",
      "loss: 0.10184729099273682, acc: 0.8046875, pred_acc: 0.7421875cossim_loss: -0.6356257200241089, cls_loss: 0.7374730110168457\n",
      "loss: -0.07605695724487305, acc: 0.8671875, pred_acc: 0.7578125cossim_loss: -0.6219768524169922, cls_loss: 0.5459198951721191\n",
      "loss: -0.06045269966125488, acc: 0.84375, pred_acc: 0.7734375cossim_loss: -0.631060004234314, cls_loss: 0.5706073045730591\n",
      "loss: -0.04536324739456177, acc: 0.84375, pred_acc: 0.75390625cossim_loss: -0.6319676041603088, cls_loss: 0.5866043567657471\n",
      "loss: -0.04157674312591553, acc: 0.8125, pred_acc: 0.7578125cossim_loss: -0.6251709461212158, cls_loss: 0.5835942029953003\n",
      "loss: 0.09975141286849976, acc: 0.81640625, pred_acc: 0.77734375cossim_loss: -0.6382265090942383, cls_loss: 0.737977921962738\n",
      "loss: -0.07242941856384277, acc: 0.83984375, pred_acc: 0.76171875cossim_loss: -0.6257667541503906, cls_loss: 0.5533373355865479\n",
      "loss: -0.018778502941131592, acc: 0.828125, pred_acc: 0.72265625cossim_loss: -0.6181932687759399, cls_loss: 0.5994147658348083\n",
      "loss: -0.09087562561035156, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6301138401031494, cls_loss: 0.5392382144927979\n",
      "loss: 0.059056639671325684, acc: 0.83984375, pred_acc: 0.76171875cossim_loss: -0.6296513080596924, cls_loss: 0.6887079477310181\n",
      "loss: 0.016343116760253906, acc: 0.8203125, pred_acc: 0.734375cossim_loss: -0.6309694051742554, cls_loss: 0.6473125219345093\n",
      "loss: 0.06786692142486572, acc: 0.8046875, pred_acc: 0.73046875cossim_loss: -0.6276595592498779, cls_loss: 0.6955264806747437\n",
      "loss: 0.02806144952774048, acc: 0.84375, pred_acc: 0.75cossim_loss: -0.619539737701416, cls_loss: 0.6476011872291565\n",
      "loss: -0.053780555725097656, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6372487545013428, cls_loss: 0.5834681987762451\n",
      "loss: 0.12347644567489624, acc: 0.81640625, pred_acc: 0.7421875cossim_loss: -0.6322669386863708, cls_loss: 0.7557433843612671\n",
      "loss: -0.12616026401519775, acc: 0.83984375, pred_acc: 0.68359375cossim_loss: -0.63507080078125, cls_loss: 0.5089105367660522\n",
      "loss: 0.16576159000396729, acc: 0.796875, pred_acc: 0.73046875cossim_loss: -0.6271718740463257, cls_loss: 0.792933464050293\n",
      "loss: 0.021485865116119385, acc: 0.80078125, pred_acc: 0.73828125cossim_loss: -0.6397914290428162, cls_loss: 0.6612772941589355\n",
      "loss: -0.13077127933502197, acc: 0.8515625, pred_acc: 0.7890625cossim_loss: -0.6314847469329834, cls_loss: 0.5007134675979614\n",
      "loss: -0.10156911611557007, acc: 0.8515625, pred_acc: 0.76171875cossim_loss: -0.6304309964179993, cls_loss: 0.5288618803024292\n",
      "loss: -0.002598106861114502, acc: 0.8203125, pred_acc: 0.76953125cossim_loss: -0.6283175349235535, cls_loss: 0.625719428062439\n",
      "loss: 0.07001888751983643, acc: 0.79296875, pred_acc: 0.73828125cossim_loss: -0.6322841644287109, cls_loss: 0.7023030519485474\n",
      "loss: -0.027116835117340088, acc: 0.828125, pred_acc: 0.77734375cossim_loss: -0.6291424036026001, cls_loss: 0.60202556848526\n",
      "loss: -0.040457725524902344, acc: 0.8359375, pred_acc: 0.8046875cossim_loss: -0.6374721527099609, cls_loss: 0.5970144271850586\n",
      "loss: -0.09496128559112549, acc: 0.82421875, pred_acc: 0.78515625cossim_loss: -0.6296217441558838, cls_loss: 0.5346604585647583\n",
      "loss: -0.005189657211303711, acc: 0.828125, pred_acc: 0.73046875cossim_loss: -0.6292424201965332, cls_loss: 0.6240527629852295\n",
      "loss: 0.051249682903289795, acc: 0.8359375, pred_acc: 0.79296875cossim_loss: -0.6279495358467102, cls_loss: 0.67919921875\n",
      "loss: 0.07335084676742554, acc: 0.84375, pred_acc: 0.73828125cossim_loss: -0.6276379227638245, cls_loss: 0.70098876953125\n",
      "loss: 0.10786360502243042, acc: 0.83203125, pred_acc: 0.76953125cossim_loss: -0.6275304555892944, cls_loss: 0.7353940606117249\n",
      "loss: -0.04665017127990723, acc: 0.81640625, pred_acc: 0.78515625cossim_loss: -0.6406434774398804, cls_loss: 0.5939933061599731\n",
      "loss: -0.022070109844207764, acc: 0.8203125, pred_acc: 0.73828125cossim_loss: -0.6321567296981812, cls_loss: 0.6100866198539734\n",
      "loss: -0.08474433422088623, acc: 0.859375, pred_acc: 0.80078125cossim_loss: -0.6259479522705078, cls_loss: 0.5412036180496216\n",
      "loss: -0.14891648292541504, acc: 0.84765625, pred_acc: 0.82421875cossim_loss: -0.6212165355682373, cls_loss: 0.47230005264282227\n",
      "loss: 0.19440370798110962, acc: 0.79296875, pred_acc: 0.80078125cossim_loss: -0.6371431946754456, cls_loss: 0.8315469026565552\n",
      "loss: -0.015700042247772217, acc: 0.828125, pred_acc: 0.7578125cossim_loss: -0.6310909986495972, cls_loss: 0.615390956401825\n",
      "loss: -0.0644233226776123, acc: 0.83203125, pred_acc: 0.76953125cossim_loss: -0.6307245492935181, cls_loss: 0.5663012266159058\n",
      "loss: 0.0636335015296936, acc: 0.8046875, pred_acc: 0.72265625cossim_loss: -0.6241297125816345, cls_loss: 0.6877632141113281\n",
      "loss: -0.07612800598144531, acc: 0.84765625, pred_acc: 0.79296875cossim_loss: -0.6307145357131958, cls_loss: 0.5545865297317505\n",
      "loss: -0.04004991054534912, acc: 0.82421875, pred_acc: 0.7734375cossim_loss: -0.6282910108566284, cls_loss: 0.5882411003112793\n",
      "loss: -0.08548688888549805, acc: 0.8515625, pred_acc: 0.76953125cossim_loss: -0.6390542984008789, cls_loss: 0.5535674095153809\n",
      "loss: -0.03892838954925537, acc: 0.8203125, pred_acc: 0.77734375cossim_loss: -0.6311230659484863, cls_loss: 0.592194676399231\n",
      "loss: 0.012347042560577393, acc: 0.80078125, pred_acc: 0.7578125cossim_loss: -0.6297503113746643, cls_loss: 0.6420973539352417\n",
      "loss: -0.10661357641220093, acc: 0.83203125, pred_acc: 0.7578125cossim_loss: -0.6325044631958008, cls_loss: 0.5258908867835999\n",
      "loss: 0.029814600944519043, acc: 0.79296875, pred_acc: 0.76953125cossim_loss: -0.6374393105506897, cls_loss: 0.6672539114952087\n",
      "loss: 0.015900254249572754, acc: 0.7890625, pred_acc: 0.76953125cossim_loss: -0.631162166595459, cls_loss: 0.6470624208450317\n",
      "loss: 0.029412508010864258, acc: 0.84375, pred_acc: 0.74609375cossim_loss: -0.6338740587234497, cls_loss: 0.663286566734314\n",
      "loss: 0.09713143110275269, acc: 0.7890625, pred_acc: 0.76171875cossim_loss: -0.6292172074317932, cls_loss: 0.7263486385345459\n",
      "loss: -0.04131990671157837, acc: 0.84765625, pred_acc: 0.7578125cossim_loss: -0.6376908421516418, cls_loss: 0.5963709354400635\n",
      "loss: 0.05568432807922363, acc: 0.8203125, pred_acc: 0.7421875cossim_loss: -0.6221396923065186, cls_loss: 0.6778240203857422\n",
      "loss: -0.09969854354858398, acc: 0.85546875, pred_acc: 0.75cossim_loss: -0.6348264217376709, cls_loss: 0.5351278781890869\n",
      "loss: 0.05526137351989746, acc: 0.83203125, pred_acc: 0.75cossim_loss: -0.6284645795822144, cls_loss: 0.6837259531021118\n",
      "loss: -0.18530553579330444, acc: 0.87109375, pred_acc: 0.765625cossim_loss: -0.6251391172409058, cls_loss: 0.4398335814476013\n",
      "loss: -0.04159587621688843, acc: 0.80078125, pred_acc: 0.76171875cossim_loss: -0.6244073510169983, cls_loss: 0.5828114748001099\n",
      "loss: -0.14058232307434082, acc: 0.8671875, pred_acc: 0.765625cossim_loss: -0.6435538530349731, cls_loss: 0.5029715299606323\n",
      "loss: -0.02716761827468872, acc: 0.8671875, pred_acc: 0.7734375cossim_loss: -0.6235055327415466, cls_loss: 0.5963379144668579\n",
      "loss: 0.005128681659698486, acc: 0.83984375, pred_acc: 0.76953125cossim_loss: -0.6292803883552551, cls_loss: 0.6344090700149536\n",
      "loss: 0.0543251633644104, acc: 0.828125, pred_acc: 0.7421875cossim_loss: -0.634801983833313, cls_loss: 0.6891271471977234\n",
      "loss: -0.11470162868499756, acc: 0.87109375, pred_acc: 0.74609375cossim_loss: -0.6392128467559814, cls_loss: 0.5245112180709839\n",
      "loss: -0.0980944037437439, acc: 0.83984375, pred_acc: 0.78125cossim_loss: -0.6388177871704102, cls_loss: 0.5407233834266663\n",
      "loss: 0.07964205741882324, acc: 0.79296875, pred_acc: 0.71484375cossim_loss: -0.6365616321563721, cls_loss: 0.7162036895751953\n",
      "loss: -0.21195852756500244, acc: 0.859375, pred_acc: 0.7890625cossim_loss: -0.635729968547821, cls_loss: 0.4237714409828186\n",
      "loss: -0.138088196516037, acc: 0.859375, pred_acc: 0.7578125cossim_loss: -0.6296554803848267, cls_loss: 0.4915672838687897\n",
      "loss: 0.01003497838973999, acc: 0.8359375, pred_acc: 0.765625cossim_loss: -0.6341920495033264, cls_loss: 0.6442270278930664\n",
      "loss: -0.08577948808670044, acc: 0.82421875, pred_acc: 0.76953125cossim_loss: -0.6405465602874756, cls_loss: 0.5547670722007751\n",
      "loss: -0.16334199905395508, acc: 0.84375, pred_acc: 0.75cossim_loss: -0.6288642883300781, cls_loss: 0.46552228927612305\n",
      "loss: -0.0007143020629882812, acc: 0.8203125, pred_acc: 0.7734375cossim_loss: -0.6290440559387207, cls_loss: 0.6283297538757324\n",
      "loss: 0.0920533537864685, acc: 0.79296875, pred_acc: 0.703125cossim_loss: -0.6430280208587646, cls_loss: 0.7350813746452332\n",
      "loss: -0.0382685661315918, acc: 0.8125, pred_acc: 0.75390625cossim_loss: -0.6448074579238892, cls_loss: 0.6065388917922974\n",
      "loss: -0.08486557006835938, acc: 0.8359375, pred_acc: 0.7421875cossim_loss: -0.6330467462539673, cls_loss: 0.5481811761856079\n",
      "loss: 0.005917191505432129, acc: 0.83984375, pred_acc: 0.7890625cossim_loss: -0.626653790473938, cls_loss: 0.6325709819793701\n",
      "loss: -0.16087669134140015, acc: 0.84765625, pred_acc: 0.796875cossim_loss: -0.63644939661026, cls_loss: 0.47557270526885986\n",
      "loss: -0.16808229684829712, acc: 0.875, pred_acc: 0.74609375cossim_loss: -0.6370772123336792, cls_loss: 0.4689949154853821\n",
      "loss: -0.17999160289764404, acc: 0.8515625, pred_acc: 0.7265625cossim_loss: -0.6332387924194336, cls_loss: 0.45324718952178955\n",
      "loss: -0.16392961144447327, acc: 0.875, pred_acc: 0.796875cossim_loss: -0.6302758455276489, cls_loss: 0.46634623408317566\n",
      "loss: -0.0021271705627441406, acc: 0.8046875, pred_acc: 0.80078125cossim_loss: -0.6252602934837341, cls_loss: 0.62313312292099\n",
      "loss: -0.006224691867828369, acc: 0.83984375, pred_acc: 0.77734375cossim_loss: -0.6350765824317932, cls_loss: 0.6288518905639648\n",
      "loss: -0.18077638745307922, acc: 0.84375, pred_acc: 0.79296875cossim_loss: -0.6298106908798218, cls_loss: 0.44903430342674255\n",
      "loss: -0.016342103481292725, acc: 0.83203125, pred_acc: 0.7890625cossim_loss: -0.6276642680168152, cls_loss: 0.6113221645355225\n",
      "loss: 0.07120853662490845, acc: 0.84375, pred_acc: 0.765625cossim_loss: -0.6338843703269958, cls_loss: 0.7050929069519043\n",
      "loss: -0.05317413806915283, acc: 0.82421875, pred_acc: 0.78125cossim_loss: -0.6319471597671509, cls_loss: 0.578773021697998\n",
      "loss: -0.08938181400299072, acc: 0.86328125, pred_acc: 0.79296875cossim_loss: -0.631037712097168, cls_loss: 0.5416558980941772\n",
      "loss: -0.1555776298046112, acc: 0.85546875, pred_acc: 0.7578125cossim_loss: -0.6381459832191467, cls_loss: 0.4825683534145355\n",
      "loss: 0.02779221534729004, acc: 0.8203125, pred_acc: 0.75cossim_loss: -0.6305205821990967, cls_loss: 0.6583127975463867\n",
      "loss: 0.12664097547531128, acc: 0.7890625, pred_acc: 0.73046875cossim_loss: -0.6341158151626587, cls_loss: 0.76075679063797\n",
      "loss: -0.12284183502197266, acc: 0.859375, pred_acc: 0.75cossim_loss: -0.6347744464874268, cls_loss: 0.5119326114654541\n",
      "loss: 0.06572854518890381, acc: 0.85546875, pred_acc: 0.76171875cossim_loss: -0.6258687376976013, cls_loss: 0.6915972828865051\n",
      "loss: -0.11541295051574707, acc: 0.859375, pred_acc: 0.78125cossim_loss: -0.6237808465957642, cls_loss: 0.5083678960800171\n",
      "loss: 0.03789794445037842, acc: 0.83203125, pred_acc: 0.7578125cossim_loss: -0.642467737197876, cls_loss: 0.6803656816482544\n",
      "loss: 0.0018745064735412598, acc: 0.82421875, pred_acc: 0.72265625cossim_loss: -0.632646918296814, cls_loss: 0.6345214247703552\n",
      "loss: 0.012298822402954102, acc: 0.8359375, pred_acc: 0.765625cossim_loss: -0.6310772895812988, cls_loss: 0.6433761119842529\n",
      "loss: -0.03810727596282959, acc: 0.83203125, pred_acc: 0.77734375cossim_loss: -0.6381863355636597, cls_loss: 0.6000790596008301\n",
      "loss: -0.04114186763763428, acc: 0.83203125, pred_acc: 0.77734375cossim_loss: -0.626024603843689, cls_loss: 0.5848827362060547\n",
      "loss: 0.1403610110282898, acc: 0.796875, pred_acc: 0.734375cossim_loss: -0.6313262581825256, cls_loss: 0.7716872692108154\n",
      "loss: -0.01234060525894165, acc: 0.828125, pred_acc: 0.72265625cossim_loss: -0.6328030824661255, cls_loss: 0.6204624772071838\n",
      "loss: -0.13816851377487183, acc: 0.88671875, pred_acc: 0.79296875cossim_loss: -0.6333680748939514, cls_loss: 0.4951995611190796\n",
      "loss: -0.05409139394760132, acc: 0.83203125, pred_acc: 0.796875cossim_loss: -0.6309205293655396, cls_loss: 0.5768291354179382\n",
      "loss: -0.0718228816986084, acc: 0.83984375, pred_acc: 0.796875cossim_loss: -0.6192686557769775, cls_loss: 0.5474457740783691\n",
      "loss: -0.10198283195495605, acc: 0.87109375, pred_acc: 0.7890625cossim_loss: -0.6248160600662231, cls_loss: 0.5228332281112671\n",
      "loss: 0.12652349472045898, acc: 0.8046875, pred_acc: 0.7265625cossim_loss: -0.6383589506149292, cls_loss: 0.7648824453353882\n",
      "loss: -0.00848323106765747, acc: 0.84765625, pred_acc: 0.81640625cossim_loss: -0.6230030059814453, cls_loss: 0.6145197749137878\n",
      "loss: -0.27386337518692017, acc: 0.88671875, pred_acc: 0.828125cossim_loss: -0.6418811082839966, cls_loss: 0.3680177330970764\n",
      "loss: -0.15348386764526367, acc: 0.86328125, pred_acc: 0.76953125cossim_loss: -0.6309961080551147, cls_loss: 0.4775122404098511\n",
      "loss: 0.009427547454833984, acc: 0.796875, pred_acc: 0.734375cossim_loss: -0.6353915929794312, cls_loss: 0.6448191404342651\n",
      "loss: -0.07943427562713623, acc: 0.8671875, pred_acc: 0.78515625cossim_loss: -0.6207255125045776, cls_loss: 0.5412912368774414\n",
      "loss: -0.03347635269165039, acc: 0.83203125, pred_acc: 0.71875cossim_loss: -0.6303576231002808, cls_loss: 0.5968812704086304\n",
      "loss: -0.1584452986717224, acc: 0.8359375, pred_acc: 0.76171875cossim_loss: -0.6386011242866516, cls_loss: 0.4801558256149292\n",
      "loss: -0.1180984377861023, acc: 0.87109375, pred_acc: 0.76171875cossim_loss: -0.6339713335037231, cls_loss: 0.5158728957176208\n",
      "loss: -0.11998021602630615, acc: 0.84375, pred_acc: 0.7734375cossim_loss: -0.6336942911148071, cls_loss: 0.513714075088501\n",
      "loss: 0.021901488304138184, acc: 0.81640625, pred_acc: 0.7578125cossim_loss: -0.6336543560028076, cls_loss: 0.6555558443069458\n",
      "loss: -0.138849675655365, acc: 0.87109375, pred_acc: 0.78125cossim_loss: -0.6267378330230713, cls_loss: 0.4878881573677063\n",
      "loss: -0.08260178565979004, acc: 0.83984375, pred_acc: 0.78125cossim_loss: -0.6249099969863892, cls_loss: 0.5423082113265991\n",
      "loss: 0.08044850826263428, acc: 0.80078125, pred_acc: 0.75cossim_loss: -0.6323491334915161, cls_loss: 0.7127976417541504\n",
      "loss: -0.006358623504638672, acc: 0.828125, pred_acc: 0.77734375cossim_loss: -0.6381243467330933, cls_loss: 0.6317657232284546\n",
      "loss: -0.06172764301300049, acc: 0.83203125, pred_acc: 0.765625cossim_loss: -0.6296941041946411, cls_loss: 0.5679664611816406\n",
      "loss: -0.11654782295227051, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6316299438476562, cls_loss: 0.5150821208953857\n",
      "loss: -0.12896251678466797, acc: 0.8515625, pred_acc: 0.78515625cossim_loss: -0.6257470846176147, cls_loss: 0.4967845678329468\n",
      "loss: -0.030909180641174316, acc: 0.83984375, pred_acc: 0.76171875cossim_loss: -0.6280531883239746, cls_loss: 0.5971440076828003\n",
      "loss: -0.13627219200134277, acc: 0.875, pred_acc: 0.78515625cossim_loss: -0.6225816011428833, cls_loss: 0.4863094091415405\n",
      "loss: 0.06591492891311646, acc: 0.7890625, pred_acc: 0.6953125cossim_loss: -0.6397228240966797, cls_loss: 0.7056377530097961\n",
      "loss: -0.18999218940734863, acc: 0.86328125, pred_acc: 0.78515625cossim_loss: -0.6353869438171387, cls_loss: 0.44539475440979004\n",
      "loss: -0.06471937894821167, acc: 0.84375, pred_acc: 0.7421875cossim_loss: -0.6381293535232544, cls_loss: 0.5734099745750427\n",
      "loss: -0.06853914260864258, acc: 0.84375, pred_acc: 0.7734375cossim_loss: -0.6410613059997559, cls_loss: 0.5725221633911133\n",
      "loss: 0.0414922833442688, acc: 0.8203125, pred_acc: 0.75cossim_loss: -0.6392406821250916, cls_loss: 0.6807329654693604\n",
      "loss: -0.03684955835342407, acc: 0.84765625, pred_acc: 0.76953125cossim_loss: -0.6322564482688904, cls_loss: 0.5954068899154663\n",
      "loss: 0.07170408964157104, acc: 0.796875, pred_acc: 0.75cossim_loss: -0.6317229270935059, cls_loss: 0.7034270167350769\n",
      "loss: -0.16833505034446716, acc: 0.8671875, pred_acc: 0.7890625cossim_loss: -0.6231197118759155, cls_loss: 0.45478466153144836\n",
      "loss: -0.031761229038238525, acc: 0.81640625, pred_acc: 0.7421875cossim_loss: -0.64021897315979, cls_loss: 0.6084577441215515\n",
      "loss: 0.1216784119606018, acc: 0.8046875, pred_acc: 0.77734375cossim_loss: -0.6373703479766846, cls_loss: 0.7590487599372864\n",
      "loss: -0.18948894739151, acc: 0.8828125, pred_acc: 0.8203125cossim_loss: -0.6208751201629639, cls_loss: 0.43138617277145386\n",
      "loss: -0.09139078855514526, acc: 0.859375, pred_acc: 0.7578125cossim_loss: -0.6294905543327332, cls_loss: 0.5380997657775879\n",
      "loss: 0.06718593835830688, acc: 0.82421875, pred_acc: 0.78515625cossim_loss: -0.6348105669021606, cls_loss: 0.7019965052604675\n",
      "loss: -0.15987038612365723, acc: 0.87890625, pred_acc: 0.80859375cossim_loss: -0.6324189901351929, cls_loss: 0.47254860401153564\n",
      "loss: 0.09902745485305786, acc: 0.83203125, pred_acc: 0.77734375cossim_loss: -0.6358427405357361, cls_loss: 0.734870195388794\n",
      "loss: -0.06758004426956177, acc: 0.8515625, pred_acc: 0.80859375cossim_loss: -0.639883816242218, cls_loss: 0.5723037719726562\n",
      "loss: -0.055767178535461426, acc: 0.8359375, pred_acc: 0.76953125cossim_loss: -0.6282615661621094, cls_loss: 0.572494387626648\n",
      "loss: -0.025757133960723877, acc: 0.83203125, pred_acc: 0.75390625cossim_loss: -0.6380541920661926, cls_loss: 0.6122970581054688\n",
      "loss: -0.24946096539497375, acc: 0.875, pred_acc: 0.78515625cossim_loss: -0.6349509954452515, cls_loss: 0.3854900300502777\n",
      "loss: -0.18682897090911865, acc: 0.8700000047683716, pred_acc: 0.8299999833106995cossim_loss: -0.6250295042991638, cls_loss: 0.43820053339004517\n",
      "loss: 0.02178281545639038, acc: 0.83203125, pred_acc: 0.74609375cossim_loss: -0.6401345133781433, cls_loss: 0.6619173288345337\n",
      "loss: -0.11926287412643433, acc: 0.875, pred_acc: 0.75390625cossim_loss: -0.629663348197937, cls_loss: 0.5104004740715027\n",
      "loss: -0.09696906805038452, acc: 0.85546875, pred_acc: 0.80859375cossim_loss: -0.632470965385437, cls_loss: 0.5355018973350525\n",
      "loss: -0.13618189096450806, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6305272579193115, cls_loss: 0.49434536695480347\n",
      "loss: -0.18108460307121277, acc: 0.875, pred_acc: 0.79296875cossim_loss: -0.6264135837554932, cls_loss: 0.4453289806842804\n",
      "loss: -0.07343035936355591, acc: 0.86328125, pred_acc: 0.79296875cossim_loss: -0.6374994516372681, cls_loss: 0.5640690922737122\n",
      "loss: -0.16195020079612732, acc: 0.87890625, pred_acc: 0.77734375cossim_loss: -0.6305626630783081, cls_loss: 0.4686124622821808\n",
      "loss: -0.06443482637405396, acc: 0.85546875, pred_acc: 0.75cossim_loss: -0.629997730255127, cls_loss: 0.565562903881073\n",
      "loss: -0.0678168535232544, acc: 0.84375, pred_acc: 0.74609375cossim_loss: -0.6403557062149048, cls_loss: 0.5725388526916504\n",
      "loss: -0.10009002685546875, acc: 0.8515625, pred_acc: 0.765625cossim_loss: -0.6331992745399475, cls_loss: 0.5331092476844788\n",
      "loss: -0.048985958099365234, acc: 0.8359375, pred_acc: 0.76953125cossim_loss: -0.6318565607070923, cls_loss: 0.582870602607727\n",
      "loss: -0.11847805976867676, acc: 0.84375, pred_acc: 0.76171875cossim_loss: -0.6347509622573853, cls_loss: 0.5162729024887085\n",
      "loss: 0.04396241903305054, acc: 0.80859375, pred_acc: 0.734375cossim_loss: -0.6393304467201233, cls_loss: 0.6832928657531738\n",
      "loss: -0.15661004185676575, acc: 0.859375, pred_acc: 0.73828125cossim_loss: -0.637252688407898, cls_loss: 0.4806426465511322\n",
      "loss: 0.03301060199737549, acc: 0.8203125, pred_acc: 0.734375cossim_loss: -0.6348776817321777, cls_loss: 0.6678882837295532\n",
      "loss: -0.09765565395355225, acc: 0.859375, pred_acc: 0.80078125cossim_loss: -0.6327253580093384, cls_loss: 0.5350697040557861\n",
      "loss: -0.06949704885482788, acc: 0.83984375, pred_acc: 0.76953125cossim_loss: -0.6345704793930054, cls_loss: 0.5650734305381775\n",
      "loss: -0.07761073112487793, acc: 0.84375, pred_acc: 0.78125cossim_loss: -0.6396123170852661, cls_loss: 0.5620015859603882\n",
      "loss: -0.09950298070907593, acc: 0.83984375, pred_acc: 0.765625cossim_loss: -0.6444995999336243, cls_loss: 0.5449966192245483\n",
      "loss: -0.0659525990486145, acc: 0.83984375, pred_acc: 0.734375cossim_loss: -0.630074143409729, cls_loss: 0.5641215443611145\n",
      "loss: -0.055878639221191406, acc: 0.83203125, pred_acc: 0.80078125cossim_loss: -0.6433905363082886, cls_loss: 0.5875118970870972\n",
      "loss: -0.0022739768028259277, acc: 0.8046875, pred_acc: 0.7734375cossim_loss: -0.6398374438285828, cls_loss: 0.6375634670257568\n",
      "loss: -0.0615307092666626, acc: 0.83984375, pred_acc: 0.76171875cossim_loss: -0.6281509399414062, cls_loss: 0.5666202306747437\n",
      "loss: 0.034166693687438965, acc: 0.83203125, pred_acc: 0.734375cossim_loss: -0.6396605968475342, cls_loss: 0.6738272905349731\n",
      "loss: 0.03643012046813965, acc: 0.8359375, pred_acc: 0.75cossim_loss: -0.6325150728225708, cls_loss: 0.6689451932907104\n",
      "loss: -0.09473192691802979, acc: 0.8359375, pred_acc: 0.77734375cossim_loss: -0.6379783153533936, cls_loss: 0.5432463884353638\n",
      "loss: -0.13199695944786072, acc: 0.84765625, pred_acc: 0.78515625cossim_loss: -0.6285184025764465, cls_loss: 0.4965214431285858\n",
      "loss: -0.1530117392539978, acc: 0.87109375, pred_acc: 0.76171875cossim_loss: -0.6406035423278809, cls_loss: 0.48759180307388306\n",
      "loss: 0.005780220031738281, acc: 0.8203125, pred_acc: 0.77734375cossim_loss: -0.6332669854164124, cls_loss: 0.6390472054481506\n",
      "loss: -0.16888931393623352, acc: 0.875, pred_acc: 0.765625cossim_loss: -0.6306824088096619, cls_loss: 0.46179309487342834\n",
      "loss: -0.04375183582305908, acc: 0.85546875, pred_acc: 0.765625cossim_loss: -0.6294494867324829, cls_loss: 0.5856976509094238\n",
      "loss: -0.06418824195861816, acc: 0.83203125, pred_acc: 0.75390625cossim_loss: -0.6422125101089478, cls_loss: 0.5780242681503296\n",
      "loss: -0.25896304845809937, acc: 0.8671875, pred_acc: 0.7890625cossim_loss: -0.6366716623306274, cls_loss: 0.3777086138725281\n",
      "loss: -0.17961335182189941, acc: 0.8671875, pred_acc: 0.78515625cossim_loss: -0.6488738059997559, cls_loss: 0.46926045417785645\n",
      "loss: -0.12786483764648438, acc: 0.84375, pred_acc: 0.7890625cossim_loss: -0.6313211917877197, cls_loss: 0.5034563541412354\n",
      "loss: -0.045037150382995605, acc: 0.8515625, pred_acc: 0.796875cossim_loss: -0.6401044130325317, cls_loss: 0.5950672626495361\n",
      "loss: -0.09195941686630249, acc: 0.83984375, pred_acc: 0.7734375cossim_loss: -0.6419341564178467, cls_loss: 0.5499747395515442\n",
      "loss: 0.14763545989990234, acc: 0.7890625, pred_acc: 0.7578125cossim_loss: -0.6373555064201355, cls_loss: 0.7849909663200378\n",
      "loss: -0.11241930723190308, acc: 0.84375, pred_acc: 0.75390625cossim_loss: -0.6367880702018738, cls_loss: 0.5243687629699707\n",
      "loss: -0.08445245027542114, acc: 0.84375, pred_acc: 0.7421875cossim_loss: -0.6358660459518433, cls_loss: 0.5514135956764221\n",
      "loss: -0.014644980430603027, acc: 0.83984375, pred_acc: 0.73828125cossim_loss: -0.640479564666748, cls_loss: 0.625834584236145\n",
      "loss: 0.07258981466293335, acc: 0.81640625, pred_acc: 0.7421875cossim_loss: -0.6320257186889648, cls_loss: 0.7046155333518982\n",
      "loss: -0.032285451889038086, acc: 0.83203125, pred_acc: 0.80078125cossim_loss: -0.6417118310928345, cls_loss: 0.6094263792037964\n",
      "loss: -0.20842322707176208, acc: 0.88671875, pred_acc: 0.7421875cossim_loss: -0.6462903022766113, cls_loss: 0.43786707520484924\n",
      "loss: -0.15237760543823242, acc: 0.859375, pred_acc: 0.79296875cossim_loss: -0.6449141502380371, cls_loss: 0.4925365447998047\n",
      "loss: -0.07202154397964478, acc: 0.84765625, pred_acc: 0.7890625cossim_loss: -0.6528875231742859, cls_loss: 0.5808659791946411\n",
      "loss: 0.16900157928466797, acc: 0.8125, pred_acc: 0.76171875cossim_loss: -0.6465355157852173, cls_loss: 0.8155370950698853\n",
      "loss: -0.06680434942245483, acc: 0.8359375, pred_acc: 0.77734375cossim_loss: -0.6430515646934509, cls_loss: 0.5762472152709961\n",
      "loss: -0.21951770782470703, acc: 0.87890625, pred_acc: 0.79296875cossim_loss: -0.635838508605957, cls_loss: 0.41632080078125\n",
      "loss: -0.14935582876205444, acc: 0.85546875, pred_acc: 0.79296875cossim_loss: -0.634270966053009, cls_loss: 0.4849151372909546\n",
      "loss: 0.03937888145446777, acc: 0.80859375, pred_acc: 0.76953125cossim_loss: -0.6321812868118286, cls_loss: 0.6715601682662964\n",
      "loss: -0.09120750427246094, acc: 0.87109375, pred_acc: 0.8125cossim_loss: -0.6353540420532227, cls_loss: 0.5441465377807617\n",
      "loss: 0.001833498477935791, acc: 0.83203125, pred_acc: 0.76953125cossim_loss: -0.6407372355461121, cls_loss: 0.6425707340240479\n",
      "loss: -0.07547008991241455, acc: 0.86328125, pred_acc: 0.73828125cossim_loss: -0.639357328414917, cls_loss: 0.5638872385025024\n",
      "loss: -0.09507173299789429, acc: 0.84765625, pred_acc: 0.73828125cossim_loss: -0.6416482925415039, cls_loss: 0.5465765595436096\n",
      "loss: -0.019670605659484863, acc: 0.83203125, pred_acc: 0.74609375cossim_loss: -0.642333984375, cls_loss: 0.6226633787155151\n",
      "loss: 0.16658133268356323, acc: 0.80078125, pred_acc: 0.73046875cossim_loss: -0.6411667466163635, cls_loss: 0.8077480792999268\n",
      "loss: -0.023818254470825195, acc: 0.859375, pred_acc: 0.7578125cossim_loss: -0.6350551843643188, cls_loss: 0.6112369298934937\n",
      "loss: -0.11150681972503662, acc: 0.85546875, pred_acc: 0.8046875cossim_loss: -0.6365036964416504, cls_loss: 0.5249968767166138\n",
      "loss: -0.09660530090332031, acc: 0.84765625, pred_acc: 0.7734375cossim_loss: -0.6419601440429688, cls_loss: 0.5453548431396484\n",
      "loss: -0.09833377599716187, acc: 0.828125, pred_acc: 0.734375cossim_loss: -0.6410751938819885, cls_loss: 0.5427414178848267\n",
      "loss: 0.02787911891937256, acc: 0.83203125, pred_acc: 0.75cossim_loss: -0.6445193290710449, cls_loss: 0.6723984479904175\n",
      "loss: 0.034221768379211426, acc: 0.83984375, pred_acc: 0.6953125cossim_loss: -0.6223090887069702, cls_loss: 0.6565308570861816\n",
      "loss: -0.022818922996520996, acc: 0.83984375, pred_acc: 0.734375cossim_loss: -0.6329433917999268, cls_loss: 0.6101244688034058\n",
      "loss: -0.125515878200531, acc: 0.875, pred_acc: 0.74609375cossim_loss: -0.631745457649231, cls_loss: 0.5062295794487\n",
      "loss: -0.15102988481521606, acc: 0.85546875, pred_acc: 0.765625cossim_loss: -0.6355836391448975, cls_loss: 0.4845537543296814\n",
      "loss: -0.010469436645507812, acc: 0.859375, pred_acc: 0.7890625cossim_loss: -0.6337788701057434, cls_loss: 0.6233094334602356\n",
      "loss: -0.11011910438537598, acc: 0.86328125, pred_acc: 0.7734375cossim_loss: -0.6504998803138733, cls_loss: 0.5403807759284973\n",
      "loss: 0.048967063426971436, acc: 0.8125, pred_acc: 0.73046875cossim_loss: -0.6374632120132446, cls_loss: 0.6864302754402161\n",
      "loss: 0.05187821388244629, acc: 0.8046875, pred_acc: 0.73828125cossim_loss: -0.6416443586349487, cls_loss: 0.693522572517395\n",
      "loss: -0.19137150049209595, acc: 0.875, pred_acc: 0.7734375cossim_loss: -0.6358902454376221, cls_loss: 0.4445187449455261\n",
      "loss: -0.02037954330444336, acc: 0.82421875, pred_acc: 0.73828125cossim_loss: -0.6451840400695801, cls_loss: 0.6248044967651367\n",
      "loss: -0.03799039125442505, acc: 0.828125, pred_acc: 0.78515625cossim_loss: -0.6459285020828247, cls_loss: 0.6079381108283997\n",
      "loss: 0.05701166391372681, acc: 0.8203125, pred_acc: 0.78125cossim_loss: -0.6310631632804871, cls_loss: 0.6880748271942139\n",
      "loss: -0.1356833577156067, acc: 0.8671875, pred_acc: 0.77734375cossim_loss: -0.6369901895523071, cls_loss: 0.5013068318367004\n",
      "loss: -0.12276017665863037, acc: 0.84765625, pred_acc: 0.7578125cossim_loss: -0.6368138790130615, cls_loss: 0.5140537023544312\n",
      "loss: -0.06018662452697754, acc: 0.83203125, pred_acc: 0.7421875cossim_loss: -0.6358264684677124, cls_loss: 0.5756398439407349\n",
      "loss: -0.13508212566375732, acc: 0.84375, pred_acc: 0.76171875cossim_loss: -0.6388823986053467, cls_loss: 0.5038002729415894\n",
      "loss: 0.006232559680938721, acc: 0.8125, pred_acc: 0.703125cossim_loss: -0.638155460357666, cls_loss: 0.6443880200386047\n",
      "loss: -0.06948232650756836, acc: 0.83984375, pred_acc: 0.76953125cossim_loss: -0.6407380700111389, cls_loss: 0.5712557435035706\n",
      "loss: -0.14974111318588257, acc: 0.86328125, pred_acc: 0.7265625cossim_loss: -0.6407811045646667, cls_loss: 0.4910399913787842\n",
      "loss: -0.06279194355010986, acc: 0.83203125, pred_acc: 0.78125cossim_loss: -0.6330265998840332, cls_loss: 0.5702346563339233\n",
      "loss: -0.017565608024597168, acc: 0.84375, pred_acc: 0.76953125cossim_loss: -0.6347647905349731, cls_loss: 0.617199182510376\n",
      "loss: -0.1252574920654297, acc: 0.8671875, pred_acc: 0.75390625cossim_loss: -0.6458615660667419, cls_loss: 0.5206040740013123\n",
      "loss: 0.029552459716796875, acc: 0.84765625, pred_acc: 0.82421875cossim_loss: -0.6382225751876831, cls_loss: 0.66777503490448\n",
      "loss: -0.054670512676239014, acc: 0.828125, pred_acc: 0.765625cossim_loss: -0.6357513070106506, cls_loss: 0.5810807943344116\n",
      "loss: -0.09723520278930664, acc: 0.8515625, pred_acc: 0.7890625cossim_loss: -0.6339927911758423, cls_loss: 0.5367575883865356\n",
      "loss: -0.0423579216003418, acc: 0.8359375, pred_acc: 0.7109375cossim_loss: -0.6337549686431885, cls_loss: 0.5913970470428467\n",
      "loss: 0.12943756580352783, acc: 0.83203125, pred_acc: 0.73046875cossim_loss: -0.6389230489730835, cls_loss: 0.7683606147766113\n",
      "loss: -0.1380605697631836, acc: 0.87890625, pred_acc: 0.74609375cossim_loss: -0.6493236422538757, cls_loss: 0.5112630724906921\n",
      "loss: -0.03978341817855835, acc: 0.83203125, pred_acc: 0.70703125cossim_loss: -0.6378809809684753, cls_loss: 0.598097562789917\n",
      "loss: -0.06562697887420654, acc: 0.84765625, pred_acc: 0.8046875cossim_loss: -0.6351253986358643, cls_loss: 0.5694984197616577\n",
      "loss: -0.08023524284362793, acc: 0.82421875, pred_acc: 0.76171875cossim_loss: -0.6388117671012878, cls_loss: 0.5585765242576599\n",
      "loss: -0.15208205580711365, acc: 0.8671875, pred_acc: 0.7734375cossim_loss: -0.6390275955200195, cls_loss: 0.4869455397129059\n",
      "loss: 0.05712920427322388, acc: 0.79296875, pred_acc: 0.7578125cossim_loss: -0.6408342719078064, cls_loss: 0.6979634761810303\n",
      "loss: -0.16070252656936646, acc: 0.85546875, pred_acc: 0.75390625cossim_loss: -0.6369439363479614, cls_loss: 0.47624140977859497\n",
      "loss: -0.08607953786849976, acc: 0.828125, pred_acc: 0.78125cossim_loss: -0.6362922191619873, cls_loss: 0.5502126812934875\n",
      "loss: -0.03497004508972168, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.6329790353775024, cls_loss: 0.5980089902877808\n",
      "loss: -0.1676110327243805, acc: 0.87890625, pred_acc: 0.77734375cossim_loss: -0.6266926527023315, cls_loss: 0.45908161997795105\n",
      "loss: 0.052455246448516846, acc: 0.8359375, pred_acc: 0.74609375cossim_loss: -0.6378604173660278, cls_loss: 0.6903156638145447\n",
      "loss: -0.06763964891433716, acc: 0.82421875, pred_acc: 0.75cossim_loss: -0.6309777498245239, cls_loss: 0.5633381009101868\n",
      "loss: 0.10914969444274902, acc: 0.82421875, pred_acc: 0.73828125cossim_loss: -0.6332188844680786, cls_loss: 0.7423685789108276\n",
      "loss: -0.07543253898620605, acc: 0.8671875, pred_acc: 0.765625cossim_loss: -0.6358659267425537, cls_loss: 0.5604333877563477\n",
      "loss: -0.18792355060577393, acc: 0.86328125, pred_acc: 0.76953125cossim_loss: -0.6285925507545471, cls_loss: 0.4406690001487732\n",
      "loss: -0.06889456510543823, acc: 0.84765625, pred_acc: 0.73828125cossim_loss: -0.6383926272392273, cls_loss: 0.5694980621337891\n",
      "loss: -0.1711137890815735, acc: 0.8671875, pred_acc: 0.7734375cossim_loss: -0.6328646540641785, cls_loss: 0.461750864982605\n",
      "loss: -0.21644827723503113, acc: 0.875, pred_acc: 0.8125cossim_loss: -0.6309481263160706, cls_loss: 0.41449984908103943\n",
      "loss: -0.14131659269332886, acc: 0.84765625, pred_acc: 0.78125cossim_loss: -0.6404831409454346, cls_loss: 0.4991665482521057\n",
      "loss: -0.004287540912628174, acc: 0.796875, pred_acc: 0.7265625cossim_loss: -0.6378849148750305, cls_loss: 0.6335973739624023\n",
      "loss: -0.1267247200012207, acc: 0.86328125, pred_acc: 0.77734375cossim_loss: -0.6306277513504028, cls_loss: 0.5039030313491821\n",
      "loss: -0.0776398777961731, acc: 0.84765625, pred_acc: 0.78515625cossim_loss: -0.6376754641532898, cls_loss: 0.5600355863571167\n",
      "loss: -0.005828499794006348, acc: 0.8359375, pred_acc: 0.7890625cossim_loss: -0.641371488571167, cls_loss: 0.6355429887771606\n",
      "loss: 0.2279927134513855, acc: 0.78515625, pred_acc: 0.77734375cossim_loss: -0.6427074670791626, cls_loss: 0.8707001805305481\n",
      "loss: 0.02633965015411377, acc: 0.8203125, pred_acc: 0.7734375cossim_loss: -0.6406632661819458, cls_loss: 0.6670029163360596\n",
      "loss: 0.006855428218841553, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.6333834528923035, cls_loss: 0.640238881111145\n",
      "loss: 0.1320042610168457, acc: 0.81640625, pred_acc: 0.71484375cossim_loss: -0.6426774263381958, cls_loss: 0.7746816873550415\n",
      "loss: 0.22935497760772705, acc: 0.76953125, pred_acc: 0.7265625cossim_loss: -0.640925407409668, cls_loss: 0.870280385017395\n",
      "loss: -0.03612798452377319, acc: 0.828125, pred_acc: 0.76171875cossim_loss: -0.6398158073425293, cls_loss: 0.6036878228187561\n",
      "loss: -0.024840593338012695, acc: 0.84375, pred_acc: 0.76953125cossim_loss: -0.6410449743270874, cls_loss: 0.6162043809890747\n",
      "loss: -0.09386157989501953, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.641857922077179, cls_loss: 0.5479963421821594\n",
      "loss: 0.30666470527648926, acc: 0.7734375, pred_acc: 0.76171875cossim_loss: -0.648654580116272, cls_loss: 0.9553192853927612\n",
      "loss: 0.1446397304534912, acc: 0.78125, pred_acc: 0.75cossim_loss: -0.6442598104476929, cls_loss: 0.7888995409011841\n",
      "loss: -0.1602405309677124, acc: 0.859375, pred_acc: 0.8046875cossim_loss: -0.6481120586395264, cls_loss: 0.48787152767181396\n",
      "loss: -0.1106104850769043, acc: 0.859375, pred_acc: 0.7734375cossim_loss: -0.6387985944747925, cls_loss: 0.5281881093978882\n",
      "loss: 0.04054766893386841, acc: 0.8359375, pred_acc: 0.77734375cossim_loss: -0.6401761174201965, cls_loss: 0.6807237863540649\n",
      "loss: -0.09690344333648682, acc: 0.85546875, pred_acc: 0.7890625cossim_loss: -0.6438277959823608, cls_loss: 0.546924352645874\n",
      "loss: -0.05611342191696167, acc: 0.85546875, pred_acc: 0.765625cossim_loss: -0.6342593431472778, cls_loss: 0.5781459212303162\n",
      "loss: -0.08096843957901001, acc: 0.83984375, pred_acc: 0.7421875cossim_loss: -0.6404541730880737, cls_loss: 0.5594857335090637\n",
      "loss: 0.04520869255065918, acc: 0.8046875, pred_acc: 0.75390625cossim_loss: -0.6445082426071167, cls_loss: 0.6897169351577759\n",
      "loss: -0.2802073061466217, acc: 0.8671875, pred_acc: 0.78515625cossim_loss: -0.6385724544525146, cls_loss: 0.35836514830589294\n",
      "loss: -0.03258848190307617, acc: 0.83984375, pred_acc: 0.75cossim_loss: -0.6421434879302979, cls_loss: 0.6095550060272217\n",
      "loss: -0.010266304016113281, acc: 0.8515625, pred_acc: 0.74609375cossim_loss: -0.6464581489562988, cls_loss: 0.6361918449401855\n",
      "loss: -0.08897507190704346, acc: 0.84375, pred_acc: 0.80078125cossim_loss: -0.6448506116867065, cls_loss: 0.5558755397796631\n",
      "loss: -0.13393229246139526, acc: 0.8671875, pred_acc: 0.78125cossim_loss: -0.638316810131073, cls_loss: 0.5043845176696777\n",
      "loss: -0.06342029571533203, acc: 0.83203125, pred_acc: 0.80078125cossim_loss: -0.6421091556549072, cls_loss: 0.5786888599395752\n",
      "loss: -0.02743542194366455, acc: 0.80078125, pred_acc: 0.75cossim_loss: -0.6390494704246521, cls_loss: 0.6116140484809875\n",
      "loss: -0.1823177933692932, acc: 0.875, pred_acc: 0.78125cossim_loss: -0.640682578086853, cls_loss: 0.4583647847175598\n",
      "loss: 0.003510415554046631, acc: 0.828125, pred_acc: 0.71875cossim_loss: -0.640228271484375, cls_loss: 0.6437386870384216\n",
      "loss: 0.027820706367492676, acc: 0.828125, pred_acc: 0.78515625cossim_loss: -0.6421970725059509, cls_loss: 0.6700177788734436\n",
      "loss: 0.070445716381073, acc: 0.8046875, pred_acc: 0.71875cossim_loss: -0.6400257349014282, cls_loss: 0.7104714512825012\n",
      "loss: -0.22716480493545532, acc: 0.89453125, pred_acc: 0.7578125cossim_loss: -0.6378379464149475, cls_loss: 0.4106731414794922\n",
      "loss: -0.12218958139419556, acc: 0.84765625, pred_acc: 0.74609375cossim_loss: -0.6386737823486328, cls_loss: 0.5164842009544373\n",
      "loss: -0.2021017074584961, acc: 0.86328125, pred_acc: 0.78515625cossim_loss: -0.6400051712989807, cls_loss: 0.4379034638404846\n",
      "loss: -0.03706389665603638, acc: 0.84765625, pred_acc: 0.76171875cossim_loss: -0.6400797963142395, cls_loss: 0.6030158996582031\n",
      "loss: 0.05174398422241211, acc: 0.8203125, pred_acc: 0.765625cossim_loss: -0.6434030532836914, cls_loss: 0.6951470375061035\n",
      "loss: -0.19772863388061523, acc: 0.88671875, pred_acc: 0.765625cossim_loss: -0.6284240484237671, cls_loss: 0.43069541454315186\n",
      "loss: -0.014079391956329346, acc: 0.828125, pred_acc: 0.7578125cossim_loss: -0.6473891139030457, cls_loss: 0.6333097219467163\n",
      "loss: 0.035471975803375244, acc: 0.82421875, pred_acc: 0.73046875cossim_loss: -0.6397364139556885, cls_loss: 0.6752083897590637\n",
      "loss: -0.1702694296836853, acc: 0.8828125, pred_acc: 0.79296875cossim_loss: -0.6278988122940063, cls_loss: 0.45762938261032104\n",
      "loss: 0.194313645362854, acc: 0.78515625, pred_acc: 0.72265625cossim_loss: -0.6538113355636597, cls_loss: 0.8481249809265137\n",
      "loss: -0.08451700210571289, acc: 0.8828125, pred_acc: 0.75cossim_loss: -0.6399519443511963, cls_loss: 0.5554349422454834\n",
      "loss: -0.009711861610412598, acc: 0.80078125, pred_acc: 0.80859375cossim_loss: -0.6421465873718262, cls_loss: 0.6324347257614136\n",
      "loss: -0.08883798122406006, acc: 0.83203125, pred_acc: 0.73828125cossim_loss: -0.6394480466842651, cls_loss: 0.5506100654602051\n",
      "loss: -0.14775213599205017, acc: 0.87890625, pred_acc: 0.76953125cossim_loss: -0.6313960552215576, cls_loss: 0.48364391922950745\n",
      "loss: -0.07501435279846191, acc: 0.83984375, pred_acc: 0.73828125cossim_loss: -0.6500883102416992, cls_loss: 0.5750739574432373\n",
      "loss: 0.008644938468933105, acc: 0.8203125, pred_acc: 0.75cossim_loss: -0.6401082277297974, cls_loss: 0.6487531661987305\n",
      "loss: -0.19507750868797302, acc: 0.8671875, pred_acc: 0.75390625cossim_loss: -0.6412619948387146, cls_loss: 0.4461844861507416\n",
      "loss: -0.1234058141708374, acc: 0.84765625, pred_acc: 0.81640625cossim_loss: -0.6328912973403931, cls_loss: 0.5094854831695557\n",
      "loss: 0.08057886362075806, acc: 0.78515625, pred_acc: 0.703125cossim_loss: -0.6488252878189087, cls_loss: 0.7294041514396667\n",
      "loss: -0.015578985214233398, acc: 0.8125, pred_acc: 0.765625cossim_loss: -0.6447266340255737, cls_loss: 0.6291476488113403\n",
      "loss: -0.1578967571258545, acc: 0.84375, pred_acc: 0.76171875cossim_loss: -0.6378954648971558, cls_loss: 0.47999870777130127\n",
      "loss: -0.10352158546447754, acc: 0.87109375, pred_acc: 0.828125cossim_loss: -0.6371550559997559, cls_loss: 0.5336334705352783\n",
      "loss: -0.004003286361694336, acc: 0.83984375, pred_acc: 0.75cossim_loss: -0.6368502378463745, cls_loss: 0.6328469514846802\n",
      "loss: -0.10738682746887207, acc: 0.87109375, pred_acc: 0.80859375cossim_loss: -0.642815351486206, cls_loss: 0.535428524017334\n",
      "loss: -0.15713542699813843, acc: 0.84375, pred_acc: 0.734375cossim_loss: -0.6397284269332886, cls_loss: 0.48259299993515015\n",
      "loss: -0.09321790933609009, acc: 0.8515625, pred_acc: 0.7734375cossim_loss: -0.6443561911582947, cls_loss: 0.5511382818222046\n",
      "loss: 0.09529495239257812, acc: 0.8515625, pred_acc: 0.73828125cossim_loss: -0.6496605277061462, cls_loss: 0.7449554800987244\n",
      "loss: -0.04615950584411621, acc: 0.80859375, pred_acc: 0.76171875cossim_loss: -0.6436694860458374, cls_loss: 0.5975099802017212\n",
      "loss: -0.049479663372039795, acc: 0.84375, pred_acc: 0.79296875cossim_loss: -0.6435537934303284, cls_loss: 0.5940741300582886\n",
      "loss: -0.07686418294906616, acc: 0.84375, pred_acc: 0.78515625cossim_loss: -0.6406176686286926, cls_loss: 0.5637534856796265\n",
      "loss: -0.06373542547225952, acc: 0.83203125, pred_acc: 0.76171875cossim_loss: -0.6314988732337952, cls_loss: 0.5677634477615356\n",
      "loss: -0.2673143446445465, acc: 0.890625, pred_acc: 0.7578125cossim_loss: -0.6446967124938965, cls_loss: 0.37738236784935\n",
      "loss: 0.09779727458953857, acc: 0.8359375, pred_acc: 0.734375cossim_loss: -0.6459095478057861, cls_loss: 0.7437068223953247\n",
      "loss: -0.05686378479003906, acc: 0.8359375, pred_acc: 0.78125cossim_loss: -0.6333156824111938, cls_loss: 0.5764518976211548\n",
      "loss: -0.011787474155426025, acc: 0.83984375, pred_acc: 0.72265625cossim_loss: -0.6436553001403809, cls_loss: 0.6318678259849548\n",
      "loss: 0.10228866338729858, acc: 0.8299999833106995, pred_acc: 0.7799999713897705cossim_loss: -0.6385399699211121, cls_loss: 0.7408286333084106\n",
      "loss: -0.13423973321914673, acc: 0.88671875, pred_acc: 0.77734375cossim_loss: -0.6362364292144775, cls_loss: 0.5019966959953308\n",
      "loss: -0.09747731685638428, acc: 0.8671875, pred_acc: 0.765625cossim_loss: -0.655672550201416, cls_loss: 0.5581952333450317\n",
      "loss: -0.09582066535949707, acc: 0.85546875, pred_acc: 0.76171875cossim_loss: -0.6490391492843628, cls_loss: 0.5532184839248657\n",
      "loss: -0.18774276971817017, acc: 0.84765625, pred_acc: 0.7890625cossim_loss: -0.6332325339317322, cls_loss: 0.445489764213562\n",
      "loss: 0.03277343511581421, acc: 0.85546875, pred_acc: 0.75390625cossim_loss: -0.6337924599647522, cls_loss: 0.6665658950805664\n",
      "loss: -0.023705661296844482, acc: 0.83984375, pred_acc: 0.796875cossim_loss: -0.6454242467880249, cls_loss: 0.6217185854911804\n",
      "loss: -0.08929002285003662, acc: 0.86328125, pred_acc: 0.81640625cossim_loss: -0.6438536643981934, cls_loss: 0.5545636415481567\n",
      "loss: -0.1465914249420166, acc: 0.8828125, pred_acc: 0.7890625cossim_loss: -0.6391556859016418, cls_loss: 0.49256426095962524\n",
      "loss: -0.1548824906349182, acc: 0.87109375, pred_acc: 0.7578125cossim_loss: -0.6349353790283203, cls_loss: 0.4800528883934021\n",
      "loss: -0.04887956380844116, acc: 0.84765625, pred_acc: 0.77734375cossim_loss: -0.6350594758987427, cls_loss: 0.5861799120903015\n",
      "loss: 0.025880157947540283, acc: 0.828125, pred_acc: 0.77734375cossim_loss: -0.6413220167160034, cls_loss: 0.6672021746635437\n",
      "loss: -0.1902827024459839, acc: 0.86328125, pred_acc: 0.78515625cossim_loss: -0.6405177712440491, cls_loss: 0.4502350687980652\n",
      "loss: -0.05702334642410278, acc: 0.83984375, pred_acc: 0.78125cossim_loss: -0.642937183380127, cls_loss: 0.5859138369560242\n",
      "loss: -0.14372166991233826, acc: 0.87890625, pred_acc: 0.8046875cossim_loss: -0.6334989070892334, cls_loss: 0.48977723717689514\n",
      "loss: -0.19114094972610474, acc: 0.859375, pred_acc: 0.75cossim_loss: -0.64874267578125, cls_loss: 0.45760172605514526\n",
      "loss: -0.2403630018234253, acc: 0.90234375, pred_acc: 0.83203125cossim_loss: -0.6380338668823242, cls_loss: 0.3976708650588989\n",
      "loss: -0.12135422229766846, acc: 0.8515625, pred_acc: 0.7109375cossim_loss: -0.6482833027839661, cls_loss: 0.5269290804862976\n",
      "loss: -0.18799811601638794, acc: 0.86328125, pred_acc: 0.78515625cossim_loss: -0.6448359489440918, cls_loss: 0.45683783292770386\n",
      "loss: -0.18233990669250488, acc: 0.87109375, pred_acc: 0.77734375cossim_loss: -0.6391450762748718, cls_loss: 0.45680516958236694\n",
      "loss: -0.005957841873168945, acc: 0.83203125, pred_acc: 0.7890625cossim_loss: -0.6396149396896362, cls_loss: 0.6336570978164673\n",
      "loss: 0.07736647129058838, acc: 0.76953125, pred_acc: 0.65234375cossim_loss: -0.6457846164703369, cls_loss: 0.7231510877609253\n",
      "loss: 0.07514345645904541, acc: 0.84765625, pred_acc: 0.76953125cossim_loss: -0.644396960735321, cls_loss: 0.7195404171943665\n",
      "loss: -0.16836124658584595, acc: 0.8671875, pred_acc: 0.7578125cossim_loss: -0.6401805281639099, cls_loss: 0.47181928157806396\n",
      "loss: -0.03396797180175781, acc: 0.83203125, pred_acc: 0.7421875cossim_loss: -0.6464956998825073, cls_loss: 0.6125277280807495\n",
      "loss: -0.10072380304336548, acc: 0.8359375, pred_acc: 0.71875cossim_loss: -0.6578042507171631, cls_loss: 0.5570804476737976\n",
      "loss: -0.031526923179626465, acc: 0.82421875, pred_acc: 0.78125cossim_loss: -0.6405309438705444, cls_loss: 0.609004020690918\n",
      "loss: -0.11509692668914795, acc: 0.84375, pred_acc: 0.78515625cossim_loss: -0.6369208097457886, cls_loss: 0.5218238830566406\n",
      "loss: -0.09845948219299316, acc: 0.85546875, pred_acc: 0.80859375cossim_loss: -0.6492804288864136, cls_loss: 0.5508209466934204\n",
      "loss: -0.06909465789794922, acc: 0.84765625, pred_acc: 0.7265625cossim_loss: -0.6514767408370972, cls_loss: 0.582382082939148\n",
      "loss: 0.04922008514404297, acc: 0.828125, pred_acc: 0.7578125cossim_loss: -0.6379464864730835, cls_loss: 0.6871665716171265\n",
      "loss: -0.18275189399719238, acc: 0.85546875, pred_acc: 0.765625cossim_loss: -0.6487908363342285, cls_loss: 0.46603894233703613\n",
      "loss: -0.12473368644714355, acc: 0.84375, pred_acc: 0.75cossim_loss: -0.6327757835388184, cls_loss: 0.5080420970916748\n",
      "loss: -0.05851030349731445, acc: 0.8515625, pred_acc: 0.734375cossim_loss: -0.6472159624099731, cls_loss: 0.5887056589126587\n",
      "loss: -0.10092461109161377, acc: 0.859375, pred_acc: 0.75390625cossim_loss: -0.6396477222442627, cls_loss: 0.5387231111526489\n",
      "loss: -0.029953956604003906, acc: 0.80859375, pred_acc: 0.75390625cossim_loss: -0.6497015953063965, cls_loss: 0.6197476387023926\n",
      "loss: -0.008611321449279785, acc: 0.8515625, pred_acc: 0.75390625cossim_loss: -0.6496274471282959, cls_loss: 0.6410161256790161\n",
      "loss: -0.08061432838439941, acc: 0.82421875, pred_acc: 0.76953125cossim_loss: -0.6472980976104736, cls_loss: 0.5666837692260742\n",
      "loss: -0.0046149492263793945, acc: 0.8203125, pred_acc: 0.7109375cossim_loss: -0.6499422788619995, cls_loss: 0.6453273296356201\n",
      "loss: -0.05743837356567383, acc: 0.8671875, pred_acc: 0.7734375cossim_loss: -0.6417015790939331, cls_loss: 0.5842632055282593\n",
      "loss: -0.11245697736740112, acc: 0.85546875, pred_acc: 0.73046875cossim_loss: -0.6426979899406433, cls_loss: 0.5302410125732422\n",
      "loss: -0.0013191699981689453, acc: 0.83203125, pred_acc: 0.75390625cossim_loss: -0.639856219291687, cls_loss: 0.6385370492935181\n",
      "loss: -0.045617520809173584, acc: 0.84375, pred_acc: 0.78125cossim_loss: -0.6505621671676636, cls_loss: 0.60494464635849\n",
      "loss: -0.03575640916824341, acc: 0.8515625, pred_acc: 0.76171875cossim_loss: -0.6549158692359924, cls_loss: 0.619159460067749\n",
      "loss: -0.054161667823791504, acc: 0.8359375, pred_acc: 0.796875cossim_loss: -0.6408185958862305, cls_loss: 0.586656928062439\n",
      "loss: -0.21665242314338684, acc: 0.8671875, pred_acc: 0.74609375cossim_loss: -0.6455295085906982, cls_loss: 0.4288770854473114\n",
      "loss: -0.1108294129371643, acc: 0.85546875, pred_acc: 0.78125cossim_loss: -0.6431147456169128, cls_loss: 0.5322853326797485\n",
      "loss: 0.07832854986190796, acc: 0.8203125, pred_acc: 0.796875cossim_loss: -0.6463474631309509, cls_loss: 0.7246760129928589\n",
      "loss: 0.03922969102859497, acc: 0.80859375, pred_acc: 0.7265625cossim_loss: -0.6495361924171448, cls_loss: 0.6887658834457397\n",
      "loss: -0.058961331844329834, acc: 0.86328125, pred_acc: 0.8203125cossim_loss: -0.6442279815673828, cls_loss: 0.585266649723053\n",
      "loss: -0.16603577136993408, acc: 0.859375, pred_acc: 0.80078125cossim_loss: -0.644949197769165, cls_loss: 0.47891342639923096\n",
      "loss: -0.19828647375106812, acc: 0.82421875, pred_acc: 0.734375cossim_loss: -0.6549282073974609, cls_loss: 0.4566417336463928\n",
      "loss: -0.07175654172897339, acc: 0.8359375, pred_acc: 0.8125cossim_loss: -0.6409202814102173, cls_loss: 0.5691637396812439\n",
      "loss: -0.040625929832458496, acc: 0.8515625, pred_acc: 0.77734375cossim_loss: -0.646918773651123, cls_loss: 0.6062928438186646\n",
      "loss: 0.042769670486450195, acc: 0.828125, pred_acc: 0.73828125cossim_loss: -0.6469266414642334, cls_loss: 0.6896963119506836\n",
      "loss: -0.02642190456390381, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6374925374984741, cls_loss: 0.6110706329345703\n",
      "loss: -0.15070438385009766, acc: 0.8515625, pred_acc: 0.78515625cossim_loss: -0.6437097787857056, cls_loss: 0.4930053949356079\n",
      "loss: -0.10624045133590698, acc: 0.87109375, pred_acc: 0.78515625cossim_loss: -0.6419898867607117, cls_loss: 0.5357494354248047\n",
      "loss: 0.02056330442428589, acc: 0.83984375, pred_acc: 0.75390625cossim_loss: -0.6454136967658997, cls_loss: 0.6659770011901855\n",
      "loss: 0.01434546709060669, acc: 0.83984375, pred_acc: 0.7734375cossim_loss: -0.6371082663536072, cls_loss: 0.6514537334442139\n",
      "loss: -0.1331315040588379, acc: 0.8828125, pred_acc: 0.78515625cossim_loss: -0.6354924440383911, cls_loss: 0.5023609399795532\n",
      "loss: -0.03907531499862671, acc: 0.82421875, pred_acc: 0.80078125cossim_loss: -0.6414377093315125, cls_loss: 0.6023623943328857\n",
      "loss: -0.05595874786376953, acc: 0.828125, pred_acc: 0.7734375cossim_loss: -0.643841028213501, cls_loss: 0.5878822803497314\n",
      "loss: -0.017206251621246338, acc: 0.8203125, pred_acc: 0.765625cossim_loss: -0.6502732634544373, cls_loss: 0.6330670118331909\n",
      "loss: -0.20829635858535767, acc: 0.87109375, pred_acc: 0.7890625cossim_loss: -0.6416444182395935, cls_loss: 0.43334805965423584\n",
      "loss: -0.09598833322525024, acc: 0.859375, pred_acc: 0.76171875cossim_loss: -0.6399654746055603, cls_loss: 0.5439771413803101\n",
      "loss: -0.004411637783050537, acc: 0.85546875, pred_acc: 0.77734375cossim_loss: -0.6397263407707214, cls_loss: 0.6353147029876709\n",
      "loss: -0.05351436138153076, acc: 0.8359375, pred_acc: 0.7578125cossim_loss: -0.6405315399169922, cls_loss: 0.5870171785354614\n",
      "loss: 0.1464863419532776, acc: 0.8125, pred_acc: 0.72265625cossim_loss: -0.6378787755966187, cls_loss: 0.7843651175498962\n",
      "loss: 0.04620397090911865, acc: 0.8203125, pred_acc: 0.75390625cossim_loss: -0.6393671035766602, cls_loss: 0.6855710744857788\n",
      "loss: -0.0668599009513855, acc: 0.8203125, pred_acc: 0.78515625cossim_loss: -0.6330257654190063, cls_loss: 0.5661658644676208\n",
      "loss: 0.016017675399780273, acc: 0.84765625, pred_acc: 0.76953125cossim_loss: -0.63639897108078, cls_loss: 0.6524166464805603\n",
      "loss: -0.07886147499084473, acc: 0.83984375, pred_acc: 0.7421875cossim_loss: -0.6392805576324463, cls_loss: 0.5604190826416016\n",
      "loss: 0.06162923574447632, acc: 0.828125, pred_acc: 0.75390625cossim_loss: -0.6407679915428162, cls_loss: 0.7023972272872925\n",
      "loss: 0.06277179718017578, acc: 0.828125, pred_acc: 0.75cossim_loss: -0.6375505328178406, cls_loss: 0.7003223299980164\n",
      "loss: -0.1763172149658203, acc: 0.875, pred_acc: 0.78515625cossim_loss: -0.6437702178955078, cls_loss: 0.4674530029296875\n",
      "loss: -0.0073435306549072266, acc: 0.84375, pred_acc: 0.71875cossim_loss: -0.6507534980773926, cls_loss: 0.6434099674224854\n",
      "loss: -0.11327254772186279, acc: 0.8203125, pred_acc: 0.76171875cossim_loss: -0.6436933279037476, cls_loss: 0.5304207801818848\n",
      "loss: 0.052749812602996826, acc: 0.8046875, pred_acc: 0.7578125cossim_loss: -0.6425647139549255, cls_loss: 0.6953145265579224\n",
      "loss: 0.07145118713378906, acc: 0.80859375, pred_acc: 0.7890625cossim_loss: -0.6450852155685425, cls_loss: 0.7165364027023315\n",
      "loss: 0.004667460918426514, acc: 0.83984375, pred_acc: 0.78125cossim_loss: -0.6393324732780457, cls_loss: 0.6439999341964722\n",
      "loss: -0.08516287803649902, acc: 0.8515625, pred_acc: 0.7890625cossim_loss: -0.63666832447052, cls_loss: 0.551505446434021\n",
      "loss: -0.36526229977607727, acc: 0.91796875, pred_acc: 0.8046875cossim_loss: -0.6318892240524292, cls_loss: 0.26662692427635193\n",
      "loss: -0.11591029167175293, acc: 0.85546875, pred_acc: 0.80859375cossim_loss: -0.641411542892456, cls_loss: 0.5255012512207031\n",
      "loss: -0.16668933629989624, acc: 0.87109375, pred_acc: 0.76171875cossim_loss: -0.6364789009094238, cls_loss: 0.4697895646095276\n",
      "loss: 0.035775840282440186, acc: 0.83203125, pred_acc: 0.72265625cossim_loss: -0.6485394835472107, cls_loss: 0.6843153238296509\n",
      "loss: -0.2126040756702423, acc: 0.91015625, pred_acc: 0.74609375cossim_loss: -0.6551921963691711, cls_loss: 0.44258812069892883\n",
      "loss: -0.07526803016662598, acc: 0.8671875, pred_acc: 0.7890625cossim_loss: -0.6410884857177734, cls_loss: 0.5658204555511475\n",
      "loss: -0.1593310832977295, acc: 0.8671875, pred_acc: 0.77734375cossim_loss: -0.6388458013534546, cls_loss: 0.4795147180557251\n",
      "loss: 0.08755332231521606, acc: 0.80859375, pred_acc: 0.7734375cossim_loss: -0.6456552743911743, cls_loss: 0.7332085967063904\n",
      "loss: 0.034370481967926025, acc: 0.8203125, pred_acc: 0.796875cossim_loss: -0.6389254331588745, cls_loss: 0.6732959151268005\n",
      "loss: 0.1484888792037964, acc: 0.828125, pred_acc: 0.78125cossim_loss: -0.6443264484405518, cls_loss: 0.7928153276443481\n",
      "loss: 0.09443897008895874, acc: 0.81640625, pred_acc: 0.77734375cossim_loss: -0.6464365124702454, cls_loss: 0.7408754825592041\n",
      "loss: -9.679794311523438e-05, acc: 0.8203125, pred_acc: 0.75cossim_loss: -0.6434204578399658, cls_loss: 0.6433236598968506\n",
      "loss: -0.08314633369445801, acc: 0.83203125, pred_acc: 0.7578125cossim_loss: -0.6484338045120239, cls_loss: 0.5652874708175659\n",
      "loss: -0.06049305200576782, acc: 0.84375, pred_acc: 0.80859375cossim_loss: -0.6458232998847961, cls_loss: 0.5853302478790283\n",
      "loss: -0.10278439521789551, acc: 0.8828125, pred_acc: 0.7578125cossim_loss: -0.6464637517929077, cls_loss: 0.5436793565750122\n",
      "loss: -0.14180094003677368, acc: 0.86328125, pred_acc: 0.7265625cossim_loss: -0.6442647576332092, cls_loss: 0.5024638175964355\n",
      "loss: -0.010147809982299805, acc: 0.8359375, pred_acc: 0.72265625cossim_loss: -0.6484843492507935, cls_loss: 0.6383365392684937\n",
      "loss: -0.06473672389984131, acc: 0.83984375, pred_acc: 0.78125cossim_loss: -0.6460644006729126, cls_loss: 0.5813276767730713\n",
      "loss: 0.04916566610336304, acc: 0.84375, pred_acc: 0.76171875cossim_loss: -0.638987123966217, cls_loss: 0.6881527900695801\n",
      "loss: 0.03606677055358887, acc: 0.81640625, pred_acc: 0.74609375cossim_loss: -0.6447566747665405, cls_loss: 0.6808234453201294\n",
      "loss: -0.11383914947509766, acc: 0.85546875, pred_acc: 0.734375cossim_loss: -0.6407299041748047, cls_loss: 0.526890754699707\n",
      "loss: 0.006640911102294922, acc: 0.84765625, pred_acc: 0.73828125cossim_loss: -0.6413224339485168, cls_loss: 0.6479633450508118\n",
      "loss: -0.0425834059715271, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6434134840965271, cls_loss: 0.600830078125\n",
      "loss: -0.0870024561882019, acc: 0.83203125, pred_acc: 0.73046875cossim_loss: -0.6475743055343628, cls_loss: 0.5605718493461609\n",
      "loss: -0.07109701633453369, acc: 0.84375, pred_acc: 0.7734375cossim_loss: -0.6401709318161011, cls_loss: 0.5690739154815674\n",
      "loss: 0.15060532093048096, acc: 0.8125, pred_acc: 0.703125cossim_loss: -0.6527756452560425, cls_loss: 0.8033809661865234\n",
      "loss: 0.03243798017501831, acc: 0.8125, pred_acc: 0.77734375cossim_loss: -0.6451427340507507, cls_loss: 0.677580714225769\n",
      "loss: -0.029714345932006836, acc: 0.83984375, pred_acc: 0.80859375cossim_loss: -0.6383558511734009, cls_loss: 0.608641505241394\n",
      "loss: -0.04508250951766968, acc: 0.859375, pred_acc: 0.796875cossim_loss: -0.6440346240997314, cls_loss: 0.5989521145820618\n",
      "loss: -0.14383366703987122, acc: 0.859375, pred_acc: 0.74609375cossim_loss: -0.6431213617324829, cls_loss: 0.4992876946926117\n",
      "loss: -0.13835307955741882, acc: 0.86328125, pred_acc: 0.7890625cossim_loss: -0.6339762806892395, cls_loss: 0.4956232011318207\n",
      "loss: -0.3334028422832489, acc: 0.890625, pred_acc: 0.8046875cossim_loss: -0.6422775387763977, cls_loss: 0.3088746964931488\n",
      "loss: -0.022472143173217773, acc: 0.84375, pred_acc: 0.81640625cossim_loss: -0.6315535306930542, cls_loss: 0.6090813875198364\n",
      "loss: 0.04041457176208496, acc: 0.83203125, pred_acc: 0.7578125cossim_loss: -0.6406880617141724, cls_loss: 0.6811026334762573\n",
      "loss: -0.02557826042175293, acc: 0.84765625, pred_acc: 0.7109375cossim_loss: -0.6502819061279297, cls_loss: 0.6247036457061768\n",
      "loss: -0.12792456150054932, acc: 0.8671875, pred_acc: 0.7421875cossim_loss: -0.6401872634887695, cls_loss: 0.5122627019882202\n",
      "loss: -0.21346715092658997, acc: 0.8671875, pred_acc: 0.765625cossim_loss: -0.6478720903396606, cls_loss: 0.4344049394130707\n",
      "loss: -0.051116108894348145, acc: 0.84765625, pred_acc: 0.828125cossim_loss: -0.6403456926345825, cls_loss: 0.5892295837402344\n",
      "loss: -0.15665274858474731, acc: 0.87890625, pred_acc: 0.83203125cossim_loss: -0.633903980255127, cls_loss: 0.47725123167037964\n",
      "loss: -0.1327219009399414, acc: 0.8515625, pred_acc: 0.78125cossim_loss: -0.6448404788970947, cls_loss: 0.5121185779571533\n",
      "loss: -0.12948250770568848, acc: 0.84375, pred_acc: 0.75390625cossim_loss: -0.644565761089325, cls_loss: 0.5150832533836365\n",
      "loss: -0.08238309621810913, acc: 0.84375, pred_acc: 0.76953125cossim_loss: -0.6403144001960754, cls_loss: 0.5579313039779663\n",
      "loss: -0.08330029249191284, acc: 0.8515625, pred_acc: 0.77734375cossim_loss: -0.6334689259529114, cls_loss: 0.5501686334609985\n",
      "loss: -0.011844933032989502, acc: 0.84375, pred_acc: 0.75cossim_loss: -0.6451435685157776, cls_loss: 0.6332986354827881\n",
      "loss: -0.1995818316936493, acc: 0.86328125, pred_acc: 0.7734375cossim_loss: -0.6418772339820862, cls_loss: 0.4422954022884369\n",
      "loss: -0.049764037132263184, acc: 0.8203125, pred_acc: 0.72265625cossim_loss: -0.6423312425613403, cls_loss: 0.5925672054290771\n",
      "loss: -0.027856171131134033, acc: 0.8359375, pred_acc: 0.75cossim_loss: -0.641425371170044, cls_loss: 0.6135692000389099\n",
      "loss: -0.13386493921279907, acc: 0.859375, pred_acc: 0.76171875cossim_loss: -0.637168824672699, cls_loss: 0.5033038854598999\n",
      "loss: -0.1615968644618988, acc: 0.83984375, pred_acc: 0.80859375cossim_loss: -0.6450446844100952, cls_loss: 0.4834478199481964\n",
      "loss: -0.016643822193145752, acc: 0.84765625, pred_acc: 0.7578125cossim_loss: -0.6495686769485474, cls_loss: 0.6329248547554016\n",
      "loss: -0.23852002620697021, acc: 0.8984375, pred_acc: 0.7734375cossim_loss: -0.6345399618148804, cls_loss: 0.39601993560791016\n",
      "loss: -0.15597087144851685, acc: 0.875, pred_acc: 0.78125cossim_loss: -0.6401496529579163, cls_loss: 0.4841787815093994\n",
      "loss: -0.2185182273387909, acc: 0.890625, pred_acc: 0.78515625cossim_loss: -0.6342535018920898, cls_loss: 0.41573527455329895\n",
      "loss: -0.12353074550628662, acc: 0.84765625, pred_acc: 0.78515625cossim_loss: -0.6517425775527954, cls_loss: 0.5282118320465088\n",
      "loss: -0.13860642910003662, acc: 0.84375, pred_acc: 0.7109375cossim_loss: -0.6495230793952942, cls_loss: 0.5109166502952576\n",
      "loss: 0.002059757709503174, acc: 0.84765625, pred_acc: 0.7890625cossim_loss: -0.6366528272628784, cls_loss: 0.6387125849723816\n",
      "loss: -0.20097678899765015, acc: 0.875, pred_acc: 0.765625cossim_loss: -0.651324987411499, cls_loss: 0.4503481984138489\n",
      "loss: -0.16870185732841492, acc: 0.8828125, pred_acc: 0.78125cossim_loss: -0.6486212015151978, cls_loss: 0.47991934418678284\n",
      "loss: 0.01102757453918457, acc: 0.828125, pred_acc: 0.7265625cossim_loss: -0.6492593288421631, cls_loss: 0.6602869033813477\n",
      "loss: -0.10529011487960815, acc: 0.8515625, pred_acc: 0.7578125cossim_loss: -0.6349050998687744, cls_loss: 0.5296149849891663\n",
      "loss: -0.23015150427818298, acc: 0.890625, pred_acc: 0.7734375cossim_loss: -0.6443231105804443, cls_loss: 0.41417160630226135\n",
      "loss: -0.17324137687683105, acc: 0.8671875, pred_acc: 0.73046875cossim_loss: -0.646395206451416, cls_loss: 0.47315382957458496\n",
      "loss: -0.08150726556777954, acc: 0.859375, pred_acc: 0.79296875cossim_loss: -0.6428601145744324, cls_loss: 0.5613528490066528\n",
      "loss: -0.0748181939125061, acc: 0.8359375, pred_acc: 0.76953125cossim_loss: -0.6426948308944702, cls_loss: 0.5678766369819641\n",
      "loss: -0.1951984167098999, acc: 0.87109375, pred_acc: 0.75390625cossim_loss: -0.6422973275184631, cls_loss: 0.44709891080856323\n",
      "loss: -0.11959195137023926, acc: 0.87109375, pred_acc: 0.76953125cossim_loss: -0.6537207365036011, cls_loss: 0.5341287851333618\n",
      "loss: 0.0579068660736084, acc: 0.8046875, pred_acc: 0.7265625cossim_loss: -0.6395022869110107, cls_loss: 0.6974091529846191\n",
      "loss: 0.07447642087936401, acc: 0.79296875, pred_acc: 0.7109375cossim_loss: -0.6501063108444214, cls_loss: 0.7245827317237854\n",
      "loss: -0.016082286834716797, acc: 0.82421875, pred_acc: 0.79296875cossim_loss: -0.6498404741287231, cls_loss: 0.6337581872940063\n",
      "loss: -0.14227402210235596, acc: 0.85546875, pred_acc: 0.79296875cossim_loss: -0.6449900269508362, cls_loss: 0.5027160048484802\n",
      "loss: -0.11256951093673706, acc: 0.8515625, pred_acc: 0.734375cossim_loss: -0.6508666276931763, cls_loss: 0.5382971167564392\n",
      "loss: -0.09314131736755371, acc: 0.84765625, pred_acc: 0.7421875cossim_loss: -0.6517929434776306, cls_loss: 0.5586516261100769\n",
      "loss: -0.09247696399688721, acc: 0.83984375, pred_acc: 0.79296875cossim_loss: -0.6576396226882935, cls_loss: 0.5651626586914062\n",
      "loss: -0.011997103691101074, acc: 0.8671875, pred_acc: 0.77734375cossim_loss: -0.6548359394073486, cls_loss: 0.6428388357162476\n",
      "loss: -0.12068331241607666, acc: 0.84375, pred_acc: 0.80859375cossim_loss: -0.645950436592102, cls_loss: 0.5252671241760254\n",
      "loss: 0.08992040157318115, acc: 0.78125, pred_acc: 0.7734375cossim_loss: -0.6481067538261414, cls_loss: 0.7380271553993225\n",
      "loss: -0.03719818592071533, acc: 0.8359375, pred_acc: 0.734375cossim_loss: -0.6496416330337524, cls_loss: 0.6124434471130371\n",
      "loss: -0.19600912928581238, acc: 0.83984375, pred_acc: 0.72265625cossim_loss: -0.6461268663406372, cls_loss: 0.45011773705482483\n",
      "loss: -0.005619466304779053, acc: 0.82421875, pred_acc: 0.734375cossim_loss: -0.6520653963088989, cls_loss: 0.6464459300041199\n",
      "loss: -0.133294939994812, acc: 0.8515625, pred_acc: 0.7890625cossim_loss: -0.6561607718467712, cls_loss: 0.5228658318519592\n",
      "loss: -0.14856553077697754, acc: 0.859375, pred_acc: 0.78125cossim_loss: -0.6474579572677612, cls_loss: 0.4988924264907837\n",
      "loss: 0.26995134353637695, acc: 0.7734375, pred_acc: 0.75cossim_loss: -0.6477721929550171, cls_loss: 0.917723536491394\n",
      "loss: -0.1667056679725647, acc: 0.859375, pred_acc: 0.80078125cossim_loss: -0.6446213722229004, cls_loss: 0.4779157042503357\n",
      "loss: 0.09461414813995361, acc: 0.8203125, pred_acc: 0.75cossim_loss: -0.6575960516929626, cls_loss: 0.7522101998329163\n",
      "loss: -0.14936786890029907, acc: 0.859375, pred_acc: 0.77734375cossim_loss: -0.6464815139770508, cls_loss: 0.4971136450767517\n",
      "loss: -0.03527653217315674, acc: 0.84375, pred_acc: 0.7890625cossim_loss: -0.6503669023513794, cls_loss: 0.6150903701782227\n",
      "loss: -0.12756919860839844, acc: 0.8671875, pred_acc: 0.80078125cossim_loss: -0.6544471979141235, cls_loss: 0.5268779993057251\n",
      "loss: -0.1990722119808197, acc: 0.89453125, pred_acc: 0.80078125cossim_loss: -0.6398295164108276, cls_loss: 0.44075730443000793\n",
      "loss: -0.006001830101013184, acc: 0.8359375, pred_acc: 0.73046875cossim_loss: -0.6504510045051575, cls_loss: 0.6444491744041443\n",
      "loss: -0.05385035276412964, acc: 0.82421875, pred_acc: 0.74609375cossim_loss: -0.6478623151779175, cls_loss: 0.5940119624137878\n",
      "loss: -0.02805572748184204, acc: 0.8359375, pred_acc: 0.75cossim_loss: -0.6489070653915405, cls_loss: 0.6208513379096985\n",
      "loss: 0.014947295188903809, acc: 0.796875, pred_acc: 0.77734375cossim_loss: -0.6487305760383606, cls_loss: 0.6636778712272644\n",
      "loss: 0.12800776958465576, acc: 0.82421875, pred_acc: 0.75cossim_loss: -0.6399165391921997, cls_loss: 0.7679243087768555\n",
      "loss: -0.04292374849319458, acc: 0.83984375, pred_acc: 0.734375cossim_loss: -0.6583028435707092, cls_loss: 0.6153790950775146\n",
      "loss: -0.05841022729873657, acc: 0.8450000286102295, pred_acc: 0.7749999761581421cossim_loss: -0.6504716873168945, cls_loss: 0.592061460018158\n"
     ]
    }
   ],
   "source": [
    "input_embed = pred_model.get_input_embeddings()\n",
    "classifier_head = pred_model.classifier\n",
    "hidden_size = config.hidden_size\n",
    "model = SurrogateInterpretation(pred_model=pred_model, classifier_head=classifier_head, input_embed=input_embed, hidden_size=hidden_size)\n",
    "model.to(device)\n",
    "outputs = model(**inputs)\n",
    "print(\"attention_output shape: \", outputs['attention_output'].shape)\n",
    "print(\"attention_weights shape: \", outputs['attention_weights'].shape)\n",
    "print(\"last_hidden_state shape: \", outputs['last_hidden_state'].shape)\n",
    "print(\"patch_reprs shape: \", outputs['patch_reprs'].shape)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        label = data['labels'].to(device)\n",
    "        outputs = model(pixel_values, label)\n",
    "        loss = outputs['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss.item()}, acc: {outputs['acc'].item()}, pred_acc: {outputs['pred_acc'].item()}\"\n",
    "              f\"cossim_loss: {outputs['cossim_loss'].item()}, cls_loss: {outputs['cls_loss'].item()}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad == True:\n",
    "#         print(name, param.requires_grad)\n",
    "model_name = 'vit_sur_loss=cls+cos_att=cossim2.pt'\n",
    "torch.save(model.state_dict(), f'model/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_output shape:  tensor([[[ -2.4139,   1.2525,  -5.8068,  ...,  -1.3713,   3.6165,  -0.0333]],\n",
      "\n",
      "        [[  0.8080,  -2.6431,  -1.7622,  ...,   7.8847,  -0.8463,  10.1360]],\n",
      "\n",
      "        [[  3.2632,  -9.8401,   4.9660,  ..., -11.6845,  -3.7304,  -4.1211]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.3884,  -4.7305,  -2.3395,  ...,  -4.6104,  -0.1576,   2.3779]],\n",
      "\n",
      "        [[  6.6678,  -0.7116,   0.7123,  ...,  -1.5447,  -0.6401,  -5.3296]],\n",
      "\n",
      "        [[ -0.0722,  -1.1478,   5.5667,  ...,   2.7927,  -1.6272,  -2.9185]]],\n",
      "       device='mps:0', grad_fn=<UnsafeViewBackward0>)\n",
      "attention_weights: tensor([[[ 0.0062, -0.0446, -0.0248,  ..., -0.0851, -0.0127, -0.0037]],\n",
      "\n",
      "        [[-0.0486,  0.0095, -0.0574,  ..., -0.0123, -0.0571, -0.0509]],\n",
      "\n",
      "        [[ 0.0132, -0.0574,  0.0335,  ..., -0.0291, -0.0285,  0.0066]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0173,  0.0006, -0.0141,  ..., -0.0051, -0.0056, -0.1184]],\n",
      "\n",
      "        [[-0.0358,  0.0102, -0.0101,  ...,  0.0503, -0.0032,  0.0283]],\n",
      "\n",
      "        [[-0.0095,  0.0219,  0.0039,  ..., -0.0193, -0.0322,  0.0029]]],\n",
      "       device='mps:0', grad_fn=<DivBackward0>)\n",
      "last_hidden_state shape:  tensor([[ -6.5898,  11.2904,  -6.6615,  ...,   1.2074,  -0.7898,  -3.9372],\n",
      "        [ -1.4653,  -5.8697,   9.1209,  ...,  -5.6890,   0.4717,  14.9723],\n",
      "        [  2.6811,  -1.9956,   5.2226,  ..., -10.0995,  10.2021,  12.7760],\n",
      "        ...,\n",
      "        [  0.2299,  -8.5136,   1.1077,  ...,  -9.6072,  -4.1624,  -0.3767],\n",
      "        [  5.3960,  -2.6165,  -6.0973,  ...,   1.1822, -14.7051,   7.1953],\n",
      "        [ -0.4192,  -1.4929,   2.8651,  ...,  -0.9941,  -5.1000,  -2.9890]],\n",
      "       device='mps:0')\n",
      "pred:  tensor([295, 886, 597, 385,  10, 182, 801, 967, 636, 388, 252, 327, 406, 119,\n",
      "        621, 878, 960, 824, 926, 202,  11, 746, 250, 990,  39, 426, 572, 813,\n",
      "        747, 824,  91, 643, 890, 697, 753,  83, 179,  76, 354,  97, 761, 349,\n",
      "        731, 466, 118, 858, 796, 721, 900, 194, 607, 188, 750, 522, 135, 510,\n",
      "        169, 397, 648, 997, 950,  44, 258, 210,  27, 895,  93, 745, 232,  87,\n",
      "        904, 940, 195, 338,  41, 315, 586, 907,  64, 802, 624, 267,  96, 847,\n",
      "        831, 855, 243, 398,  28, 197, 435, 317, 299, 466, 831,  95, 326, 642,\n",
      "        898,  86, 159, 562, 306, 728, 941, 546, 488, 311, 391, 784, 203, 602,\n",
      "        128, 313, 898, 632, 387, 326, 650, 530,  99, 957, 533, 813, 416,  26,\n",
      "        206, 951, 336, 128, 864,  13, 334, 848, 823, 277, 322, 535, 692, 723,\n",
      "        693, 698, 693, 894, 754, 156, 201, 141, 997, 594, 479, 398, 646, 537,\n",
      "        871, 372, 236, 912, 254,  81, 767, 699, 166, 476,  94, 669, 594, 238,\n",
      "        678, 364, 880, 423, 761, 775,  84, 259, 639, 269, 725, 473, 995, 468,\n",
      "        332, 565, 239, 980, 180, 868, 167, 897, 150, 500, 580,  57, 661, 900,\n",
      "        722, 564, 214, 312], device='mps:0')\n",
      "labels:  tensor([295, 886, 597, 386,  10, 182, 801, 967, 414, 388, 196, 327, 677, 119,\n",
      "        621, 878, 415, 824, 926, 154,  11, 746, 250, 990,  39, 426, 572, 618,\n",
      "        747, 474,  91, 643, 890, 697, 753,  83, 179,  76, 354,  97, 761, 349,\n",
      "        731, 466, 119, 858, 796, 721, 900, 194, 607, 188, 750, 522, 135, 510,\n",
      "        169, 397, 648, 997, 950,  44, 258, 210,  27, 895,  93, 745, 852,  87,\n",
      "        799, 941, 253, 338,  46, 315, 847, 892,  64, 802, 454, 267, 382, 847,\n",
      "        831, 855, 243, 443,  28, 197, 435, 317, 299, 895, 831,  95, 326, 722,\n",
      "        653,  86, 159, 682, 306, 728, 941, 546, 488, 311, 391, 784, 203, 602,\n",
      "        128, 313, 898, 632, 387, 326, 650, 811,  99, 957, 533, 813, 416,  25,\n",
      "        206, 951, 336, 128, 864,  13, 334, 848, 823, 287, 322, 535, 737, 723,\n",
      "        693, 538, 693, 493, 485, 156, 201, 141, 997, 793, 817, 398, 646, 537,\n",
      "        871, 372, 236, 912, 254, 187, 767, 699, 167, 476, 127, 669, 594, 240,\n",
      "        515, 364, 880, 423, 761, 830,  84, 259, 638, 269, 505, 473, 995, 468,\n",
      "        647, 565, 239, 980, 179, 868, 167, 897, 703, 500, 580,  57, 661, 270,\n",
      "        722, 564, 156, 312])\n",
      "interp:  tensor([295, 886, 597, 385,  10, 182, 639, 967, 748, 388, 252, 327, 406, 119,\n",
      "        880, 878, 960, 824, 926, 204,  11, 746, 250, 990,  39, 426, 572, 813,\n",
      "        747, 735,  91, 643, 890, 697, 753,  83, 179,  76, 354,  97, 761, 350,\n",
      "        731, 466, 121, 858, 796, 721, 900, 194, 607, 188, 750, 522, 135, 510,\n",
      "        169, 397, 648, 997, 950,  44, 258, 210,  27, 895,  93, 745, 232,  87,\n",
      "        904, 940, 253, 338,  41, 315, 586, 420,  64, 802, 727, 267, 364, 847,\n",
      "        831, 855, 243, 747,  28, 197, 435, 317, 299, 705, 831,  95, 326, 420,\n",
      "        898,  86, 159, 562, 306, 728, 941, 546, 488, 311, 391, 784, 203, 702,\n",
      "        128, 313, 898, 409, 387, 326, 513, 530,  99, 957, 533, 813, 416,  35,\n",
      "        206, 951, 336, 128, 864,  13, 334, 848, 823, 271, 322, 535, 692, 723,\n",
      "        693, 743, 693, 894, 754, 156, 201, 141, 997, 594, 817, 398, 646, 537,\n",
      "        871, 372, 236, 912, 254,  81, 767, 699, 167, 476,  22, 669, 594, 238,\n",
      "        515, 364, 880, 423, 761, 462,  84, 231, 639, 269, 773, 473, 995, 468,\n",
      "        332, 565, 239, 980, 242, 659, 167, 897, 150, 500, 580,  57, 661, 682,\n",
      "        830, 564, 214, 312], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "print(\"attention_output shape: \", outputs['attention_output'])\n",
    "print(\"attention_weights:\", outputs['attention_weights'])\n",
    "print(\"last_hidden_state shape: \", outputs['last_hidden_state'])\n",
    "logit = model.classifier(outputs['last_hidden_state'])\n",
    "pred = logit.argmax(-1)\n",
    "print('pred: ', pred)\n",
    "int_logit = model.classifier(outputs['attention_output'])\n",
    "interp = int_logit.argmax(-1)\n",
    "print('labels: ', data['labels'])\n",
    "print('interp: ', interp.reshape([-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0062, -0.0446, -0.0248,  ..., -0.0851, -0.0127, -0.0037]],\n",
       "\n",
       "        [[-0.0486,  0.0095, -0.0574,  ..., -0.0123, -0.0571, -0.0509]],\n",
       "\n",
       "        [[ 0.0132, -0.0574,  0.0335,  ..., -0.0291, -0.0285,  0.0066]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0173,  0.0006, -0.0141,  ..., -0.0051, -0.0056, -0.1184]],\n",
       "\n",
       "        [[-0.0358,  0.0102, -0.0101,  ...,  0.0503, -0.0032,  0.0283]],\n",
       "\n",
       "        [[-0.0095,  0.0219,  0.0039,  ..., -0.0193, -0.0322,  0.0029]]],\n",
       "       device='mps:0', grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['attention_weights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"sur_model01\",\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=10,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return dict(accuracy=accuracy_score(predictions, labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|         | 500/13500 [01:45<43:35,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0818, 'grad_norm': 0.0, 'learning_rate': 1.925925925925926e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|         | 1000/13500 [03:28<45:42,  4.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0758, 'grad_norm': 0.0, 'learning_rate': 1.851851851851852e-05, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|         | 1500/13500 [05:12<42:13,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0668, 'grad_norm': 0.0, 'learning_rate': 1.7777777777777777e-05, 'epoch': 0.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|        | 2000/13500 [06:57<40:17,  4.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0879, 'grad_norm': 0.0, 'learning_rate': 1.7037037037037038e-05, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|        | 2500/13500 [08:41<37:57,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0409, 'grad_norm': 0.0, 'learning_rate': 1.6296296296296297e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|       | 3000/13500 [10:25<36:15,  4.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0952, 'grad_norm': 0.0, 'learning_rate': 1.555555555555556e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|       | 3501/13500 [12:10<33:33,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.1246, 'grad_norm': 0.0, 'learning_rate': 1.4814814814814815e-05, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|       | 4000/13500 [13:54<32:42,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.038, 'grad_norm': 0.0, 'learning_rate': 1.4074074074074075e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|      | 4500/13500 [15:38<29:02,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': -0.0617, 'grad_norm': 0.0, 'learning_rate': 1.3333333333333333e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 35.93 GB, other allocations: 2.95 MB, max allowed: 36.27 GB). Tried to allocate 882.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:2049\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2048\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2049\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2052\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available():\n\u001b[1;32m   2053\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:2412\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2410\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2412\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer.py:3444\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3442\u001b[0m         logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_logits_for_metrics(logits, labels)\n\u001b[1;32m   3443\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((logits))\n\u001b[0;32m-> 3444\u001b[0m     preds_host \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;28;01mif\u001b[39;00m preds_host \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3447\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function((labels))\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:123\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    120\u001b[0m     new_tensors\n\u001b[1;32m    121\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(new_tensors)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\u001b[43mnested_concat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001b[38;5;241m=\u001b[39mpadding_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:125\u001b[0m, in \u001b[0;36mnested_concat\u001b[0;34m(tensors, new_tensors, padding_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(nested_concat(t, n, padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m t, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(tensors, new_tensors))\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch_pad_and_concatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensors, Mapping):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(tensors)(\n\u001b[1;32m    128\u001b[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001b[38;5;241m=\u001b[39mpadding_index) \u001b[38;5;28;01mfor\u001b[39;00m k, t \u001b[38;5;129;01min\u001b[39;00m tensors\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/ts/lib/python3.11/site-packages/transformers/trainer_pt_utils.py:84\u001b[0m, in \u001b[0;36mtorch_pad_and_concatenate\u001b[0;34m(tensor1, tensor2, padding_index)\u001b[0m\n\u001b[1;32m     81\u001b[0m tensor2 \u001b[38;5;241m=\u001b[39m atleast_1d(tensor2)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Let's figure out the new shape\u001b[39;00m\n\u001b[1;32m     87\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mmax\u001b[39m(tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], tensor2\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])) \u001b[38;5;241m+\u001b[39m tensor1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 35.93 GB, other allocations: 2.95 MB, max allowed: 36.27 GB). Tried to allocate 882.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=dataset['train'][:512]['image'], return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][:512]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
