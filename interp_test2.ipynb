{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "from load_data import (load_tourism,\n",
    "                       create_train_dataloader,\n",
    "                       create_backtest_dataloader,\n",
    "                       create_test_dataloader)\n",
    "from sur_ts import SurrogateTimeSeriesTransformerConfig, SurrogateTimeSeriesTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = '1M'\n",
    "train_dataset, test_dataset, lags_sequence, time_features = load_tourism()\n",
    "prediction_length = 24\n",
    "\n",
    "\n",
    "config = SurrogateTimeSeriesTransformerConfig(\n",
    "    prediction_length=prediction_length,\n",
    "    # context length:\n",
    "    context_length=prediction_length * 2,\n",
    "    # lags coming from helper given the freq:\n",
    "    lags_sequence=lags_sequence,\n",
    "    # we'll add 2 time features (\"month of year\" and \"age\", see further):\n",
    "    num_time_features=len(time_features) + 1,\n",
    "    # we have a single static categorical feature, namely time series ID:\n",
    "    num_static_categorical_features=1,\n",
    "    # it has 366 possible values:\n",
    "    cardinality=[len(train_dataset)],\n",
    "    # the model will learn an embedding of size 2 for each of the 366 possible values:\n",
    "    embedding_dimension=[2],\n",
    "    \n",
    "    # transformer params:\n",
    "    encoder_layers=4,\n",
    "    decoder_layers=4,\n",
    "    d_model=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = create_train_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=train_dataset,\n",
    "    batch_size=256,\n",
    "    num_batches_per_epoch=100,\n",
    ")\n",
    "\n",
    "test_dataloader = create_backtest_dataloader(\n",
    "    config=config,\n",
    "    freq=freq,\n",
    "    data=test_dataset,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sur_model = SurrogateTimeSeriesTransformer(config)\n",
    "batch = next(iter(train_dataloader))\n",
    "sur_outputs = sur_model(\n",
    "    past_values=batch[\"past_values\"],\n",
    "    past_time_features=batch[\"past_time_features\"],\n",
    "    past_observed_mask=batch[\"past_observed_mask\"],\n",
    "    static_categorical_features=batch[\"static_categorical_features\"]\n",
    "    if config.num_static_categorical_features > 0\n",
    "    else None,\n",
    "    static_real_features=batch[\"static_real_features\"]\n",
    "    if config.num_static_real_features > 0\n",
    "    else None,\n",
    "    future_values=batch[\"future_values\"],\n",
    "    future_time_features=batch[\"future_time_features\"],\n",
    "    future_observed_mask=batch[\"future_observed_mask\"],\n",
    "    output_hidden_states=True,\n",
    ")\n",
    "print(\"attention_output shape: \" ,sur_outputs.attention_output.shape)\n",
    "print(\"attention_weights shape: \" ,sur_outputs.attention_weights.shape)\n",
    "print(\"last_encoder_state shape: \" ,sur_outputs.encoder_hidden_states[-1].shape)\n",
    "print(\"last_decoder_state shape: \" ,sur_outputs.decoder_hidden_states[-1].shape)\n",
    "print(\"Surrogate loss: \", sur_outputs.sur_loss)\n",
    "print(\"Prediction loss: \", sur_outputs.pred_loss)\n",
    "print(\"Total loss: \", sur_outputs.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test how many batch per epoch\n",
    "# for idx, batch in enumerate(train_dataloader):\n",
    "#     print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device\n",
    "# device = 'cpu'\n",
    "\n",
    "sur_model.to(device)\n",
    "optimizer = AdamW(sur_model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)\n",
    "\n",
    "sur_model, optimizer, train_dataloader = accelerator.prepare(\n",
    "    sur_model,\n",
    "    optimizer,\n",
    "    train_dataloader,\n",
    ")\n",
    "\n",
    "sur_model.train()\n",
    "\n",
    "print(f\"Started training: \")\n",
    "for epoch in range(40):\n",
    "    for idx, batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = sur_model(\n",
    "            static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "            if config.num_static_categorical_features > 0\n",
    "            else None,\n",
    "            static_real_features=batch[\"static_real_features\"].to(device)\n",
    "            if config.num_static_real_features > 0\n",
    "            else None,\n",
    "            past_time_features=batch[\"past_time_features\"].to(device),\n",
    "            past_values=batch[\"past_values\"].to(device),\n",
    "            future_time_features=batch[\"future_time_features\"].to(device),\n",
    "            future_values=batch[\"future_values\"].to(device),\n",
    "            past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "            future_observed_mask=batch[\"future_observed_mask\"].to(device),\n",
    "            output_hidden_states=True,\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagation\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            pred_loss = outputs.pred_loss\n",
    "            sur_loss = outputs.sur_loss\n",
    "            print(f\"Epoch [{epoch+1}] Step [{idx+1}]: loss: {loss.item():.4f},\\tpred loss: \"\n",
    "                  f\"{pred_loss.item():.4f},\\tsur loss: {sur_loss.item():.4f}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the file path to save the model\n",
    "model_path = \"model2\"\n",
    "\n",
    "# Save the model\n",
    "sur_model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sur_model.eval()\n",
    "\n",
    "forecasts = []\n",
    "\n",
    "for batch in test_dataloader:\n",
    "    outputs = sur_model.generate(\n",
    "        static_categorical_features=batch[\"static_categorical_features\"].to(device)\n",
    "        if config.num_static_categorical_features > 0\n",
    "        else None,\n",
    "        static_real_features=batch[\"static_real_features\"].to(device)\n",
    "        if config.num_static_real_features > 0\n",
    "        else None,\n",
    "        past_time_features=batch[\"past_time_features\"].to(device),\n",
    "        past_values=batch[\"past_values\"].to(device),\n",
    "        future_time_features=batch[\"future_time_features\"].to(device),\n",
    "        past_observed_mask=batch[\"past_observed_mask\"].to(device),\n",
    "        # future_observed_mask=None,\n",
    "    )\n",
    "    forecasts.append(outputs.sequences.cpu().numpy())\n",
    "forecasts = np.vstack(forecasts)\n",
    "print(forecasts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "mase_metric = load(\"evaluate-metric/mase\")\n",
    "smape_metric = load(\"evaluate-metric/smape\")\n",
    "\n",
    "forecast_median = np.median(forecasts, 1)\n",
    "\n",
    "mase_metrics = []\n",
    "smape_metrics = []\n",
    "for item_id, ts in enumerate(test_dataset):\n",
    "    training_data = ts[\"target\"][:-prediction_length]\n",
    "    ground_truth = ts[\"target\"][-prediction_length:]\n",
    "    mase = mase_metric.compute(\n",
    "        predictions=forecast_median[item_id], \n",
    "        references=np.array(ground_truth), \n",
    "        training=np.array(training_data), \n",
    "        periodicity=get_seasonality(freq))\n",
    "    mase_metrics.append(mase[\"mase\"])\n",
    "    \n",
    "    smape = smape_metric.compute(\n",
    "        predictions=forecast_median[item_id], \n",
    "        references=np.array(ground_truth), \n",
    "    )\n",
    "    smape_metrics.append(smape[\"smape\"])\n",
    "\n",
    "print(f\"MASE: {np.mean(mase_metrics)}\")\n",
    "print(f\"sMAPE: {np.mean(smape_metrics)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
