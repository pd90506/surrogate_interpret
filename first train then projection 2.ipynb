{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=5, bottleneck_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            shortcut_layers = []\n",
    "            shortcut_layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.ReLU())  # Using ReLU for simplicity; you can choose other activations as needed\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, hidden_dim))\n",
    "            shortcut_layers.append(nn.BatchNorm1d(num_features=hidden_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            self.layers.append(nn.Sequential(*shortcut_layers))\n",
    "\n",
    "        self.output_layer= nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.layers:\n",
    "            residual = layer(x)\n",
    "            x = x + residual # shortcut\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "def pairwise_cosine_similarity(Q, K):\n",
    "    \"\"\"\n",
    "    Q: (N, d)\n",
    "    K: (N, L, d)\n",
    "    \"\"\"\n",
    "    attention_scores = torch.matmul(Q.unsqueeze(1), K.transpose(-2, -1)).squeeze(1) # [N, L]\n",
    "    # denominator = torch.sqrt((Q**2).sum(-1).unsqueeze(-1) * (K**2).sum(-1).unsqueeze(-2))\n",
    "    denominator = (K**2).sum(-1) # [N, L]\n",
    "    attention_weights = attention_scores / (denominator + 1e-5)\n",
    "    return attention_weights # [N, L]\n",
    "\n",
    "# def pinv_float32(A):\n",
    "#     \"\"\"\n",
    "#     Compute the Moore-Penrose pseudoinverse of a matrix using SVD,\n",
    "#     ensuring all operations are performed in float32 precision.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - A: The input tensor of shape [..., M, N]\n",
    "    \n",
    "#     Returns:\n",
    "#     - The pseudoinverse of A with float32 precision.\n",
    "#     \"\"\"\n",
    "#     # Ensure A is float32\n",
    "#     A = A.to(dtype=torch.float32)\n",
    "    \n",
    "#     # Compute SVD\n",
    "#     U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "#     # Invert S with thresholding (regularization)\n",
    "#     S_inv = torch.where(S > 1e-15, 1.0 / S, torch.tensor(0.0, device=S.device, dtype=S.dtype))\n",
    "    \n",
    "#     # Compute pseudoinverse\n",
    "#     A_pinv = torch.matmul(Vh.transpose(-2, -1), torch.matmul(torch.diag_embed(S_inv), U.transpose(-2, -1)))\n",
    "    \n",
    "#     return A_pinv\n",
    "\n",
    "\n",
    "class SimplifiedAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SimplifiedAttention, self).__init__()\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        # self.value = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        q: (N, d)\n",
    "        k: (N, L, d)\n",
    "        v: (N, L, d)\n",
    "        \"\"\"\n",
    "        Q = q # self.query(q) # [N, d]\n",
    "        K = k # self.key(k) # [N, L, d]\n",
    "        V = v # [N, L, d]\n",
    "        Q_gate = self.query(q) # [N, d]\n",
    "        K_gate = self.key(k) # [N, L, d]\n",
    "\n",
    "        gate = torch.matmul(K_gate, Q_gate.unsqueeze(-1)).squeeze(-1) # [N, L]\n",
    "        gate = torch.sigmoid(gate) # [N, L]\n",
    "\n",
    "        \n",
    "        # Compute the attention scores [N, L]\n",
    "        # attention_scores = torch.matmul(Q.unsqueeze(1), K.transpose(-2, -1)).squeeze(1) / torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))\n",
    "        attention_weights = pairwise_cosine_similarity(Q, K) # [N, L]\n",
    "        # attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values using the attention weights\n",
    "        attention_outputs = attention_weights.unsqueeze(-1) * V # (N, L, d)\n",
    "        # pseudo_inverse_K = pinv_float32(K.transpose(-2, -1)) # get pseudoinverse K(K^T K)^{-1} # [N, L, d]\n",
    "\n",
    "        # zoomed_projection = torch.matmul(K, q.unsqueeze(-1)).squeeze(-1) # get a direct projection, which is not rescaled # [N, L]\n",
    "\n",
    "        # projection = torch.matmal(pseudo_inverse_K.transpose(-2, -1), zoomed_projection.unsqueeze(-1)).squeeze(-1) # [N, d]\n",
    "        # projection = pseudo_inverse_K * zoomed_projection.unsqueeze(-1) # [N, L, d]\n",
    "\n",
    "        return attention_outputs, gate  # Return  [N, L, d], [N, L]\n",
    "\n",
    "        # return projection, gate  # Return  [N, L, d], [N, L]\n",
    "\n",
    "\n",
    "def normalize(x, device):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).reshape(1,-1,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).reshape(1,-1,1,1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SurrogateInterpretation(nn.Module):\n",
    "    def __init__(self, query_vector_size, classifier, num_labels) -> None:\n",
    "        \"\"\"\n",
    "        pred_model: prediction model\n",
    "        classifier_head: last fully connected layer \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # self.feature_extractor = feature_extractor\n",
    "        self.hidden_size = query_vector_size\n",
    "        patch_size = 16\n",
    "        n_patches = (224 * 224) // (patch_size * patch_size)\n",
    "        self.patch_embedding = nn.Conv2d(3, query_vector_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "\n",
    "        # Transform function to non-linearly transform feature map embedding to the representation space\n",
    "        self.transform_func = MLP(input_dim=query_vector_size,\n",
    "                                  hidden_dim=query_vector_size,\n",
    "                                  output_dim=query_vector_size,\n",
    "                                  num_blocks=5,\n",
    "                                  bottleneck_dim=64)\n",
    "        self.attention = SimplifiedAttention(embed_size=query_vector_size)\n",
    "        # self.classifer = nn.Linear(query_vector_size, num_labels, bias=False)\n",
    "        self.classifier = classifier # (N, d) -> (N, 1000)\n",
    "        \n",
    "        self.cls_loss_func = nn.CrossEntropyLoss()\n",
    "        # self.sim_loss_func = nn.MSELoss()\n",
    "        # # self.cls_loss_func = nn.NLLLoss()\n",
    "        # self.kl_loss = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "        # self.cossim_loss_func = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, pred, pseudo_label, gate):    \n",
    "        cls_loss = self.cls_loss_func(pred, pseudo_label)\n",
    "        # gate_loss = torch.mean(gate.sum(-1))\n",
    "\n",
    "        loss = cls_loss \n",
    "\n",
    "        return {'loss':loss, \n",
    "                'cls_loss': cls_loss,\n",
    "                # 'gate_loss': gate_loss\n",
    "                }\n",
    "        \n",
    "    \n",
    "    def forward(self, pixel_values, query_vector, pseudo_probs, labels=None):\n",
    "        \"\"\" \n",
    "        pixel_values: [N, 3, 224, 224]\n",
    "        query_vector: [N, d]\n",
    "        pseudo_probs: [N, 1000]\n",
    "        label: [N,]\n",
    "        \"\"\"\n",
    "        patch_features = self.patch_embedding(pixel_values) # [N, d, sqrt(L), sqrt(L)] \n",
    "        patch_features = torch.flatten(patch_features, start_dim=-2).transpose(1, 2) # [N, L, d]\n",
    "        shape = patch_features.shape\n",
    "        feature_reprs = self.transform_func(patch_features.reshape(-1, self.hidden_size)).reshape(shape) # [N, L, d]\n",
    "\n",
    "        projection, gate = self.attention(\n",
    "            query_vector, # [N, d]\n",
    "            feature_reprs, # [N, L, d]\n",
    "            feature_reprs, # [N, L, d]\n",
    "        ) # Return  [N, L, d], [N, L]\n",
    "\n",
    "        # Get the pseudo labels and interpretable predictions\n",
    "\n",
    "        # pred = self.classifer(torch.mean(attention_output, dim=(-2,-1))) # (N, L, H, W) -> (N, L) -> (N, 1000)\n",
    "        # avgpooled = torch.mean(feature_reprs, dim=-1) # [N, L]\n",
    "        # approx_hidden_state = torch.sum(projection * gate.unsqueeze(-1), dim=-2) # (N, d)\n",
    "        approx_hidden_state = torch.sum(feature_reprs, dim=-2) #  [N, d]\n",
    "        pred = self.classifier(approx_hidden_state) # [N, 1000]\n",
    "        \n",
    "        pred_labels = pred.argmax(-1).view(-1) # (N,)\n",
    "\n",
    "        pseudo_label = pseudo_probs.argmax(-1) # [N,]\n",
    "        pseudo_label = pseudo_label.contiguous().view(-1) # (N,)\n",
    "\n",
    "        # compute loss\n",
    "        loss_dict = self.compute_loss(pred, pseudo_label, gate)\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        accuracy = (pred_labels == pseudo_label).sum() / len(pseudo_label)\n",
    "\n",
    "        if labels is not None:\n",
    "            pred_accuracy = (pseudo_label == labels).sum() / len(labels)\n",
    "\n",
    "        # outputs['feature_maps'] = feature_maps\n",
    "        # outputs['attention_output'] = attention_output\n",
    "        # outputs['attention_weights'] = attention_weights\n",
    "        outputs['query_vector'] = query_vector\n",
    "        outputs['loss'] = loss\n",
    "        # outputs['gate_loss'] = loss_dict['gate_loss']\n",
    "        outputs['cls_loss'] = loss_dict['cls_loss']\n",
    "        outputs['acc'] = accuracy\n",
    "        outputs['pred_acc'] = pred_accuracy if labels is not None else None\n",
    "        # outputs['pred_split'] = pred_split\n",
    "        outputs['pred'] = pred\n",
    "        # outputs['pred_split'] = pred_split\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import (CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "dataset = dataset['train']\n",
    "splits = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([256, 3, 224, 224])\n",
      "labels torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=1024, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_vector shape:  torch.Size([1, 768])\n",
      "loss: 95.19258880615234, acc: 0.0, pred_acc: 0.7421875 cls_loss: 95.19258880615234\n",
      "loss: 99.4499282836914, acc: 0.0, pred_acc: 0.77734375 cls_loss: 99.4499282836914\n",
      "loss: 106.68620300292969, acc: 0.00390625, pred_acc: 0.7578125 cls_loss: 106.68620300292969\n",
      "loss: 93.98472595214844, acc: 0.0, pred_acc: 0.79296875 cls_loss: 93.98472595214844\n",
      "loss: 96.31971740722656, acc: 0.00390625, pred_acc: 0.7421875 cls_loss: 96.31971740722656\n",
      "loss: 96.3554458618164, acc: 0.0, pred_acc: 0.7578125 cls_loss: 96.3554458618164\n",
      "loss: 92.01483154296875, acc: 0.00390625, pred_acc: 0.82421875 cls_loss: 92.01483154296875\n",
      "loss: 94.90765380859375, acc: 0.0078125, pred_acc: 0.75390625 cls_loss: 94.90765380859375\n",
      "loss: 89.65113067626953, acc: 0.0, pred_acc: 0.73046875 cls_loss: 89.65113067626953\n",
      "loss: 81.56440734863281, acc: 0.0, pred_acc: 0.75390625 cls_loss: 81.56440734863281\n",
      "loss: 86.7008056640625, acc: 0.0, pred_acc: 0.7578125 cls_loss: 86.7008056640625\n",
      "loss: 92.05384826660156, acc: 0.00390625, pred_acc: 0.80078125 cls_loss: 92.05384826660156\n",
      "loss: 88.28941345214844, acc: 0.00390625, pred_acc: 0.7890625 cls_loss: 88.28941345214844\n",
      "loss: 86.19365692138672, acc: 0.00390625, pred_acc: 0.7265625 cls_loss: 86.19365692138672\n",
      "loss: 84.30278778076172, acc: 0.00390625, pred_acc: 0.8203125 cls_loss: 84.30278778076172\n",
      "loss: 85.87908935546875, acc: 0.00390625, pred_acc: 0.765625 cls_loss: 85.87908935546875\n",
      "loss: 86.31060791015625, acc: 0.0, pred_acc: 0.75390625 cls_loss: 86.31060791015625\n",
      "loss: 85.35475158691406, acc: 0.0, pred_acc: 0.7421875 cls_loss: 85.35475158691406\n",
      "loss: 89.03736877441406, acc: 0.0, pred_acc: 0.7734375 cls_loss: 89.03736877441406\n",
      "loss: 80.61085510253906, acc: 0.0, pred_acc: 0.74609375 cls_loss: 80.61085510253906\n",
      "loss: 86.08485412597656, acc: 0.0, pred_acc: 0.8359375 cls_loss: 86.08485412597656\n",
      "loss: 81.85867309570312, acc: 0.00390625, pred_acc: 0.78515625 cls_loss: 81.85867309570312\n",
      "loss: 85.32206726074219, acc: 0.0, pred_acc: 0.76171875 cls_loss: 85.32206726074219\n",
      "loss: 85.83763885498047, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 85.83763885498047\n",
      "loss: 80.04695892333984, acc: 0.0, pred_acc: 0.80859375 cls_loss: 80.04695892333984\n",
      "loss: 81.90856170654297, acc: 0.0, pred_acc: 0.74609375 cls_loss: 81.90856170654297\n",
      "loss: 73.65238952636719, acc: 0.0, pred_acc: 0.7421875 cls_loss: 73.65238952636719\n",
      "loss: 74.36105346679688, acc: 0.00390625, pred_acc: 0.7421875 cls_loss: 74.36105346679688\n",
      "loss: 75.4517822265625, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 75.4517822265625\n",
      "loss: 73.78350830078125, acc: 0.0, pred_acc: 0.77734375 cls_loss: 73.78350830078125\n",
      "loss: 72.93438720703125, acc: 0.00390625, pred_acc: 0.765625 cls_loss: 72.93438720703125\n",
      "loss: 77.48918914794922, acc: 0.0078125, pred_acc: 0.76953125 cls_loss: 77.48918914794922\n",
      "loss: 77.83346557617188, acc: 0.0, pred_acc: 0.7578125 cls_loss: 77.83346557617188\n",
      "loss: 69.27963256835938, acc: 0.0, pred_acc: 0.796875 cls_loss: 69.27963256835938\n",
      "loss: 75.56785583496094, acc: 0.00390625, pred_acc: 0.75 cls_loss: 75.56785583496094\n",
      "loss: 73.20552062988281, acc: 0.0, pred_acc: 0.77734375 cls_loss: 73.20552062988281\n",
      "loss: 70.33682250976562, acc: 0.0, pred_acc: 0.7265625 cls_loss: 70.33682250976562\n",
      "loss: 68.88159942626953, acc: 0.0, pred_acc: 0.80078125 cls_loss: 68.88159942626953\n",
      "loss: 72.56568908691406, acc: 0.00390625, pred_acc: 0.71875 cls_loss: 72.56568908691406\n",
      "loss: 72.61634826660156, acc: 0.00390625, pred_acc: 0.734375 cls_loss: 72.61634826660156\n",
      "loss: 72.2371826171875, acc: 0.0, pred_acc: 0.76953125 cls_loss: 72.2371826171875\n",
      "loss: 73.31629180908203, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 73.31629180908203\n",
      "loss: 74.49896240234375, acc: 0.0, pred_acc: 0.71484375 cls_loss: 74.49896240234375\n",
      "loss: 69.67791748046875, acc: 0.00390625, pred_acc: 0.8203125 cls_loss: 69.67791748046875\n",
      "loss: 72.04782104492188, acc: 0.0, pred_acc: 0.73828125 cls_loss: 72.04782104492188\n",
      "loss: 70.9811782836914, acc: 0.0, pred_acc: 0.79296875 cls_loss: 70.9811782836914\n",
      "loss: 71.72763061523438, acc: 0.00390625, pred_acc: 0.76953125 cls_loss: 71.72763061523438\n",
      "loss: 68.88766479492188, acc: 0.01171875, pred_acc: 0.77734375 cls_loss: 68.88766479492188\n",
      "loss: 70.81532287597656, acc: 0.0, pred_acc: 0.7421875 cls_loss: 70.81532287597656\n",
      "loss: 69.00076293945312, acc: 0.0, pred_acc: 0.8125 cls_loss: 69.00076293945312\n",
      "loss: 71.42100524902344, acc: 0.0, pred_acc: 0.7265625 cls_loss: 71.42100524902344\n",
      "loss: 70.31645965576172, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 70.31645965576172\n",
      "loss: 72.25831604003906, acc: 0.0, pred_acc: 0.734375 cls_loss: 72.25831604003906\n",
      "loss: 72.17901611328125, acc: 0.0, pred_acc: 0.74609375 cls_loss: 72.17901611328125\n",
      "loss: 67.08596801757812, acc: 0.0, pred_acc: 0.76171875 cls_loss: 67.08596801757812\n",
      "loss: 71.25987243652344, acc: 0.0078125, pred_acc: 0.8125 cls_loss: 71.25987243652344\n",
      "loss: 68.32334899902344, acc: 0.0, pred_acc: 0.79296875 cls_loss: 68.32334899902344\n",
      "loss: 68.65998840332031, acc: 0.0, pred_acc: 0.73828125 cls_loss: 68.65998840332031\n",
      "loss: 73.28549194335938, acc: 0.0078125, pred_acc: 0.7421875 cls_loss: 73.28549194335938\n",
      "loss: 73.90514373779297, acc: 0.0, pred_acc: 0.76171875 cls_loss: 73.90514373779297\n",
      "loss: 70.65546417236328, acc: 0.0078125, pred_acc: 0.76171875 cls_loss: 70.65546417236328\n",
      "loss: 69.09696960449219, acc: 0.0, pred_acc: 0.78125 cls_loss: 69.09696960449219\n",
      "loss: 70.31753540039062, acc: 0.0, pred_acc: 0.8046875 cls_loss: 70.31753540039062\n",
      "loss: 72.38963317871094, acc: 0.0, pred_acc: 0.7421875 cls_loss: 72.38963317871094\n",
      "loss: 69.24918365478516, acc: 0.0, pred_acc: 0.76953125 cls_loss: 69.24918365478516\n",
      "loss: 68.11675262451172, acc: 0.00390625, pred_acc: 0.76953125 cls_loss: 68.11675262451172\n",
      "loss: 71.23763275146484, acc: 0.0, pred_acc: 0.72265625 cls_loss: 71.23763275146484\n",
      "loss: 72.40458679199219, acc: 0.0, pred_acc: 0.7734375 cls_loss: 72.40458679199219\n",
      "loss: 66.48689270019531, acc: 0.0, pred_acc: 0.765625 cls_loss: 66.48689270019531\n",
      "loss: 70.07380676269531, acc: 0.00390625, pred_acc: 0.75 cls_loss: 70.07380676269531\n",
      "loss: 69.76618957519531, acc: 0.00390625, pred_acc: 0.8046875 cls_loss: 69.76618957519531\n",
      "loss: 70.70664978027344, acc: 0.0, pred_acc: 0.7734375 cls_loss: 70.70664978027344\n",
      "loss: 70.98876190185547, acc: 0.0, pred_acc: 0.78515625 cls_loss: 70.98876190185547\n",
      "loss: 71.14604187011719, acc: 0.0, pred_acc: 0.765625 cls_loss: 71.14604187011719\n",
      "loss: 67.26421356201172, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 67.26421356201172\n",
      "loss: 76.81288146972656, acc: 0.0, pred_acc: 0.78515625 cls_loss: 76.81288146972656\n",
      "loss: 75.85163879394531, acc: 0.0, pred_acc: 0.80078125 cls_loss: 75.85163879394531\n",
      "loss: 73.45053100585938, acc: 0.0, pred_acc: 0.77734375 cls_loss: 73.45053100585938\n",
      "loss: 80.60221862792969, acc: 0.00390625, pred_acc: 0.75 cls_loss: 80.60221862792969\n",
      "loss: 72.6781005859375, acc: 0.0, pred_acc: 0.78515625 cls_loss: 72.6781005859375\n",
      "loss: 77.71516418457031, acc: 0.0, pred_acc: 0.77734375 cls_loss: 77.71516418457031\n",
      "loss: 76.91311645507812, acc: 0.0, pred_acc: 0.7890625 cls_loss: 76.91311645507812\n",
      "loss: 83.56985473632812, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 83.56985473632812\n",
      "loss: 69.57344055175781, acc: 0.0, pred_acc: 0.83203125 cls_loss: 69.57344055175781\n",
      "loss: 68.90708923339844, acc: 0.0, pred_acc: 0.76171875 cls_loss: 68.90708923339844\n",
      "loss: 72.82998657226562, acc: 0.0, pred_acc: 0.75 cls_loss: 72.82998657226562\n",
      "loss: 74.79708099365234, acc: 0.0, pred_acc: 0.7890625 cls_loss: 74.79708099365234\n",
      "loss: 75.46322631835938, acc: 0.0078125, pred_acc: 0.8203125 cls_loss: 75.46322631835938\n",
      "loss: 70.435302734375, acc: 0.0, pred_acc: 0.765625 cls_loss: 70.435302734375\n",
      "loss: 71.80812072753906, acc: 0.00390625, pred_acc: 0.7890625 cls_loss: 71.80812072753906\n",
      "loss: 71.28343200683594, acc: 0.0, pred_acc: 0.77734375 cls_loss: 71.28343200683594\n",
      "loss: 70.32527923583984, acc: 0.0, pred_acc: 0.73046875 cls_loss: 70.32527923583984\n",
      "loss: 68.49555206298828, acc: 0.0, pred_acc: 0.78125 cls_loss: 68.49555206298828\n",
      "loss: 74.308349609375, acc: 0.0, pred_acc: 0.75390625 cls_loss: 74.308349609375\n",
      "loss: 71.62422943115234, acc: 0.01171875, pred_acc: 0.78125 cls_loss: 71.62422943115234\n",
      "loss: 70.00176239013672, acc: 0.00390625, pred_acc: 0.79296875 cls_loss: 70.00176239013672\n",
      "loss: 69.31793212890625, acc: 0.00390625, pred_acc: 0.75390625 cls_loss: 69.31793212890625\n",
      "loss: 69.64810180664062, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 69.64810180664062\n",
      "loss: 70.33256530761719, acc: 0.0, pred_acc: 0.8046875 cls_loss: 70.33256530761719\n",
      "loss: 67.71358489990234, acc: 0.0, pred_acc: 0.734375 cls_loss: 67.71358489990234\n",
      "loss: 68.85274505615234, acc: 0.0, pred_acc: 0.7265625 cls_loss: 68.85274505615234\n",
      "loss: 66.83899688720703, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 66.83899688720703\n",
      "loss: 65.59794616699219, acc: 0.0, pred_acc: 0.75 cls_loss: 65.59794616699219\n",
      "loss: 75.12184143066406, acc: 0.0, pred_acc: 0.70703125 cls_loss: 75.12184143066406\n",
      "loss: 77.97274780273438, acc: 0.00390625, pred_acc: 0.76953125 cls_loss: 77.97274780273438\n",
      "loss: 73.02926635742188, acc: 0.0, pred_acc: 0.77734375 cls_loss: 73.02926635742188\n",
      "loss: 68.96826171875, acc: 0.0, pred_acc: 0.79296875 cls_loss: 68.96826171875\n",
      "loss: 66.76716613769531, acc: 0.01171875, pred_acc: 0.77734375 cls_loss: 66.76716613769531\n",
      "loss: 72.8877182006836, acc: 0.0, pred_acc: 0.73828125 cls_loss: 72.8877182006836\n",
      "loss: 74.24540710449219, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 74.24540710449219\n",
      "loss: 67.56626892089844, acc: 0.0, pred_acc: 0.75390625 cls_loss: 67.56626892089844\n",
      "loss: 66.03641510009766, acc: 0.0, pred_acc: 0.78125 cls_loss: 66.03641510009766\n",
      "loss: 70.39662170410156, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 70.39662170410156\n",
      "loss: 71.36438751220703, acc: 0.00390625, pred_acc: 0.79296875 cls_loss: 71.36438751220703\n",
      "loss: 67.79365539550781, acc: 0.0, pred_acc: 0.7734375 cls_loss: 67.79365539550781\n",
      "loss: 70.49444580078125, acc: 0.00390625, pred_acc: 0.7265625 cls_loss: 70.49444580078125\n",
      "loss: 67.611083984375, acc: 0.00390625, pred_acc: 0.78125 cls_loss: 67.611083984375\n",
      "loss: 73.75651550292969, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 73.75651550292969\n",
      "loss: 69.94554901123047, acc: 0.0, pred_acc: 0.8046875 cls_loss: 69.94554901123047\n",
      "loss: 80.3313980102539, acc: 0.0078125, pred_acc: 0.7578125 cls_loss: 80.3313980102539\n",
      "loss: 82.94297790527344, acc: 0.0, pred_acc: 0.73046875 cls_loss: 82.94297790527344\n",
      "loss: 92.2677993774414, acc: 0.0, pred_acc: 0.7890625 cls_loss: 92.2677993774414\n",
      "loss: 79.62953186035156, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 79.62953186035156\n",
      "loss: 94.82331085205078, acc: 0.0, pred_acc: 0.80078125 cls_loss: 94.82331085205078\n",
      "loss: 90.22611999511719, acc: 0.0, pred_acc: 0.73828125 cls_loss: 90.22611999511719\n",
      "loss: 73.34976959228516, acc: 0.0, pred_acc: 0.78125 cls_loss: 73.34976959228516\n",
      "loss: 77.99960327148438, acc: 0.0, pred_acc: 0.76953125 cls_loss: 77.99960327148438\n",
      "loss: 77.45384216308594, acc: 0.0, pred_acc: 0.76171875 cls_loss: 77.45384216308594\n",
      "loss: 77.33354187011719, acc: 0.0, pred_acc: 0.76953125 cls_loss: 77.33354187011719\n",
      "loss: 73.47579956054688, acc: 0.00390625, pred_acc: 0.74609375 cls_loss: 73.47579956054688\n",
      "loss: 73.51217651367188, acc: 0.0, pred_acc: 0.765625 cls_loss: 73.51217651367188\n",
      "loss: 73.34194946289062, acc: 0.0, pred_acc: 0.77734375 cls_loss: 73.34194946289062\n",
      "loss: 73.15563201904297, acc: 0.00390625, pred_acc: 0.765625 cls_loss: 73.15563201904297\n",
      "loss: 76.20171356201172, acc: 0.00390625, pred_acc: 0.7421875 cls_loss: 76.20171356201172\n",
      "loss: 77.85243225097656, acc: 0.00390625, pred_acc: 0.7734375 cls_loss: 77.85243225097656\n",
      "loss: 72.5149917602539, acc: 0.00390625, pred_acc: 0.80078125 cls_loss: 72.5149917602539\n",
      "loss: 77.68223571777344, acc: 0.0, pred_acc: 0.75 cls_loss: 77.68223571777344\n",
      "loss: 82.01132202148438, acc: 0.0, pred_acc: 0.75390625 cls_loss: 82.01132202148438\n",
      "loss: 83.15888214111328, acc: 0.0, pred_acc: 0.7265625 cls_loss: 83.15888214111328\n",
      "loss: 82.40020751953125, acc: 0.0, pred_acc: 0.7578125 cls_loss: 82.40020751953125\n",
      "loss: 70.83045959472656, acc: 0.0078125, pred_acc: 0.7734375 cls_loss: 70.83045959472656\n",
      "loss: 84.72776794433594, acc: 0.0, pred_acc: 0.78515625 cls_loss: 84.72776794433594\n",
      "loss: 79.43319702148438, acc: 0.00390625, pred_acc: 0.7578125 cls_loss: 79.43319702148438\n",
      "loss: 87.7381591796875, acc: 0.00390625, pred_acc: 0.8125 cls_loss: 87.7381591796875\n",
      "loss: 76.81475830078125, acc: 0.0, pred_acc: 0.75390625 cls_loss: 76.81475830078125\n",
      "loss: 85.18240356445312, acc: 0.0, pred_acc: 0.71484375 cls_loss: 85.18240356445312\n",
      "loss: 72.98570251464844, acc: 0.0, pred_acc: 0.77734375 cls_loss: 72.98570251464844\n",
      "loss: 73.97315979003906, acc: 0.0, pred_acc: 0.765625 cls_loss: 73.97315979003906\n",
      "loss: 76.96171569824219, acc: 0.0, pred_acc: 0.7734375 cls_loss: 76.96171569824219\n",
      "loss: 69.42236328125, acc: 0.00390625, pred_acc: 0.8203125 cls_loss: 69.42236328125\n",
      "loss: 69.62155151367188, acc: 0.0, pred_acc: 0.76171875 cls_loss: 69.62155151367188\n",
      "loss: 72.54386901855469, acc: 0.0078125, pred_acc: 0.80859375 cls_loss: 72.54386901855469\n",
      "loss: 71.9921875, acc: 0.00390625, pred_acc: 0.734375 cls_loss: 71.9921875\n",
      "loss: 70.78919982910156, acc: 0.0, pred_acc: 0.80078125 cls_loss: 70.78919982910156\n",
      "loss: 73.8655776977539, acc: 0.01171875, pred_acc: 0.72265625 cls_loss: 73.8655776977539\n",
      "loss: 72.83564758300781, acc: 0.0, pred_acc: 0.7890625 cls_loss: 72.83564758300781\n",
      "loss: 68.96160888671875, acc: 0.00390625, pred_acc: 0.78125 cls_loss: 68.96160888671875\n",
      "loss: 74.49992370605469, acc: 0.0, pred_acc: 0.7734375 cls_loss: 74.49992370605469\n",
      "loss: 72.34207153320312, acc: 0.0, pred_acc: 0.78515625 cls_loss: 72.34207153320312\n",
      "loss: 70.33502197265625, acc: 0.0, pred_acc: 0.75 cls_loss: 70.33502197265625\n",
      "loss: 77.59927368164062, acc: 0.0, pred_acc: 0.80078125 cls_loss: 77.59927368164062\n",
      "loss: 71.54806518554688, acc: 0.00390625, pred_acc: 0.74609375 cls_loss: 71.54806518554688\n",
      "loss: 70.54600524902344, acc: 0.00390625, pred_acc: 0.74609375 cls_loss: 70.54600524902344\n",
      "loss: 67.81534576416016, acc: 0.00390625, pred_acc: 0.765625 cls_loss: 67.81534576416016\n",
      "loss: 70.17975616455078, acc: 0.00390625, pred_acc: 0.78125 cls_loss: 70.17975616455078\n",
      "loss: 67.13027954101562, acc: 0.0, pred_acc: 0.79296875 cls_loss: 67.13027954101562\n",
      "loss: 69.90003967285156, acc: 0.00390625, pred_acc: 0.73046875 cls_loss: 69.90003967285156\n",
      "loss: 66.78946685791016, acc: 0.0, pred_acc: 0.73828125 cls_loss: 66.78946685791016\n",
      "loss: 66.14912414550781, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 66.14912414550781\n",
      "loss: 70.93962097167969, acc: 0.00390625, pred_acc: 0.734375 cls_loss: 70.93962097167969\n",
      "loss: 67.32911682128906, acc: 0.0078125, pred_acc: 0.80078125 cls_loss: 67.32911682128906\n",
      "loss: 65.00189208984375, acc: 0.0, pred_acc: 0.7421875 cls_loss: 65.00189208984375\n",
      "loss: 65.03889465332031, acc: 0.0, pred_acc: 0.75390625 cls_loss: 65.03889465332031\n",
      "loss: 65.47940826416016, acc: 0.0, pred_acc: 0.71875 cls_loss: 65.47940826416016\n",
      "loss: 66.7601318359375, acc: 0.0078125, pred_acc: 0.7578125 cls_loss: 66.7601318359375\n",
      "loss: 63.336185455322266, acc: 0.004999999888241291, pred_acc: 0.7599999904632568 cls_loss: 63.336185455322266\n",
      "loss: 61.270423889160156, acc: 0.0, pred_acc: 0.75390625 cls_loss: 61.270423889160156\n",
      "loss: 61.77691650390625, acc: 0.0, pred_acc: 0.7421875 cls_loss: 61.77691650390625\n",
      "loss: 62.7757453918457, acc: 0.0, pred_acc: 0.78125 cls_loss: 62.7757453918457\n",
      "loss: 65.67324829101562, acc: 0.0, pred_acc: 0.7265625 cls_loss: 65.67324829101562\n",
      "loss: 62.63056564331055, acc: 0.00390625, pred_acc: 0.7734375 cls_loss: 62.63056564331055\n",
      "loss: 65.54493713378906, acc: 0.0, pred_acc: 0.7578125 cls_loss: 65.54493713378906\n",
      "loss: 62.465396881103516, acc: 0.0, pred_acc: 0.765625 cls_loss: 62.465396881103516\n",
      "loss: 58.78777313232422, acc: 0.0, pred_acc: 0.75 cls_loss: 58.78777313232422\n",
      "loss: 62.95452117919922, acc: 0.0078125, pred_acc: 0.76171875 cls_loss: 62.95452117919922\n",
      "loss: 59.799163818359375, acc: 0.0, pred_acc: 0.78125 cls_loss: 59.799163818359375\n",
      "loss: 63.04150390625, acc: 0.0, pred_acc: 0.73828125 cls_loss: 63.04150390625\n",
      "loss: 62.446937561035156, acc: 0.0, pred_acc: 0.78125 cls_loss: 62.446937561035156\n",
      "loss: 61.7716064453125, acc: 0.00390625, pred_acc: 0.734375 cls_loss: 61.7716064453125\n",
      "loss: 63.029808044433594, acc: 0.0, pred_acc: 0.75390625 cls_loss: 63.029808044433594\n",
      "loss: 59.886695861816406, acc: 0.0, pred_acc: 0.71484375 cls_loss: 59.886695861816406\n",
      "loss: 59.57946014404297, acc: 0.00390625, pred_acc: 0.80078125 cls_loss: 59.57946014404297\n",
      "loss: 58.757957458496094, acc: 0.0, pred_acc: 0.7421875 cls_loss: 58.757957458496094\n",
      "loss: 59.419288635253906, acc: 0.0, pred_acc: 0.8125 cls_loss: 59.419288635253906\n",
      "loss: 57.338157653808594, acc: 0.0, pred_acc: 0.76171875 cls_loss: 57.338157653808594\n",
      "loss: 62.61566925048828, acc: 0.0, pred_acc: 0.70703125 cls_loss: 62.61566925048828\n",
      "loss: 60.39715576171875, acc: 0.0078125, pred_acc: 0.7734375 cls_loss: 60.39715576171875\n",
      "loss: 59.64204788208008, acc: 0.0, pred_acc: 0.76171875 cls_loss: 59.64204788208008\n",
      "loss: 63.165260314941406, acc: 0.0, pred_acc: 0.79296875 cls_loss: 63.165260314941406\n",
      "loss: 60.59405517578125, acc: 0.0, pred_acc: 0.75 cls_loss: 60.59405517578125\n",
      "loss: 58.123809814453125, acc: 0.0, pred_acc: 0.76171875 cls_loss: 58.123809814453125\n",
      "loss: 58.4282112121582, acc: 0.0078125, pred_acc: 0.79296875 cls_loss: 58.4282112121582\n",
      "loss: 64.19242858886719, acc: 0.00390625, pred_acc: 0.78515625 cls_loss: 64.19242858886719\n",
      "loss: 66.19517517089844, acc: 0.00390625, pred_acc: 0.75390625 cls_loss: 66.19517517089844\n",
      "loss: 62.37582015991211, acc: 0.0, pred_acc: 0.7890625 cls_loss: 62.37582015991211\n",
      "loss: 69.87751770019531, acc: 0.0, pred_acc: 0.7890625 cls_loss: 69.87751770019531\n",
      "loss: 79.07721710205078, acc: 0.0, pred_acc: 0.77734375 cls_loss: 79.07721710205078\n",
      "loss: 84.0284194946289, acc: 0.0, pred_acc: 0.74609375 cls_loss: 84.0284194946289\n",
      "loss: 68.6798095703125, acc: 0.0, pred_acc: 0.6953125 cls_loss: 68.6798095703125\n",
      "loss: 65.09310913085938, acc: 0.0, pred_acc: 0.74609375 cls_loss: 65.09310913085938\n",
      "loss: 71.67839050292969, acc: 0.0078125, pred_acc: 0.75390625 cls_loss: 71.67839050292969\n",
      "loss: 69.48356628417969, acc: 0.00390625, pred_acc: 0.76953125 cls_loss: 69.48356628417969\n",
      "loss: 69.92951965332031, acc: 0.00390625, pred_acc: 0.796875 cls_loss: 69.92951965332031\n",
      "loss: 71.04884338378906, acc: 0.00390625, pred_acc: 0.7109375 cls_loss: 71.04884338378906\n",
      "loss: 76.77021789550781, acc: 0.0, pred_acc: 0.79296875 cls_loss: 76.77021789550781\n",
      "loss: 74.12921905517578, acc: 0.00390625, pred_acc: 0.82421875 cls_loss: 74.12921905517578\n",
      "loss: 95.2766342163086, acc: 0.0, pred_acc: 0.76953125 cls_loss: 95.2766342163086\n",
      "loss: 81.12142944335938, acc: 0.0, pred_acc: 0.76953125 cls_loss: 81.12142944335938\n",
      "loss: 72.40567016601562, acc: 0.0078125, pred_acc: 0.7578125 cls_loss: 72.40567016601562\n",
      "loss: 69.7828369140625, acc: 0.0, pred_acc: 0.75 cls_loss: 69.7828369140625\n",
      "loss: 79.11976623535156, acc: 0.0, pred_acc: 0.77734375 cls_loss: 79.11976623535156\n",
      "loss: 94.48628234863281, acc: 0.0, pred_acc: 0.7578125 cls_loss: 94.48628234863281\n",
      "loss: 86.98545837402344, acc: 0.0, pred_acc: 0.82421875 cls_loss: 86.98545837402344\n",
      "loss: 111.10159301757812, acc: 0.0, pred_acc: 0.74609375 cls_loss: 111.10159301757812\n",
      "loss: 87.734130859375, acc: 0.0, pred_acc: 0.80078125 cls_loss: 87.734130859375\n",
      "loss: 79.90003967285156, acc: 0.0, pred_acc: 0.80078125 cls_loss: 79.90003967285156\n",
      "loss: 80.30860900878906, acc: 0.0, pred_acc: 0.75390625 cls_loss: 80.30860900878906\n",
      "loss: 84.63386535644531, acc: 0.00390625, pred_acc: 0.74609375 cls_loss: 84.63386535644531\n",
      "loss: 82.11276245117188, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 82.11276245117188\n",
      "loss: 82.71513366699219, acc: 0.0, pred_acc: 0.75390625 cls_loss: 82.71513366699219\n",
      "loss: 77.09835815429688, acc: 0.01171875, pred_acc: 0.81640625 cls_loss: 77.09835815429688\n",
      "loss: 79.19027709960938, acc: 0.0, pred_acc: 0.765625 cls_loss: 79.19027709960938\n",
      "loss: 84.78315734863281, acc: 0.0, pred_acc: 0.75 cls_loss: 84.78315734863281\n",
      "loss: 90.26544189453125, acc: 0.0, pred_acc: 0.7734375 cls_loss: 90.26544189453125\n",
      "loss: 115.648193359375, acc: 0.0, pred_acc: 0.8125 cls_loss: 115.648193359375\n",
      "loss: 117.93806457519531, acc: 0.0078125, pred_acc: 0.80859375 cls_loss: 117.93806457519531\n",
      "loss: 102.62689971923828, acc: 0.0, pred_acc: 0.765625 cls_loss: 102.62689971923828\n",
      "loss: 119.5416488647461, acc: 0.00390625, pred_acc: 0.76953125 cls_loss: 119.5416488647461\n",
      "loss: 102.29388427734375, acc: 0.0, pred_acc: 0.73828125 cls_loss: 102.29388427734375\n",
      "loss: 104.69914245605469, acc: 0.00390625, pred_acc: 0.78515625 cls_loss: 104.69914245605469\n",
      "loss: 100.22972106933594, acc: 0.00390625, pred_acc: 0.7734375 cls_loss: 100.22972106933594\n",
      "loss: 94.53489685058594, acc: 0.0, pred_acc: 0.81640625 cls_loss: 94.53489685058594\n",
      "loss: 101.18253326416016, acc: 0.0, pred_acc: 0.72265625 cls_loss: 101.18253326416016\n",
      "loss: 122.022705078125, acc: 0.0, pred_acc: 0.7890625 cls_loss: 122.022705078125\n",
      "loss: 136.20513916015625, acc: 0.0, pred_acc: 0.78125 cls_loss: 136.20513916015625\n",
      "loss: 142.1890411376953, acc: 0.00390625, pred_acc: 0.79296875 cls_loss: 142.1890411376953\n",
      "loss: 120.86095428466797, acc: 0.0, pred_acc: 0.72265625 cls_loss: 120.86095428466797\n",
      "loss: 111.11639404296875, acc: 0.00390625, pred_acc: 0.72265625 cls_loss: 111.11639404296875\n",
      "loss: 116.3464584350586, acc: 0.0, pred_acc: 0.80859375 cls_loss: 116.3464584350586\n",
      "loss: 98.56166076660156, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 98.56166076660156\n",
      "loss: 124.51773071289062, acc: 0.0, pred_acc: 0.734375 cls_loss: 124.51773071289062\n",
      "loss: 146.14511108398438, acc: 0.0, pred_acc: 0.7578125 cls_loss: 146.14511108398438\n",
      "loss: 170.7584228515625, acc: 0.0, pred_acc: 0.79296875 cls_loss: 170.7584228515625\n",
      "loss: 184.24569702148438, acc: 0.0, pred_acc: 0.78125 cls_loss: 184.24569702148438\n",
      "loss: 194.04153442382812, acc: 0.0, pred_acc: 0.79296875 cls_loss: 194.04153442382812\n",
      "loss: 170.94680786132812, acc: 0.0, pred_acc: 0.74609375 cls_loss: 170.94680786132812\n",
      "loss: 142.669189453125, acc: 0.00390625, pred_acc: 0.7109375 cls_loss: 142.669189453125\n",
      "loss: 126.85113525390625, acc: 0.0, pred_acc: 0.7421875 cls_loss: 126.85113525390625\n",
      "loss: 134.6899871826172, acc: 0.0, pred_acc: 0.77734375 cls_loss: 134.6899871826172\n",
      "loss: 133.1510772705078, acc: 0.0, pred_acc: 0.75390625 cls_loss: 133.1510772705078\n",
      "loss: 139.10586547851562, acc: 0.00390625, pred_acc: 0.73828125 cls_loss: 139.10586547851562\n",
      "loss: 149.67115783691406, acc: 0.00390625, pred_acc: 0.79296875 cls_loss: 149.67115783691406\n",
      "loss: 182.716064453125, acc: 0.0, pred_acc: 0.7421875 cls_loss: 182.716064453125\n",
      "loss: 183.8067626953125, acc: 0.00390625, pred_acc: 0.765625 cls_loss: 183.8067626953125\n",
      "loss: 146.22503662109375, acc: 0.0, pred_acc: 0.7890625 cls_loss: 146.22503662109375\n",
      "loss: 149.81439208984375, acc: 0.0, pred_acc: 0.765625 cls_loss: 149.81439208984375\n",
      "loss: 175.14752197265625, acc: 0.0, pred_acc: 0.79296875 cls_loss: 175.14752197265625\n",
      "loss: 147.63812255859375, acc: 0.00390625, pred_acc: 0.7890625 cls_loss: 147.63812255859375\n",
      "loss: 121.53388214111328, acc: 0.0, pred_acc: 0.75390625 cls_loss: 121.53388214111328\n",
      "loss: 142.5609588623047, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 142.5609588623047\n",
      "loss: 136.98318481445312, acc: 0.0, pred_acc: 0.7421875 cls_loss: 136.98318481445312\n",
      "loss: 153.63784790039062, acc: 0.00390625, pred_acc: 0.7578125 cls_loss: 153.63784790039062\n",
      "loss: 161.15216064453125, acc: 0.0, pred_acc: 0.7734375 cls_loss: 161.15216064453125\n",
      "loss: 158.09619140625, acc: 0.0, pred_acc: 0.8125 cls_loss: 158.09619140625\n",
      "loss: 192.42556762695312, acc: 0.0078125, pred_acc: 0.8046875 cls_loss: 192.42556762695312\n",
      "loss: 182.36134338378906, acc: 0.0, pred_acc: 0.79296875 cls_loss: 182.36134338378906\n",
      "loss: 138.65841674804688, acc: 0.0, pred_acc: 0.79296875 cls_loss: 138.65841674804688\n",
      "loss: 132.610595703125, acc: 0.00390625, pred_acc: 0.73046875 cls_loss: 132.610595703125\n",
      "loss: 133.91226196289062, acc: 0.0, pred_acc: 0.78515625 cls_loss: 133.91226196289062\n",
      "loss: 131.66885375976562, acc: 0.00390625, pred_acc: 0.7890625 cls_loss: 131.66885375976562\n",
      "loss: 126.88108825683594, acc: 0.00390625, pred_acc: 0.7578125 cls_loss: 126.88108825683594\n",
      "loss: 148.58181762695312, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 148.58181762695312\n",
      "loss: 151.7542724609375, acc: 0.00390625, pred_acc: 0.77734375 cls_loss: 151.7542724609375\n",
      "loss: 233.10549926757812, acc: 0.0, pred_acc: 0.828125 cls_loss: 233.10549926757812\n",
      "loss: 171.64263916015625, acc: 0.0078125, pred_acc: 0.74609375 cls_loss: 171.64263916015625\n",
      "loss: 141.14743041992188, acc: 0.0, pred_acc: 0.75390625 cls_loss: 141.14743041992188\n",
      "loss: 138.35670471191406, acc: 0.0, pred_acc: 0.76171875 cls_loss: 138.35670471191406\n",
      "loss: 145.99774169921875, acc: 0.00390625, pred_acc: 0.75 cls_loss: 145.99774169921875\n",
      "loss: 194.4896240234375, acc: 0.0, pred_acc: 0.72265625 cls_loss: 194.4896240234375\n",
      "loss: 169.15444946289062, acc: 0.0, pred_acc: 0.72265625 cls_loss: 169.15444946289062\n",
      "loss: 192.4053955078125, acc: 0.0078125, pred_acc: 0.76171875 cls_loss: 192.4053955078125\n",
      "loss: 185.98178100585938, acc: 0.0, pred_acc: 0.75 cls_loss: 185.98178100585938\n",
      "loss: 201.87774658203125, acc: 0.00390625, pred_acc: 0.75390625 cls_loss: 201.87774658203125\n",
      "loss: 183.73675537109375, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 183.73675537109375\n",
      "loss: 164.54116821289062, acc: 0.0, pred_acc: 0.7578125 cls_loss: 164.54116821289062\n",
      "loss: 170.43287658691406, acc: 0.0, pred_acc: 0.7890625 cls_loss: 170.43287658691406\n",
      "loss: 239.16665649414062, acc: 0.00390625, pred_acc: 0.80859375 cls_loss: 239.16665649414062\n",
      "loss: 281.0014953613281, acc: 0.0, pred_acc: 0.7421875 cls_loss: 281.0014953613281\n",
      "loss: 338.36602783203125, acc: 0.00390625, pred_acc: 0.75 cls_loss: 338.36602783203125\n",
      "loss: 278.66436767578125, acc: 0.00390625, pred_acc: 0.734375 cls_loss: 278.66436767578125\n",
      "loss: 336.87933349609375, acc: 0.0, pred_acc: 0.80078125 cls_loss: 336.87933349609375\n",
      "loss: 285.10003662109375, acc: 0.0, pred_acc: 0.76171875 cls_loss: 285.10003662109375\n",
      "loss: 328.2586364746094, acc: 0.0, pred_acc: 0.78125 cls_loss: 328.2586364746094\n",
      "loss: 255.04898071289062, acc: 0.0, pred_acc: 0.7890625 cls_loss: 255.04898071289062\n",
      "loss: 294.23480224609375, acc: 0.0, pred_acc: 0.7265625 cls_loss: 294.23480224609375\n",
      "loss: 295.0060729980469, acc: 0.0, pred_acc: 0.7265625 cls_loss: 295.0060729980469\n",
      "loss: 233.70716857910156, acc: 0.0, pred_acc: 0.74609375 cls_loss: 233.70716857910156\n",
      "loss: 238.71060180664062, acc: 0.0, pred_acc: 0.8046875 cls_loss: 238.71060180664062\n",
      "loss: 229.74586486816406, acc: 0.00390625, pred_acc: 0.80859375 cls_loss: 229.74586486816406\n",
      "loss: 233.21646118164062, acc: 0.0078125, pred_acc: 0.75390625 cls_loss: 233.21646118164062\n",
      "loss: 250.0681915283203, acc: 0.00390625, pred_acc: 0.80078125 cls_loss: 250.0681915283203\n",
      "loss: 325.5301818847656, acc: 0.0, pred_acc: 0.7890625 cls_loss: 325.5301818847656\n",
      "loss: 302.014892578125, acc: 0.0, pred_acc: 0.7734375 cls_loss: 302.014892578125\n",
      "loss: 308.1866149902344, acc: 0.0, pred_acc: 0.74609375 cls_loss: 308.1866149902344\n",
      "loss: 306.08416748046875, acc: 0.0, pred_acc: 0.77734375 cls_loss: 306.08416748046875\n",
      "loss: 359.64849853515625, acc: 0.0, pred_acc: 0.8046875 cls_loss: 359.64849853515625\n",
      "loss: 410.1387939453125, acc: 0.0, pred_acc: 0.8046875 cls_loss: 410.1387939453125\n",
      "loss: 408.974853515625, acc: 0.0, pred_acc: 0.734375 cls_loss: 408.974853515625\n",
      "loss: 472.62469482421875, acc: 0.0, pred_acc: 0.7734375 cls_loss: 472.62469482421875\n",
      "loss: 479.2489013671875, acc: 0.00390625, pred_acc: 0.76171875 cls_loss: 479.2489013671875\n",
      "loss: 302.473876953125, acc: 0.0, pred_acc: 0.765625 cls_loss: 302.473876953125\n",
      "loss: 333.23944091796875, acc: 0.0, pred_acc: 0.75 cls_loss: 333.23944091796875\n",
      "loss: 508.3914794921875, acc: 0.00390625, pred_acc: 0.78125 cls_loss: 508.3914794921875\n",
      "loss: 425.36761474609375, acc: 0.00390625, pred_acc: 0.74609375 cls_loss: 425.36761474609375\n",
      "loss: 411.814697265625, acc: 0.0078125, pred_acc: 0.796875 cls_loss: 411.814697265625\n",
      "loss: 488.1539306640625, acc: 0.0, pred_acc: 0.77734375 cls_loss: 488.1539306640625\n",
      "loss: 565.2735595703125, acc: 0.0, pred_acc: 0.72265625 cls_loss: 565.2735595703125\n",
      "loss: 441.3978271484375, acc: 0.01171875, pred_acc: 0.75390625 cls_loss: 441.3978271484375\n",
      "loss: 534.669921875, acc: 0.0, pred_acc: 0.77734375 cls_loss: 534.669921875\n",
      "loss: 478.67120361328125, acc: 0.0, pred_acc: 0.77734375 cls_loss: 478.67120361328125\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 70\u001b[0m\n\u001b[1;32m     68\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     69\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, pred_acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_acc\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m#   f\" gate_loss: {outputs['gate_loss'].item()}, \"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cls_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcls_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# if (idx + 1) % 5 == 0:\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m#     print(\"evaluation\")\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m#     model.eval()\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m#     evaluation()\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m#     model.train()\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = config.hidden_size\n",
    "\n",
    "# set pred_model to evaluation mode, becasue we don't train it\n",
    "# set the extractor_model to training mode.\n",
    "pred_model.eval()\n",
    "\n",
    "model = SurrogateInterpretation(query_vector_size=hidden_size, classifier=pred_model.classifier, num_labels=1000) \n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Freeze parameters of the pred model \n",
    "def freeze_params(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        param.requires_grad = False\n",
    "freeze_params(pred_model)\n",
    "\n",
    "def get_pseudo_logits(pred_model, inputs):\n",
    "    # obtain the cls token output of the last hidden state\n",
    "    pred_outputs = pred_model(pixel_values=inputs, output_hidden_states=True)\n",
    "    query_vector = pred_outputs.hidden_states[-1][:,0,:] # (N, d)\n",
    "    pseudo_logits = pred_outputs.logits\n",
    "    return pseudo_logits, query_vector\n",
    "\n",
    "pseudo_logits, query_vector = get_pseudo_logits(pred_model, inputs['pixel_values'])\n",
    "\n",
    "outputs = model(pixel_values=inputs['pixel_values'], \n",
    "                query_vector=query_vector, \n",
    "                pseudo_probs=pseudo_logits, \n",
    "                labels=None)\n",
    "# print(\"attention_output shape: \", outputs['attention_output'].shape)\n",
    "# print(\"attention_weights shape: \", outputs['attention_weights'].shape)\n",
    "# print(\"feature_maps shape: \", outputs['feature_maps'].shape)\n",
    "print(\"query_vector shape: \", outputs['query_vector'].shape)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.0005)\n",
    "\n",
    "def evaluation():\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(val_dataloader):\n",
    "            pixel_values = data['pixel_values'].to(device)\n",
    "            label = data['labels'].to(device)\n",
    "            pseudo_logits, query_vector = get_pseudo_logits(pred_model, pixel_values)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, \n",
    "                            query_vector=query_vector, \n",
    "                            pseudo_probs=pseudo_logits, \n",
    "                            labels=label)\n",
    "        \n",
    "            loss = outputs['loss']\n",
    "            print(f\"loss: {loss.item()}, acc: {outputs['acc'].item()}, pred_acc: {outputs['pred_acc'].item()}\")\n",
    "            break\n",
    "                #f\"cossim_loss: {outputs['cossim_loss'].item()}, cls_loss: {outputs['cls_loss'].item()}\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        label = data['labels'].to(device)\n",
    "        pseudo_logits, query_vector = get_pseudo_logits(pred_model, pixel_values)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, \n",
    "                        query_vector=query_vector, \n",
    "                        pseudo_probs=pseudo_logits, \n",
    "                        labels=label)\n",
    "    \n",
    "        loss = outputs['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss.item()}, acc: {outputs['acc'].item()}, pred_acc: {outputs['pred_acc'].item()}\"\n",
    "            #   f\" gate_loss: {outputs['gate_loss'].item()}, \"\n",
    "              f\" cls_loss: {outputs['cls_loss'].item()}\")\n",
    "        # if (idx + 1) % 5 == 0:\n",
    "        #     print(\"evaluation\")\n",
    "        #     model.eval()\n",
    "        #     evaluation()\n",
    "        #     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SurrogateInterpretation(\n",
       "  (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (transform_func): MLP(\n",
       "    (input_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x Sequential(\n",
       "        (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "        (1): Dropout(p=0.5, inplace=False)\n",
       "        (2): ReLU()\n",
       "        (3): Linear(in_features=64, out_features=64, bias=True)\n",
       "        (4): Dropout(p=0.5, inplace=False)\n",
       "        (5): Linear(in_features=64, out_features=768, bias=True)\n",
       "        (6): BatchNorm1d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (7): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (attention): SimplifiedAttention(\n",
       "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  (cls_loss_func): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embedding.weight\n",
      "True\n",
      "patch_embedding.bias\n",
      "True\n",
      "transform_func.input_layer.weight\n",
      "True\n",
      "transform_func.input_layer.bias\n",
      "True\n",
      "transform_func.layers.0.0.weight\n",
      "True\n",
      "transform_func.layers.0.0.bias\n",
      "True\n",
      "transform_func.layers.0.3.weight\n",
      "True\n",
      "transform_func.layers.0.3.bias\n",
      "True\n",
      "transform_func.layers.1.0.weight\n",
      "True\n",
      "transform_func.layers.1.0.bias\n",
      "True\n",
      "transform_func.layers.1.3.weight\n",
      "True\n",
      "transform_func.layers.1.3.bias\n",
      "True\n",
      "transform_func.layers.2.0.weight\n",
      "True\n",
      "transform_func.layers.2.0.bias\n",
      "True\n",
      "transform_func.layers.2.3.weight\n",
      "True\n",
      "transform_func.layers.2.3.bias\n",
      "True\n",
      "transform_func.layers.3.0.weight\n",
      "True\n",
      "transform_func.layers.3.0.bias\n",
      "True\n",
      "transform_func.layers.3.3.weight\n",
      "True\n",
      "transform_func.layers.3.3.bias\n",
      "True\n",
      "transform_func.layers.4.0.weight\n",
      "True\n",
      "transform_func.layers.4.0.bias\n",
      "True\n",
      "transform_func.layers.4.3.weight\n",
      "True\n",
      "transform_func.layers.4.3.bias\n",
      "True\n",
      "transform_func.output_layer.weight\n",
      "True\n",
      "transform_func.output_layer.bias\n",
      "True\n",
      "classifier.weight\n",
      "False\n",
      "classifier.bias\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    # print(param.grad)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=None, logits=tensor([[-2.7440e-01,  8.2152e-01, -8.3644e-02,  4.1588e-01,  5.6233e-01,\n",
       "          1.8593e-01, -5.7729e-01, -4.6004e-01, -5.3389e-01,  2.4017e-01,\n",
       "         -3.1957e-01, -5.9910e-01, -6.6402e-01, -4.9756e-01, -6.2448e-01,\n",
       "         -1.3501e+00, -1.0016e-01, -6.2170e-01,  1.1088e-01, -1.1060e+00,\n",
       "         -2.0846e-01,  3.1697e-01, -9.3152e-01, -3.0693e-01, -1.0124e+00,\n",
       "         -1.8751e-01,  5.8825e-01, -3.6161e-01, -7.4696e-01,  7.4135e-01,\n",
       "         -3.6652e-01, -2.7586e-01,  3.6596e-01, -1.1206e+00, -8.8844e-02,\n",
       "         -1.1328e+00,  1.5458e-01, -1.0399e+00,  1.0136e+00, -1.0395e+00,\n",
       "         -2.4214e+00,  5.1125e-01,  4.9458e-01, -7.4005e-01, -1.5815e+00,\n",
       "         -3.2451e-01, -2.0448e+00, -4.8128e-01, -6.3616e-01, -1.1355e+00,\n",
       "         -1.0902e+00, -4.5294e-02, -6.4045e-01, -2.3987e-01,  1.3110e-01,\n",
       "         -1.2664e+00, -4.7160e-01, -4.3717e-01, -9.5664e-01, -5.9685e-01,\n",
       "          5.0885e-01, -8.4835e-02,  2.6987e-01, -1.4994e-03, -5.3329e-01,\n",
       "          1.8374e-02,  1.3334e+00,  2.4888e-01,  1.0063e+00,  7.6835e-01,\n",
       "         -1.5822e+00, -1.1380e-01, -3.2713e-01, -2.5522e-02, -3.2676e-01,\n",
       "         -3.1565e-01,  3.4420e-01, -6.7578e-01,  5.6247e-02,  1.0190e+00,\n",
       "          5.4856e-01, -1.2385e+00,  2.1597e-03,  1.1123e+00,  1.0890e-01,\n",
       "         -9.7251e-02, -2.5952e-02,  5.7346e-01, -1.5262e-01, -1.8867e+00,\n",
       "         -2.8063e-01,  3.5609e-01,  6.1713e-01,  1.6875e+00, -4.3178e-01,\n",
       "          8.9138e-03,  1.1920e+00, -3.2913e-01, -7.3960e-01, -7.0972e-01,\n",
       "          4.8708e-01, -9.3547e-02,  2.6652e-01,  5.2837e-01,  1.3479e+00,\n",
       "          1.1221e-02,  4.6142e-01, -5.7131e-01, -1.6571e-01,  1.1242e-01,\n",
       "         -2.3517e-01, -4.8880e-01, -9.9192e-01, -5.0106e-01, -4.8489e-01,\n",
       "         -4.9375e-01, -3.3806e-01,  2.4019e-01, -1.9119e-01, -1.1011e+00,\n",
       "         -1.7069e+00, -2.2294e-01, -6.8953e-01, -4.9889e-01, -4.8608e-01,\n",
       "         -7.6868e-01,  8.1537e-01, -1.1233e+00, -2.1399e-01, -6.4167e-01,\n",
       "         -1.9539e-03, -2.9136e-01,  1.1972e-01,  1.0402e-01,  3.5878e-01,\n",
       "         -3.7325e-01,  1.0803e+00,  7.0626e-01,  1.6979e+00, -1.1570e+00,\n",
       "         -5.6739e-02, -3.8451e-01, -6.8758e-01, -1.0975e+00, -1.3078e+00,\n",
       "         -4.1955e-01, -8.2479e-01, -2.4859e-01, -7.0805e-01, -4.2823e-01,\n",
       "          4.9027e-01,  2.4361e+00, -6.9993e-01, -1.3069e-01,  2.4626e-01,\n",
       "         -1.1333e-01, -2.9626e-01,  2.5771e-01,  9.4812e-01, -2.9462e-01,\n",
       "          2.7271e-01,  1.2025e-01, -1.8200e-01,  4.1967e-01, -6.1763e-01,\n",
       "         -6.5494e-01, -5.3568e-01, -1.6535e+00, -2.3745e-01,  1.6641e-01,\n",
       "          4.8847e-01,  6.6036e-01,  1.1190e+00,  1.1518e+00,  5.8662e-01,\n",
       "         -1.3715e+00, -3.2914e-01, -1.7764e-01,  1.6187e+00, -5.6090e-01,\n",
       "         -2.8725e-01, -9.0181e-01, -1.0113e+00, -6.3814e-01,  5.2166e-01,\n",
       "          7.4613e-01,  1.0333e+00,  8.1840e-01, -1.3876e+00, -5.9455e-01,\n",
       "         -1.0704e+00,  7.6961e-02, -7.0642e-02,  1.1513e+00, -9.4308e-01,\n",
       "          1.0859e+00,  1.4987e-02,  3.6259e-01,  1.4790e+00, -8.0545e-01,\n",
       "         -1.2812e+00,  4.6480e-01, -1.4895e-01, -1.0959e-01,  6.5870e-01,\n",
       "          8.0927e-01,  1.6042e+00,  1.2460e+00,  1.4121e+00,  9.3363e-01,\n",
       "          8.8407e-01,  6.6707e-01,  1.1530e-01,  6.6987e-02, -3.6389e-01,\n",
       "         -1.1114e-01, -7.0843e-01, -2.1488e-01, -7.7224e-02,  5.7771e-01,\n",
       "         -6.1893e-01, -5.0870e-01,  6.6297e-01,  2.5727e-01,  1.1874e+00,\n",
       "         -1.0786e+00, -2.1505e-01, -6.0127e-01, -7.2901e-01, -8.8007e-01,\n",
       "         -3.6280e-01, -1.0738e-01,  7.0195e-01, -2.6081e-01, -5.2853e-01,\n",
       "          6.4915e-01,  6.4392e-01,  6.2249e-01, -7.7752e-01,  2.3403e-01,\n",
       "          4.0907e-01, -4.9212e-01, -4.0451e-01,  2.2779e+00,  2.5964e-01,\n",
       "          7.3747e-01,  2.0521e+00, -1.2726e+00,  1.2519e+00,  1.8160e-01,\n",
       "          9.5821e-01,  1.0097e+00, -3.6869e-01, -7.0830e-02,  2.4287e+00,\n",
       "          3.1362e-01, -8.8793e-02,  3.6186e-01,  1.3932e-01,  9.4903e-01,\n",
       "         -2.8977e-01,  4.5243e-01,  2.4066e-01,  2.7516e-01,  6.4583e-01,\n",
       "          2.2476e-01,  2.8123e-01, -2.7115e-02,  2.1226e+00, -9.7692e-01,\n",
       "         -9.3125e-01, -1.3479e-01,  1.5560e-01, -9.5712e-02,  5.0978e-02,\n",
       "          3.4425e-01,  1.7494e+00,  3.0856e-01,  6.7257e-01,  1.1297e-01,\n",
       "          1.1354e+00,  9.2246e+00,  8.2434e+00,  3.4349e+00,  5.1892e+00,\n",
       "          1.2419e+01,  2.7142e+00,  6.7615e+00,  2.7834e+00,  3.3213e+00,\n",
       "          2.7445e+00,  1.1721e+00,  2.9049e+00,  3.2908e+00, -1.1714e+00,\n",
       "         -3.9413e-01, -1.0572e+00, -1.1997e+00,  1.3443e+00,  1.0496e+00,\n",
       "         -1.9585e-01, -9.2810e-01,  8.0215e-01,  1.1943e+00,  1.5537e-01,\n",
       "         -2.4694e-01,  4.1648e-01,  4.3922e-01,  5.6566e-01, -5.9879e-01,\n",
       "         -9.4627e-01, -3.9202e-01, -4.1245e-01, -1.9356e+00,  7.1512e-02,\n",
       "         -1.8446e+00, -7.2554e-01, -1.2953e-01, -3.8618e-01, -5.7988e-01,\n",
       "         -4.6144e-01, -1.1922e+00, -1.0444e+00, -1.4972e+00, -9.3877e-01,\n",
       "         -1.5967e+00, -4.8206e-01, -9.9910e-01, -1.0700e+00,  2.8532e-01,\n",
       "          7.9193e-01,  7.4121e-01,  1.9572e+00,  4.9881e-01, -2.4233e-01,\n",
       "         -6.0462e-01, -1.7086e+00,  4.2006e-01, -9.2753e-02, -5.5906e-02,\n",
       "          1.1660e+00,  1.3959e+00,  2.5617e-01,  2.0618e-01, -2.0619e-01,\n",
       "          5.0448e-01, -4.5773e-01, -6.0199e-02, -4.4010e-01, -7.7120e-01,\n",
       "         -4.7521e-01,  4.2124e-01, -1.6700e-01, -2.0272e-01,  2.1957e-01,\n",
       "          3.0896e-01,  1.4704e+00,  2.0342e+00,  1.7751e+00,  2.7294e+00,\n",
       "          8.3779e-01,  1.2457e+00,  7.6051e-01,  1.0273e+00, -6.9038e-01,\n",
       "         -1.0521e+00, -9.8197e-01, -1.5549e+00, -2.7766e-01, -5.6514e-01,\n",
       "         -4.6554e-01,  3.2675e-02, -2.8894e-01, -3.7544e-01,  4.9535e-01,\n",
       "         -1.6795e+00,  8.2198e-01,  3.3524e-01, -7.8521e-02, -6.3497e-01,\n",
       "          2.7987e-01, -1.2589e+00, -5.0363e-01,  5.8114e-01, -1.1996e-01,\n",
       "          1.0035e+00, -2.2806e-01, -4.5122e-01, -1.0766e+00, -8.7707e-01,\n",
       "         -5.9994e-01, -3.3739e-01, -2.1972e-01, -5.7738e-01,  4.8167e-01,\n",
       "         -3.0148e-02,  1.1394e+00, -2.1640e-01, -1.3076e+00,  3.0939e-01,\n",
       "         -6.2128e-01, -9.3001e-01, -1.2858e+00, -4.3141e-01,  3.2152e-01,\n",
       "          1.0875e-01,  2.9716e-01,  2.7964e-01,  4.8177e-01, -1.0744e+00,\n",
       "         -1.1445e+00,  3.2509e-01,  5.6577e-01,  4.0458e-01,  1.7592e+00,\n",
       "         -2.3753e-01, -2.8474e-02, -3.6832e-03,  7.6421e-02,  5.7744e-01,\n",
       "         -8.6129e-01, -7.1559e-01, -6.3676e-01, -8.3527e-01, -7.7784e-01,\n",
       "         -1.0800e-01, -3.5608e-01, -6.9455e-01,  3.2584e-01, -5.1246e-01,\n",
       "         -8.0428e-01,  4.2397e-01, -3.5443e-01, -4.4501e-02, -1.1915e+00,\n",
       "          8.2980e-01,  7.8841e-01,  1.1845e-01, -4.5100e-01, -6.4679e-01,\n",
       "         -1.0271e+00, -3.7493e-01,  2.8910e-01, -5.2395e-01,  4.9606e-01,\n",
       "         -5.8948e-01,  6.5075e-01,  2.5874e-01, -1.7141e+00, -3.6961e-01,\n",
       "         -5.5960e-01,  1.6648e-01,  6.4772e-01, -3.8953e-02,  1.2290e-01,\n",
       "          2.1829e-01, -2.6131e-01,  1.8974e+00, -7.0874e-01,  8.1532e-01,\n",
       "          8.4909e-01, -6.4389e-01,  5.1968e-01,  1.6918e-01,  3.4676e-01,\n",
       "          8.2632e-01,  9.9245e-01,  4.9263e-01, -1.9909e-02,  4.1964e-01,\n",
       "         -1.3451e+00, -4.0571e-01, -3.5356e-01,  6.2517e-01, -1.3734e-01,\n",
       "          2.4840e-01,  5.3368e-01,  1.1360e+00,  7.0188e-01, -6.5715e-01,\n",
       "          7.3631e-01, -5.2197e-01, -1.1021e-01,  1.8150e-01,  2.6710e-01,\n",
       "          6.8624e-01, -2.3975e-01,  9.8127e-01, -1.2459e-01, -7.5877e-01,\n",
       "          7.3446e-02, -9.9351e-01, -4.2778e-01, -4.1876e-01, -8.8321e-01,\n",
       "         -4.3163e-01, -3.0265e-01,  1.4437e-03,  7.3518e-01,  4.8785e-01,\n",
       "         -1.7422e-01,  5.2704e-01, -6.6004e-01,  2.0918e-01,  1.1516e-01,\n",
       "         -5.2116e-01,  2.0090e-01, -6.7433e-01,  9.1537e-01, -1.1738e+00,\n",
       "         -7.6295e-01,  6.6664e-01, -4.1686e-01, -1.4499e+00,  1.3901e-01,\n",
       "          9.9322e-01,  9.8052e-02, -1.0718e+00,  8.1044e-01, -6.7835e-02,\n",
       "         -6.7398e-01, -5.1529e-01,  2.0845e-01,  1.6544e+00, -8.4660e-01,\n",
       "         -7.9887e-01,  1.2858e-01,  4.9094e-01, -9.8153e-01,  2.6284e-01,\n",
       "         -5.0355e-01, -1.5533e-01,  2.8212e-02, -8.8739e-01, -1.5766e-02,\n",
       "         -7.4631e-01, -8.1062e-01, -3.0902e-01, -5.4260e-01,  1.5043e+00,\n",
       "         -7.3558e-01, -9.4519e-01,  1.5595e-01,  7.0972e-01, -6.7472e-01,\n",
       "         -1.2607e+00, -1.2296e-01, -1.0845e+00, -2.1875e-01, -2.9757e-01,\n",
       "         -8.3845e-01,  1.4301e-01, -5.0874e-01,  2.5717e-02, -7.6290e-01,\n",
       "         -3.3885e-01, -4.2421e-02, -1.5976e-01,  3.0160e-01,  1.3272e+00,\n",
       "         -1.6781e+00, -5.4436e-01, -2.8526e-01, -7.8079e-01, -1.6894e-01,\n",
       "          5.0532e-01, -7.5371e-02,  1.6244e-01,  1.1134e+00, -1.1903e+00,\n",
       "         -2.8913e-01, -5.6730e-01, -4.0905e-01,  6.1022e-01, -6.1974e-01,\n",
       "          6.4292e-01,  2.1886e-01, -1.1264e+00,  3.5417e-01,  3.1475e-01,\n",
       "         -1.8573e-01, -4.6855e-01,  1.1824e-01, -3.0977e-01, -4.0062e-01,\n",
       "          2.5505e-01,  1.0325e+00, -2.6545e-01,  5.2354e-01, -6.4195e-01,\n",
       "         -2.7811e-02, -1.9681e-01, -3.7175e-01,  2.0471e-01,  1.8113e-01,\n",
       "          4.4919e-01,  7.6955e-01,  3.3565e-01,  5.6310e-01, -1.9201e+00,\n",
       "         -3.2162e-01,  3.0347e-01, -1.1089e+00, -2.0784e-01, -1.0610e+00,\n",
       "          2.9501e-01, -8.4815e-01, -7.9827e-01,  6.1276e-01, -4.0935e-03,\n",
       "         -1.9935e-01, -1.2166e-01,  6.2084e-01,  4.5649e-01, -8.8974e-01,\n",
       "          5.8859e-01, -4.2699e-01,  1.3895e-01, -1.0995e+00, -6.1191e-01,\n",
       "          2.1028e+00,  6.3405e-01,  1.1876e+00,  3.3910e-01,  5.1124e-03,\n",
       "         -2.2754e-01, -1.6974e+00,  1.4149e+00, -7.1520e-01, -5.4555e-01,\n",
       "         -3.7761e-01,  8.6435e-01,  3.9194e-01, -1.1095e+00, -4.6969e-01,\n",
       "          1.7947e-01,  4.2453e-01, -8.4034e-01,  7.4392e-01,  2.9486e-01,\n",
       "         -4.0502e-01, -3.8891e-01, -1.1372e+00,  3.3015e-01,  8.8614e-02,\n",
       "         -2.0107e-01,  1.4688e+00, -1.5054e-01, -1.9651e-01,  8.5752e-02,\n",
       "         -4.8125e-01,  3.6400e-01, -1.2768e-01, -1.2785e+00,  3.7881e-01,\n",
       "          5.1384e-01,  2.2562e+00,  7.1396e-01,  1.9604e-01,  1.3043e-01,\n",
       "          5.4616e-02, -7.1471e-01, -6.2496e-01, -5.8215e-01,  6.4207e-01,\n",
       "          1.6644e+00, -6.5639e-01, -6.0605e-01, -3.6229e-01, -8.5739e-01,\n",
       "          1.8551e+00,  3.1924e-02,  1.3245e+00,  1.4893e+00,  4.1012e-01,\n",
       "         -9.1091e-01,  1.7001e+00,  5.8547e-01,  1.1650e+00,  2.1780e-01,\n",
       "          2.2610e+00,  1.4491e+00, -4.0709e-01, -6.3869e-01, -3.0787e-01,\n",
       "          2.0166e+00, -1.5269e-01,  1.5882e-01,  2.7698e-01,  5.5207e-02,\n",
       "          1.2022e-01,  4.2927e-01, -2.4668e-01, -7.7653e-01, -5.5620e-01,\n",
       "         -7.4234e-01,  7.3249e-02,  1.7739e+00,  5.2387e-01, -5.3746e-01,\n",
       "          4.5963e-01,  2.8235e-01, -1.8619e+00, -2.8060e-01,  6.9280e-01,\n",
       "          1.2346e-01, -8.2240e-01, -5.0643e-02, -7.0809e-01,  5.3008e-01,\n",
       "          3.6089e-02, -5.1863e-01, -1.2513e+00,  9.5009e-01, -9.4893e-01,\n",
       "          3.8106e-01, -4.1750e-02,  1.3699e-01, -1.0686e+00, -1.5132e+00,\n",
       "          5.1401e-01,  1.3833e+00,  1.7781e+00, -2.6448e-01, -1.5462e+00,\n",
       "         -7.0315e-02, -1.6882e+00,  1.8422e-01,  1.5900e+00, -8.0075e-01,\n",
       "         -1.2254e+00,  5.7594e-01, -2.2488e-02, -4.6677e-01, -1.2282e+00,\n",
       "         -1.2785e+00,  4.9974e-01, -1.3482e+00, -1.6023e-01, -2.6754e-01,\n",
       "         -5.9868e-01,  9.2814e-01,  1.8817e+00, -6.8048e-01,  1.1702e+00,\n",
       "         -5.3678e-01, -4.9709e-01, -5.8847e-01,  1.4025e-01, -1.2399e+00,\n",
       "          2.5401e+00,  9.6138e-02, -8.4394e-01,  2.2136e+00, -5.3244e-01,\n",
       "          2.7419e-02,  4.5158e-01, -6.7256e-01,  9.0891e-02, -5.2418e-01,\n",
       "          5.0862e-01,  2.1475e+00, -3.4158e-01, -6.3769e-01, -1.0648e-01,\n",
       "         -4.5618e-01,  1.1507e+00,  1.1930e+00, -1.0939e+00,  7.5883e-01,\n",
       "          8.4337e-03, -7.9318e-01, -2.8178e-01, -7.5870e-01,  4.4148e-02,\n",
       "         -2.1432e-01, -2.4516e-01,  6.2996e-01, -7.9923e-01, -2.0618e-01,\n",
       "         -1.3106e+00, -5.4182e-01,  1.0733e+00,  4.6978e-01,  5.0547e-01,\n",
       "          3.1910e+00,  8.3485e-02, -6.1830e-01, -2.8446e-01,  3.3359e-02,\n",
       "          4.6518e-01, -1.7266e-01,  2.1146e-01,  5.5168e-01, -3.7052e-02,\n",
       "          6.0374e-02,  6.3373e-01,  2.5814e+00, -4.9609e-01,  8.4855e-01,\n",
       "          2.9119e-01,  4.1263e-01,  8.7382e-01, -2.6696e-01, -2.3373e-01,\n",
       "         -2.2684e+00, -1.3204e-01,  8.4525e-02,  2.5386e-01, -6.0917e-02,\n",
       "         -2.0888e-01,  2.7876e+00, -4.6880e-01, -1.3111e-01, -7.6445e-01,\n",
       "         -9.0823e-02, -9.0412e-01,  9.9204e-01,  1.8669e-01,  1.2859e+00,\n",
       "         -8.3082e-01, -1.6369e+00, -1.0891e+00,  4.8475e-01, -4.1585e-01,\n",
       "         -2.5464e-01, -3.6219e-01,  3.0436e-01,  1.2003e-01, -3.0959e-01,\n",
       "          1.6867e+00,  2.0983e+00, -6.8362e-01, -4.2406e-01, -4.1992e-01,\n",
       "         -5.3525e-01, -5.4704e-01, -1.7306e-01,  1.9723e+00, -1.3187e-01,\n",
       "          2.7744e-01, -2.9025e-01,  8.0483e-01, -1.2614e+00,  3.3269e-01,\n",
       "          1.4268e+00, -4.5754e-01,  2.5705e-01,  1.4156e-01, -1.2762e-01,\n",
       "          1.1515e+00,  1.5074e+00,  7.6817e-02, -5.2184e-01,  1.8905e+00,\n",
       "         -1.0434e+00, -3.7242e-01,  5.4028e-01, -5.1568e-01,  3.7434e-01,\n",
       "         -4.9443e-01,  4.9512e-01, -1.7179e-01, -6.5369e-01,  1.7693e-01,\n",
       "          9.6347e-01,  7.1896e-02,  1.2054e+00, -5.0967e-01,  5.1654e-02,\n",
       "         -1.0479e+00,  2.9732e-02,  9.2197e-01, -5.8221e-01, -1.3241e+00,\n",
       "         -3.4358e-01,  5.7025e-01, -4.7002e-01, -6.3321e-01,  1.3735e+00,\n",
       "          1.1043e-01,  4.8302e-01,  9.3428e-01, -2.5793e-01, -1.1571e+00,\n",
       "          1.8266e+00,  3.4023e-01, -1.2013e+00, -7.6217e-01, -3.8434e-01,\n",
       "         -4.8746e-01, -9.8153e-01, -6.2929e-01, -3.5941e-01,  1.1546e+00,\n",
       "          2.9395e-01,  6.1522e-01,  1.3506e-01,  1.5891e+00,  5.2941e-01,\n",
       "         -6.0901e-01, -7.7012e-01,  4.4061e-01,  7.6849e-01,  9.6495e-01,\n",
       "          9.3405e-02,  3.6738e-01, -5.7888e-01, -8.2987e-03, -5.2764e-01,\n",
       "         -9.4817e-01,  2.5897e-02,  1.1908e-01, -2.7109e-02, -1.0012e+00,\n",
       "          5.4563e-02, -4.1429e-01, -2.9267e-01,  2.2967e+00, -1.8628e-01,\n",
       "          1.6242e-01, -9.7087e-01, -1.9991e-01, -5.2036e-02, -5.5518e-01,\n",
       "         -6.2326e-01,  4.0608e-01, -1.4164e+00, -5.7858e-01,  4.7241e-01,\n",
       "          2.4966e-02, -3.0017e-01, -3.1323e-01, -1.5171e+00, -6.2090e-01,\n",
       "         -6.7453e-01, -1.0476e+00,  8.4142e-02, -3.7253e-01,  1.2036e-01,\n",
       "         -1.4899e+00, -1.6975e+00, -1.2126e+00, -8.3006e-01,  1.0227e-01,\n",
       "         -8.8374e-01, -5.9594e-02, -7.1446e-01, -4.0916e-01, -2.5237e-01,\n",
       "         -1.3012e+00, -7.1904e-01, -3.6575e-01, -1.1435e+00,  3.7217e-01,\n",
       "         -8.3983e-01, -1.2421e+00, -2.0016e+00,  6.3994e-01, -2.9642e-01,\n",
       "         -1.2733e+00, -1.0175e+00, -8.1577e-02, -1.2890e+00, -7.5889e-01,\n",
       "         -5.4226e-01, -2.2517e-01, -6.6734e-01,  4.2124e-01, -5.5034e-02,\n",
       "         -9.3223e-01,  5.6551e-01,  1.2528e-01, -2.2980e-01, -4.6202e-01,\n",
       "         -6.4327e-01, -8.6787e-01, -1.1124e-01, -6.4736e-02, -8.1440e-02,\n",
       "          7.5749e-02, -4.4920e-01,  4.4366e-01,  3.5033e-01, -6.1740e-01,\n",
       "         -1.0161e+00, -8.0745e-01, -9.0065e-01, -1.4300e+00,  3.9205e-01,\n",
       "         -9.0834e-01, -3.3837e-01, -1.4402e+00,  1.4775e-01, -7.8797e-01,\n",
       "         -8.5843e-01,  2.9447e-01, -6.6196e-01, -9.0431e-01,  6.1096e-02]],\n",
       "       device='mps:0'), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 28, 28])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_feature_map['cnn_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x38b19fad0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "idx = 1\n",
    "plt.imshow(output_feature_map['cnn_output'][0, idx].detach().cpu().numpy())\n",
    "# Add a color bar\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x38b61b350>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(output_feature_map['cnn_output'][0].sum(0).detach().cpu().numpy())\n",
    "# Add a color bar\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " resnet18(weights=ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m196\u001b[39m, \u001b[38;5;241m768\u001b[39m)\n\u001b[1;32m      2\u001b[0m A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "A = torch.randn(1, 196, 768)\n",
    "A = A.to(device)\n",
    "torch.linalg.pinv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
