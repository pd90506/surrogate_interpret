{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "accelerator = Accelerator()\n",
    "device = accelerator.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# model predicts one of the 1000 ImageNet classes\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m predicted_class_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted class:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pred_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mid2label[predicted_class_idx])\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, ViTForImageClassification, ViTConfig\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "pretrained_name = 'google/vit-base-patch16-224'\n",
    "config = ViTConfig.from_pretrained(pretrained_name)\n",
    "processor = ViTImageProcessor.from_pretrained(pretrained_name)\n",
    "pred_model = ViTForImageClassification.from_pretrained(pretrained_name)\n",
    "pred_model.to(device)\n",
    "\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs.to(device)\n",
    "outputs = pred_model(**inputs, output_hidden_states=True)\n",
    "logits = outputs.logits\n",
    "# model predicts one of the 1000 ImageNet classes\n",
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"Predicted class:\", pred_model.config.id2label[predicted_class_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_blocks=5, bottleneck_dim=64):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            shortcut_layers = []\n",
    "            shortcut_layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.ReLU())  # Using ReLU for simplicity; you can choose other activations as needed\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, bottleneck_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            shortcut_layers.append(nn.Linear(bottleneck_dim, hidden_dim))\n",
    "            # shortcut_layers.append(nn.BatchNorm1d(num_features=hidden_dim))\n",
    "            shortcut_layers.append(nn.Dropout())\n",
    "            self.layers.append(nn.Sequential(*shortcut_layers))\n",
    "\n",
    "        self.output_layer= nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "        for layer in self.layers:\n",
    "            residual = layer(x)\n",
    "            x = x + residual # shortcut\n",
    "        return self.output_layer(x)\n",
    "\n",
    "\n",
    "def pairwise_cosine_similarity(Q, K):\n",
    "    \"\"\"\n",
    "    Q: (N, d)\n",
    "    K: (N, L, d)\n",
    "    \"\"\"\n",
    "    attention_scores = torch.matmul(Q.unsqueeze(1), K.transpose(-2, -1)).squeeze(1) # [N, L]\n",
    "    # denominator = torch.sqrt((Q**2).sum(-1).unsqueeze(-1) * (K**2).sum(-1).unsqueeze(-2))\n",
    "    denominator = (K**2).sum(-1) # [N, L]\n",
    "    attention_weights = attention_scores / (denominator + 1e-5)\n",
    "    return attention_weights # [N, L]\n",
    "\n",
    "# def pinv_float32(A):\n",
    "#     \"\"\"\n",
    "#     Compute the Moore-Penrose pseudoinverse of a matrix using SVD,\n",
    "#     ensuring all operations are performed in float32 precision.\n",
    "    \n",
    "#     Parameters:\n",
    "#     - A: The input tensor of shape [..., M, N]\n",
    "    \n",
    "#     Returns:\n",
    "#     - The pseudoinverse of A with float32 precision.\n",
    "#     \"\"\"\n",
    "#     # Ensure A is float32\n",
    "#     A = A.to(dtype=torch.float32)\n",
    "    \n",
    "#     # Compute SVD\n",
    "#     U, S, Vh = torch.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "#     # Invert S with thresholding (regularization)\n",
    "#     S_inv = torch.where(S > 1e-15, 1.0 / S, torch.tensor(0.0, device=S.device, dtype=S.dtype))\n",
    "    \n",
    "#     # Compute pseudoinverse\n",
    "#     A_pinv = torch.matmul(Vh.transpose(-2, -1), torch.matmul(torch.diag_embed(S_inv), U.transpose(-2, -1)))\n",
    "    \n",
    "#     return A_pinv\n",
    "\n",
    "\n",
    "class SimplifiedAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(SimplifiedAttention, self).__init__()\n",
    "        self.query = nn.Linear(embed_size, embed_size)\n",
    "        self.key = nn.Linear(embed_size, embed_size)\n",
    "        # self.value = nn.Linear(embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, q, k, v):\n",
    "        \"\"\"\n",
    "        q: (N, d)\n",
    "        k: (N, L, d)\n",
    "        v: (N, L, d)\n",
    "        \"\"\"\n",
    "        Q = q # self.query(q) # [N, d]\n",
    "        K = k # self.key(k) # [N, L, d]\n",
    "        V = v # [N, L, d]\n",
    "        Q_gate = self.query(q) # [N, d]\n",
    "        K_gate = self.key(k) # [N, L, d]\n",
    "\n",
    "        gate = torch.matmul(K_gate, Q_gate.unsqueeze(-1)).squeeze(-1) # [N, L]\n",
    "        gate = torch.sigmoid(gate) # [N, L]\n",
    "\n",
    "        \n",
    "        # Compute the attention scores [N, L]\n",
    "        attention_scores = torch.matmul(Q.unsqueeze(1), K.transpose(-2, -1)).squeeze(1) / torch.sqrt(torch.tensor(Q.size(-1), dtype=torch.float32))\n",
    "        # attention_weights = pairwise_cosine_similarity(Q, K) # [N, L]\n",
    "        # attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = attention_scores\n",
    "        \n",
    "        # Compute the weighted sum of values using the attention weights\n",
    "        attention_outputs = attention_weights.unsqueeze(-1) * V # (N, L, d)\n",
    "        # pseudo_inverse_K = pinv_float32(K.transpose(-2, -1)) # get pseudoinverse K(K^T K)^{-1} # [N, L, d]\n",
    "\n",
    "        # zoomed_projection = torch.matmul(K, q.unsqueeze(-1)).squeeze(-1) # get a direct projection, which is not rescaled # [N, L]\n",
    "\n",
    "        # projection = torch.matmal(pseudo_inverse_K.transpose(-2, -1), zoomed_projection.unsqueeze(-1)).squeeze(-1) # [N, d]\n",
    "        # projection = pseudo_inverse_K * zoomed_projection.unsqueeze(-1) # [N, L, d]\n",
    "\n",
    "        return attention_outputs, gate  # Return  [N, L, d], [N, L]\n",
    "\n",
    "        # return projection, gate  # Return  [N, L, d], [N, L]\n",
    "\n",
    "\n",
    "def normalize(x, device):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], device=device).reshape(1,-1,1,1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], device=device).reshape(1,-1,1,1)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SurrogateInterpretation(nn.Module):\n",
    "    def __init__(self, query_vector_size, classifier, num_labels) -> None:\n",
    "        \"\"\"\n",
    "        pred_model: prediction model\n",
    "        classifier_head: last fully connected layer \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # self.feature_extractor = feature_extractor\n",
    "        patch_size = 16\n",
    "        n_patches = (224 * 224) // (patch_size * patch_size)\n",
    "        self.patch_embedding = nn.Conv2d(3, query_vector_size, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "\n",
    "        # Transform function to non-linearly transform feature map embedding to the representation space\n",
    "        self.transform_func = MLP(input_dim=query_vector_size,\n",
    "                                  hidden_dim=query_vector_size,\n",
    "                                  output_dim=query_vector_size,\n",
    "                                  num_blocks=5,\n",
    "                                  bottleneck_dim=64)\n",
    "        self.attention = SimplifiedAttention(embed_size=query_vector_size)\n",
    "        # self.classifer = nn.Linear(query_vector_size, num_labels, bias=False)\n",
    "        self.classifier = classifier # (N, d) -> (N, 1000)\n",
    "        \n",
    "        self.cls_loss_func = nn.CrossEntropyLoss()\n",
    "        # self.sim_loss_func = nn.MSELoss()\n",
    "        # # self.cls_loss_func = nn.NLLLoss()\n",
    "        # self.kl_loss = torch.nn.KLDivLoss(reduction='batchmean', log_target=True)\n",
    "        self.cossim_loss_func = nn.CosineSimilarity(dim=-1)\n",
    "    \n",
    "    \n",
    "    def compute_loss(self, pred, pseudo_label, gate, query_vector, projection):    \n",
    "        cls_loss = self.cls_loss_func(pred, pseudo_label)\n",
    "        gate_loss = torch.mean(gate.sum(-1))\n",
    "        cos_loss = - torch.log(self.cossim_loss_func(projection, query_vector).mean())\n",
    "\n",
    "\n",
    "        loss = cos_loss# + 0.01 * gate_loss\n",
    "\n",
    "        return {'loss':loss, \n",
    "                'cls_loss': cls_loss,\n",
    "                'gate_loss': gate_loss,\n",
    "                'cos_loss': cos_loss\n",
    "                }\n",
    "        \n",
    "    \n",
    "    def forward(self, pixel_values, query_vector, pseudo_probs, labels=None):\n",
    "        \"\"\" \n",
    "        pixel_values: [N, 3, 224, 224]\n",
    "        query_vector: [N, d]\n",
    "        pseudo_probs: [N, 1000]\n",
    "        label: [N,]\n",
    "        \"\"\"\n",
    "        patch_features = self.patch_embedding(pixel_values) # [N, d, sqrt(L), sqrt(L)] \n",
    "        patch_features = torch.flatten(patch_features, start_dim=-2).transpose(1, 2) # [N, L, d]\n",
    "        \n",
    "        \n",
    "        feature_reprs = self.transform_func(patch_features) # [N, L, d]\n",
    "\n",
    "        projection, gate = self.attention(\n",
    "            query_vector, # [N, d]\n",
    "            feature_reprs, # [N, L, d]\n",
    "            feature_reprs, # [N, L, d]\n",
    "        ) # Return  [N, L, d], [N, L]\n",
    "\n",
    "        # Get the pseudo labels and interpretable predictions\n",
    "\n",
    "        # pred = self.classifer(torch.mean(attention_output, dim=(-2,-1))) # (N, L, H, W) -> (N, L) -> (N, 1000)\n",
    "        # avgpooled = torch.mean(feature_reprs, dim=-1) # [N, L]\n",
    "        approx_hidden_state = torch.sum(projection * gate.unsqueeze(-1), dim=-2) # (N, d)\n",
    "        pred = self.classifier(approx_hidden_state) # [N, 1000]\n",
    "        \n",
    "        pred_labels = pred.argmax(-1).view(-1) # (N,)\n",
    "\n",
    "        pseudo_label = pseudo_probs.argmax(-1) # [N,]\n",
    "        pseudo_label = pseudo_label.contiguous().view(-1) # (N,)\n",
    "\n",
    "        # compute loss\n",
    "        loss_dict = self.compute_loss(pred, pseudo_label, gate, query_vector, approx_hidden_state)\n",
    "        loss = loss_dict['loss']\n",
    "\n",
    "        accuracy = (pred_labels == pseudo_label).sum() / len(pseudo_label)\n",
    "\n",
    "        if labels is not None:\n",
    "            pred_accuracy = (pseudo_label == labels).sum() / len(labels)\n",
    "\n",
    "        # outputs['feature_maps'] = feature_maps\n",
    "        # outputs['attention_output'] = attention_output\n",
    "        # outputs['attention_weights'] = attention_weights\n",
    "        outputs['query_vector'] = query_vector\n",
    "        outputs['loss'] = loss\n",
    "        outputs['gate_loss'] = loss_dict['gate_loss']\n",
    "        outputs['cls_loss'] = loss_dict['cls_loss']\n",
    "        outputs['cos_loss'] = loss_dict['cos_loss']\n",
    "        outputs['acc'] = accuracy\n",
    "        outputs['pred_acc'] = pred_accuracy if labels is not None else None\n",
    "        # outputs['pred_split'] = pred_split\n",
    "        outputs['pred'] = pred\n",
    "        outputs['gate'] = gate\n",
    "        # outputs['pred_split'] = pred_split\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pd905\\anaconda3\\envs\\torch\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import (CenterCrop, Compose, Normalize, RandomHorizontalFlip, RandomResizedCrop, Resize, ToTensor)\n",
    "\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "size = processor.size[\"height\"]\n",
    "\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(size),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize(size),\n",
    "        CenterCrop(size),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=image_mean, std=image_std)\n",
    "    ]\n",
    ")\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert('RGB')) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mrm8488/ImageNet1K-val\")\n",
    "dataset = dataset['train']\n",
    "splits = dataset.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "train_ds.set_transform(train_transforms)\n",
    "val_ds.set_transform(val_transforms)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([256, 3, 224, 224])\n",
      "labels torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "train_dataloader = DataLoader(train_ds, collate_fn=collate_fn, batch_size=256, shuffle=True)\n",
    "val_dataloader = DataLoader(val_ds, collate_fn=collate_fn, batch_size=1024, shuffle=True)\n",
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_vector shape:  torch.Size([1, 768])\n",
      "loss: 1.9519517421722412, acc: 0.1796875, pred_acc: 0.78515625 gate_loss: 83.43122863769531, cls_loss: 12.581979751586914 cos_loss: 1.9519517421722412\n",
      "loss: 1.9668872356414795, acc: 0.01953125, pred_acc: 0.734375 gate_loss: 187.83111572265625, cls_loss: 967.6129760742188 cos_loss: 1.9668872356414795\n",
      "loss: 1.478937029838562, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 184.99668884277344, cls_loss: 5307.56201171875 cos_loss: 1.478937029838562\n",
      "loss: 1.3306843042373657, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 170.16619873046875, cls_loss: 11244.57421875 cos_loss: 1.3306843042373657\n",
      "loss: 1.181206464767456, acc: 0.0, pred_acc: 0.74609375 gate_loss: 158.68589782714844, cls_loss: 26184.126953125 cos_loss: 1.181206464767456\n",
      "loss: 1.1747322082519531, acc: 0.0, pred_acc: 0.76953125 gate_loss: 136.7436981201172, cls_loss: 43200.80859375 cos_loss: 1.1747322082519531\n",
      "loss: 1.153815507888794, acc: 0.0, pred_acc: 0.76953125 gate_loss: 129.1397247314453, cls_loss: 75906.53125 cos_loss: 1.153815507888794\n",
      "loss: 1.1561191082000732, acc: 0.0078125, pred_acc: 0.7890625 gate_loss: 117.35470581054688, cls_loss: 98321.8046875 cos_loss: 1.1561191082000732\n",
      "loss: 1.174701452255249, acc: 0.0, pred_acc: 0.78125 gate_loss: 116.1304702758789, cls_loss: 136304.796875 cos_loss: 1.174701452255249\n",
      "loss: 1.1161142587661743, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 105.61650085449219, cls_loss: 200433.84375 cos_loss: 1.1161142587661743\n",
      "loss: 1.1361963748931885, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 112.58055114746094, cls_loss: 259012.734375 cos_loss: 1.1361963748931885\n",
      "loss: 1.107062578201294, acc: 0.0, pred_acc: 0.8203125 gate_loss: 116.76560974121094, cls_loss: 388965.34375 cos_loss: 1.107062578201294\n",
      "loss: 1.1024550199508667, acc: 0.0, pred_acc: 0.78125 gate_loss: 123.38346862792969, cls_loss: 479394.84375 cos_loss: 1.1024550199508667\n",
      "loss: 1.1397064924240112, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 122.37751007080078, cls_loss: 562312.1875 cos_loss: 1.1397064924240112\n",
      "loss: 1.1263515949249268, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 119.3944320678711, cls_loss: 612329.6875 cos_loss: 1.1263515949249268\n",
      "loss: 1.115439534187317, acc: 0.0, pred_acc: 0.7421875 gate_loss: 126.10123443603516, cls_loss: 893690.6875 cos_loss: 1.115439534187317\n",
      "loss: 1.1137070655822754, acc: 0.0, pred_acc: 0.76171875 gate_loss: 125.49177551269531, cls_loss: 1150218.0 cos_loss: 1.1137070655822754\n",
      "loss: 1.1359636783599854, acc: 0.0, pred_acc: 0.73828125 gate_loss: 119.1578140258789, cls_loss: 1251360.375 cos_loss: 1.1359636783599854\n",
      "loss: 1.1222646236419678, acc: 0.0, pred_acc: 0.703125 gate_loss: 117.05567169189453, cls_loss: 1812308.5 cos_loss: 1.1222646236419678\n",
      "loss: 1.1121715307235718, acc: 0.0, pred_acc: 0.796875 gate_loss: 116.03570556640625, cls_loss: 2201487.75 cos_loss: 1.1121715307235718\n",
      "loss: 1.1206157207489014, acc: 0.0, pred_acc: 0.7109375 gate_loss: 114.9394302368164, cls_loss: 2366401.75 cos_loss: 1.1206157207489014\n",
      "loss: 1.1267447471618652, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 105.78599548339844, cls_loss: 2209669.75 cos_loss: 1.1267447471618652\n",
      "loss: 1.0904011726379395, acc: 0.0, pred_acc: 0.75 gate_loss: 109.82177734375, cls_loss: 2808422.5 cos_loss: 1.0904011726379395\n",
      "loss: 1.0684868097305298, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 93.17236328125, cls_loss: 2359423.5 cos_loss: 1.0684868097305298\n",
      "loss: 1.1299622058868408, acc: 0.01171875, pred_acc: 0.734375 gate_loss: 86.88995361328125, cls_loss: 2115110.5 cos_loss: 1.1299622058868408\n",
      "loss: 1.1527820825576782, acc: 0.0, pred_acc: 0.74609375 gate_loss: 77.24998474121094, cls_loss: 1644501.25 cos_loss: 1.1527820825576782\n",
      "loss: 1.1259548664093018, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 84.46672058105469, cls_loss: 2022140.5 cos_loss: 1.1259548664093018\n",
      "loss: 1.101995825767517, acc: 0.0, pred_acc: 0.765625 gate_loss: 95.29219818115234, cls_loss: 2415558.0 cos_loss: 1.101995825767517\n",
      "loss: 1.1028633117675781, acc: 0.0, pred_acc: 0.76171875 gate_loss: 110.13468933105469, cls_loss: 2922621.5 cos_loss: 1.1028633117675781\n",
      "loss: 1.0918664932250977, acc: 0.0, pred_acc: 0.75 gate_loss: 127.17071533203125, cls_loss: 3675627.0 cos_loss: 1.0918664932250977\n",
      "loss: 1.0507011413574219, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 132.42727661132812, cls_loss: 4042041.75 cos_loss: 1.0507011413574219\n",
      "loss: 1.1313178539276123, acc: 0.0078125, pred_acc: 0.7265625 gate_loss: 132.8214874267578, cls_loss: 4035360.25 cos_loss: 1.1313178539276123\n",
      "loss: 1.070276141166687, acc: 0.0, pred_acc: 0.78125 gate_loss: 139.92529296875, cls_loss: 4059103.5 cos_loss: 1.070276141166687\n",
      "loss: 1.0831243991851807, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 156.65480041503906, cls_loss: 3993945.25 cos_loss: 1.0831243991851807\n",
      "loss: 1.047684907913208, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 166.33949279785156, cls_loss: 3541603.5 cos_loss: 1.047684907913208\n",
      "loss: 1.0561068058013916, acc: 0.0, pred_acc: 0.796875 gate_loss: 161.94248962402344, cls_loss: 3768393.25 cos_loss: 1.0561068058013916\n",
      "loss: 1.1016864776611328, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 161.12115478515625, cls_loss: 3845893.0 cos_loss: 1.1016864776611328\n",
      "loss: 1.0924042463302612, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 152.6396484375, cls_loss: 3342260.25 cos_loss: 1.0924042463302612\n",
      "loss: 1.0795879364013672, acc: 0.01171875, pred_acc: 0.74609375 gate_loss: 147.14244079589844, cls_loss: 3077073.5 cos_loss: 1.0795879364013672\n",
      "loss: 1.0934662818908691, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 152.68438720703125, cls_loss: 3126546.0 cos_loss: 1.0934662818908691\n",
      "loss: 1.0654041767120361, acc: 0.02734375, pred_acc: 0.7421875 gate_loss: 156.78005981445312, cls_loss: 3202627.75 cos_loss: 1.0654041767120361\n",
      "loss: 1.0778532028198242, acc: 0.01171875, pred_acc: 0.80078125 gate_loss: 172.52493286132812, cls_loss: 4167309.5 cos_loss: 1.0778532028198242\n",
      "loss: 1.141912817955017, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 173.66131591796875, cls_loss: 5229981.0 cos_loss: 1.141912817955017\n",
      "loss: 1.0669598579406738, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 175.94300842285156, cls_loss: 4743283.5 cos_loss: 1.0669598579406738\n",
      "loss: 1.146310806274414, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 170.79226684570312, cls_loss: 5089052.0 cos_loss: 1.146310806274414\n",
      "loss: 1.0833609104156494, acc: 0.00390625, pred_acc: 0.81640625 gate_loss: 167.087646484375, cls_loss: 6059970.0 cos_loss: 1.0833609104156494\n",
      "loss: 1.071479320526123, acc: 0.0, pred_acc: 0.7734375 gate_loss: 163.42874145507812, cls_loss: 5292894.5 cos_loss: 1.071479320526123\n",
      "loss: 1.1504144668579102, acc: 0.00390625, pred_acc: 0.7109375 gate_loss: 155.63218688964844, cls_loss: 4989642.0 cos_loss: 1.1504144668579102\n",
      "loss: 1.0833089351654053, acc: 0.01171875, pred_acc: 0.78125 gate_loss: 157.9440460205078, cls_loss: 4393460.5 cos_loss: 1.0833089351654053\n",
      "loss: 1.0931997299194336, acc: 0.01171875, pred_acc: 0.70703125 gate_loss: 145.15298461914062, cls_loss: 3476356.5 cos_loss: 1.0931997299194336\n",
      "loss: 1.0558502674102783, acc: 0.02734375, pred_acc: 0.71484375 gate_loss: 137.27130126953125, cls_loss: 3357846.0 cos_loss: 1.0558502674102783\n",
      "loss: 1.0608569383621216, acc: 0.015625, pred_acc: 0.78125 gate_loss: 125.82228088378906, cls_loss: 2424602.25 cos_loss: 1.0608569383621216\n",
      "loss: 1.0376033782958984, acc: 0.01953125, pred_acc: 0.75390625 gate_loss: 122.12994384765625, cls_loss: 2411511.0 cos_loss: 1.0376033782958984\n",
      "loss: 1.069653034210205, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 132.5172882080078, cls_loss: 3575370.25 cos_loss: 1.069653034210205\n",
      "loss: 1.0740736722946167, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 135.86302185058594, cls_loss: 3628611.5 cos_loss: 1.0740736722946167\n",
      "loss: 1.0642859935760498, acc: 0.015625, pred_acc: 0.74609375 gate_loss: 135.5460968017578, cls_loss: 4020375.25 cos_loss: 1.0642859935760498\n",
      "loss: 1.0875236988067627, acc: 0.0, pred_acc: 0.8125 gate_loss: 133.8511962890625, cls_loss: 4027725.75 cos_loss: 1.0875236988067627\n",
      "loss: 1.079562783241272, acc: 0.0234375, pred_acc: 0.79296875 gate_loss: 137.33551025390625, cls_loss: 4200669.5 cos_loss: 1.079562783241272\n",
      "loss: 1.0775556564331055, acc: 0.015625, pred_acc: 0.7734375 gate_loss: 140.40689086914062, cls_loss: 5225102.0 cos_loss: 1.0775556564331055\n",
      "loss: 1.0796685218811035, acc: 0.0234375, pred_acc: 0.703125 gate_loss: 139.80184936523438, cls_loss: 4916477.0 cos_loss: 1.0796685218811035\n",
      "loss: 1.055051326751709, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 134.76930236816406, cls_loss: 5025778.5 cos_loss: 1.055051326751709\n",
      "loss: 1.0759278535842896, acc: 0.015625, pred_acc: 0.78515625 gate_loss: 125.90408325195312, cls_loss: 4998157.0 cos_loss: 1.0759278535842896\n",
      "loss: 1.0698940753936768, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 118.73336791992188, cls_loss: 4914616.0 cos_loss: 1.0698940753936768\n",
      "loss: 1.0562551021575928, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 110.42676544189453, cls_loss: 5554502.5 cos_loss: 1.0562551021575928\n",
      "loss: 1.0620980262756348, acc: 0.01171875, pred_acc: 0.77734375 gate_loss: 108.51041412353516, cls_loss: 4579375.5 cos_loss: 1.0620980262756348\n",
      "loss: 1.0442601442337036, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 114.71057891845703, cls_loss: 6975782.0 cos_loss: 1.0442601442337036\n",
      "loss: 1.075530767440796, acc: 0.0078125, pred_acc: 0.78125 gate_loss: 120.14071655273438, cls_loss: 8154795.0 cos_loss: 1.075530767440796\n",
      "loss: 1.0640668869018555, acc: 0.01171875, pred_acc: 0.765625 gate_loss: 128.75697326660156, cls_loss: 9927646.0 cos_loss: 1.0640668869018555\n",
      "loss: 1.1024487018585205, acc: 0.0, pred_acc: 0.71484375 gate_loss: 128.3086700439453, cls_loss: 11691662.0 cos_loss: 1.1024487018585205\n",
      "loss: 1.1060199737548828, acc: 0.01953125, pred_acc: 0.734375 gate_loss: 124.72380828857422, cls_loss: 8952423.0 cos_loss: 1.1060199737548828\n",
      "loss: 1.0973223447799683, acc: 0.0, pred_acc: 0.77734375 gate_loss: 111.8883285522461, cls_loss: 9058738.0 cos_loss: 1.0973223447799683\n",
      "loss: 1.0427577495574951, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 91.99488830566406, cls_loss: 6626218.5 cos_loss: 1.0427577495574951\n",
      "loss: 1.0828444957733154, acc: 0.01953125, pred_acc: 0.765625 gate_loss: 76.52137756347656, cls_loss: 5003431.0 cos_loss: 1.0828444957733154\n",
      "loss: 1.145981788635254, acc: 0.015625, pred_acc: 0.73828125 gate_loss: 65.21783447265625, cls_loss: 2849748.75 cos_loss: 1.145981788635254\n",
      "loss: 1.1112737655639648, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 57.66171646118164, cls_loss: 2242911.0 cos_loss: 1.1112737655639648\n",
      "loss: 1.074005365371704, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 59.34123992919922, cls_loss: 2449740.0 cos_loss: 1.074005365371704\n",
      "loss: 1.0451735258102417, acc: 0.015625, pred_acc: 0.80078125 gate_loss: 93.41724395751953, cls_loss: 7413613.5 cos_loss: 1.0451735258102417\n",
      "loss: 1.0929547548294067, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 127.47122192382812, cls_loss: 14843388.0 cos_loss: 1.0929547548294067\n",
      "loss: 1.1854045391082764, acc: 0.00390625, pred_acc: 0.75 gate_loss: 142.56578063964844, cls_loss: 20428358.0 cos_loss: 1.1854045391082764\n",
      "loss: 1.22162926197052, acc: 0.0078125, pred_acc: 0.73828125 gate_loss: 140.4972686767578, cls_loss: 28482038.0 cos_loss: 1.22162926197052\n",
      "loss: 1.2159473896026611, acc: 0.00390625, pred_acc: 0.71875 gate_loss: 146.09017944335938, cls_loss: 34012140.0 cos_loss: 1.2159473896026611\n",
      "loss: 1.2248730659484863, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 144.7508544921875, cls_loss: 34564332.0 cos_loss: 1.2248730659484863\n",
      "loss: 1.160347580909729, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 141.86080932617188, cls_loss: 38871704.0 cos_loss: 1.160347580909729\n",
      "loss: 1.1008687019348145, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 133.42764282226562, cls_loss: 41045348.0 cos_loss: 1.1008687019348145\n",
      "loss: 1.0905673503875732, acc: 0.00390625, pred_acc: 0.75 gate_loss: 119.922119140625, cls_loss: 37044548.0 cos_loss: 1.0905673503875732\n",
      "loss: 1.1175802946090698, acc: 0.015625, pred_acc: 0.76171875 gate_loss: 112.24996948242188, cls_loss: 37882364.0 cos_loss: 1.1175802946090698\n",
      "loss: 1.1287328004837036, acc: 0.0, pred_acc: 0.77734375 gate_loss: 106.57960510253906, cls_loss: 33640204.0 cos_loss: 1.1287328004837036\n",
      "loss: 1.1097049713134766, acc: 0.01171875, pred_acc: 0.81640625 gate_loss: 99.63275146484375, cls_loss: 35881980.0 cos_loss: 1.1097049713134766\n",
      "loss: 1.1184356212615967, acc: 0.0, pred_acc: 0.77734375 gate_loss: 104.16199493408203, cls_loss: 38474140.0 cos_loss: 1.1184356212615967\n",
      "loss: 1.1026216745376587, acc: 0.01171875, pred_acc: 0.78515625 gate_loss: 115.70327758789062, cls_loss: 46275224.0 cos_loss: 1.1026216745376587\n",
      "loss: 1.0753203630447388, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 127.23696899414062, cls_loss: 57182420.0 cos_loss: 1.0753203630447388\n",
      "loss: 1.082266092300415, acc: 0.0, pred_acc: 0.7734375 gate_loss: 139.33514404296875, cls_loss: 68209792.0 cos_loss: 1.082266092300415\n",
      "loss: 1.046233057975769, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 158.37525939941406, cls_loss: 92608088.0 cos_loss: 1.046233057975769\n",
      "loss: 1.0680772066116333, acc: 0.01171875, pred_acc: 0.7890625 gate_loss: 167.8079071044922, cls_loss: 103360392.0 cos_loss: 1.0680772066116333\n",
      "loss: 1.0720245838165283, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 172.7095489501953, cls_loss: 124349016.0 cos_loss: 1.0720245838165283\n",
      "loss: 1.1029839515686035, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 171.999755859375, cls_loss: 115050584.0 cos_loss: 1.1029839515686035\n",
      "loss: 1.09534752368927, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 173.01327514648438, cls_loss: 123078832.0 cos_loss: 1.09534752368927\n",
      "loss: 1.0922327041625977, acc: 0.0, pred_acc: 0.76171875 gate_loss: 175.83615112304688, cls_loss: 114930992.0 cos_loss: 1.0922327041625977\n",
      "loss: 1.082244873046875, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 172.85345458984375, cls_loss: 112192288.0 cos_loss: 1.082244873046875\n",
      "loss: 1.091662883758545, acc: 0.00390625, pred_acc: 0.73046875 gate_loss: 160.76913452148438, cls_loss: 101496536.0 cos_loss: 1.091662883758545\n",
      "loss: 1.0578275918960571, acc: 0.00390625, pred_acc: 0.8125 gate_loss: 152.51564025878906, cls_loss: 81745360.0 cos_loss: 1.0578275918960571\n",
      "loss: 1.1349762678146362, acc: 0.01171875, pred_acc: 0.796875 gate_loss: 139.66029357910156, cls_loss: 68394216.0 cos_loss: 1.1349762678146362\n",
      "loss: 1.1274981498718262, acc: 0.0, pred_acc: 0.796875 gate_loss: 129.52174377441406, cls_loss: 61071536.0 cos_loss: 1.1274981498718262\n",
      "loss: 1.1553049087524414, acc: 0.01171875, pred_acc: 0.765625 gate_loss: 128.4492950439453, cls_loss: 56092908.0 cos_loss: 1.1553049087524414\n",
      "loss: 1.1452593803405762, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 129.32058715820312, cls_loss: 71087776.0 cos_loss: 1.1452593803405762\n",
      "loss: 1.1540604829788208, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 132.314453125, cls_loss: 85279592.0 cos_loss: 1.1540604829788208\n",
      "loss: 1.1311447620391846, acc: 0.01171875, pred_acc: 0.72265625 gate_loss: 136.19932556152344, cls_loss: 89899080.0 cos_loss: 1.1311447620391846\n",
      "loss: 1.107569694519043, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 137.55938720703125, cls_loss: 113433696.0 cos_loss: 1.107569694519043\n",
      "loss: 1.0697119235992432, acc: 0.01171875, pred_acc: 0.765625 gate_loss: 146.53201293945312, cls_loss: 145758112.0 cos_loss: 1.0697119235992432\n",
      "loss: 1.0555572509765625, acc: 0.0, pred_acc: 0.78515625 gate_loss: 146.19140625, cls_loss: 167179104.0 cos_loss: 1.0555572509765625\n",
      "loss: 1.0858116149902344, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 149.78823852539062, cls_loss: 170692272.0 cos_loss: 1.0858116149902344\n",
      "loss: 1.097591757774353, acc: 0.0, pred_acc: 0.734375 gate_loss: 149.63064575195312, cls_loss: 227114880.0 cos_loss: 1.097591757774353\n",
      "loss: 1.0893049240112305, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 149.23191833496094, cls_loss: 242103408.0 cos_loss: 1.0893049240112305\n",
      "loss: 1.068622350692749, acc: 0.0, pred_acc: 0.75 gate_loss: 139.75390625, cls_loss: 237506288.0 cos_loss: 1.068622350692749\n",
      "loss: 1.0773894786834717, acc: 0.0, pred_acc: 0.796875 gate_loss: 138.21514892578125, cls_loss: 242872352.0 cos_loss: 1.0773894786834717\n",
      "loss: 1.062936544418335, acc: 0.0, pred_acc: 0.765625 gate_loss: 133.78555297851562, cls_loss: 208335376.0 cos_loss: 1.062936544418335\n",
      "loss: 1.1292626857757568, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 129.43333435058594, cls_loss: 209457936.0 cos_loss: 1.1292626857757568\n",
      "loss: 1.0816012620925903, acc: 0.00390625, pred_acc: 0.703125 gate_loss: 125.74028015136719, cls_loss: 226903648.0 cos_loss: 1.0816012620925903\n",
      "loss: 1.094231367111206, acc: 0.0078125, pred_acc: 0.80078125 gate_loss: 117.83953857421875, cls_loss: 180251488.0 cos_loss: 1.094231367111206\n",
      "loss: 1.0949000120162964, acc: 0.0, pred_acc: 0.78515625 gate_loss: 109.81231689453125, cls_loss: 132954696.0 cos_loss: 1.0949000120162964\n",
      "loss: 1.0413738489151, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 106.43785095214844, cls_loss: 174470960.0 cos_loss: 1.0413738489151\n",
      "loss: 1.1011338233947754, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 104.0193099975586, cls_loss: 143114928.0 cos_loss: 1.1011338233947754\n",
      "loss: 1.1148109436035156, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 100.54350280761719, cls_loss: 128885080.0 cos_loss: 1.1148109436035156\n",
      "loss: 1.108056664466858, acc: 0.01171875, pred_acc: 0.7578125 gate_loss: 97.69988250732422, cls_loss: 131492416.0 cos_loss: 1.108056664466858\n",
      "loss: 1.078845739364624, acc: 0.0078125, pred_acc: 0.72265625 gate_loss: 103.024169921875, cls_loss: 135342896.0 cos_loss: 1.078845739364624\n",
      "loss: 1.0577640533447266, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 104.09905242919922, cls_loss: 127973456.0 cos_loss: 1.0577640533447266\n",
      "loss: 1.0919734239578247, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 87.52507019042969, cls_loss: 106994312.0 cos_loss: 1.0919734239578247\n",
      "loss: 1.1056969165802002, acc: 0.0, pred_acc: 0.75390625 gate_loss: 75.69871520996094, cls_loss: 93408824.0 cos_loss: 1.1056969165802002\n",
      "loss: 1.1070966720581055, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 69.47895050048828, cls_loss: 95218384.0 cos_loss: 1.1070966720581055\n",
      "loss: 1.1188347339630127, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 64.21671295166016, cls_loss: 85802064.0 cos_loss: 1.1188347339630127\n",
      "loss: 1.135514736175537, acc: 0.01171875, pred_acc: 0.81640625 gate_loss: 61.885902404785156, cls_loss: 73568984.0 cos_loss: 1.135514736175537\n",
      "loss: 1.1541997194290161, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 56.71484375, cls_loss: 75127208.0 cos_loss: 1.1541997194290161\n",
      "loss: 1.130784034729004, acc: 0.00390625, pred_acc: 0.8046875 gate_loss: 50.5013427734375, cls_loss: 77276264.0 cos_loss: 1.130784034729004\n",
      "loss: 1.0953651666641235, acc: 0.0, pred_acc: 0.76953125 gate_loss: 49.1717643737793, cls_loss: 85425856.0 cos_loss: 1.0953651666641235\n",
      "loss: 1.1469066143035889, acc: 0.0, pred_acc: 0.734375 gate_loss: 55.973052978515625, cls_loss: 97618128.0 cos_loss: 1.1469066143035889\n",
      "loss: 1.1324533224105835, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 62.84001159667969, cls_loss: 113923984.0 cos_loss: 1.1324533224105835\n",
      "loss: 1.1200602054595947, acc: 0.0078125, pred_acc: 0.7890625 gate_loss: 69.22264099121094, cls_loss: 141576608.0 cos_loss: 1.1200602054595947\n",
      "loss: 1.132228136062622, acc: 0.0, pred_acc: 0.78515625 gate_loss: 74.17691040039062, cls_loss: 181027312.0 cos_loss: 1.132228136062622\n",
      "loss: 1.1233093738555908, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 82.81156158447266, cls_loss: 230644656.0 cos_loss: 1.1233093738555908\n",
      "loss: 1.1421866416931152, acc: 0.0, pred_acc: 0.6875 gate_loss: 84.9347152709961, cls_loss: 237856416.0 cos_loss: 1.1421866416931152\n",
      "loss: 1.0580214262008667, acc: 0.00390625, pred_acc: 0.8046875 gate_loss: 78.64479064941406, cls_loss: 262054448.0 cos_loss: 1.0580214262008667\n",
      "loss: 1.059542179107666, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 87.07280731201172, cls_loss: 322909888.0 cos_loss: 1.059542179107666\n",
      "loss: 1.0959794521331787, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 87.82810974121094, cls_loss: 280214752.0 cos_loss: 1.0959794521331787\n",
      "loss: 1.1272649765014648, acc: 0.0, pred_acc: 0.74609375 gate_loss: 89.20161437988281, cls_loss: 292178080.0 cos_loss: 1.1272649765014648\n",
      "loss: 1.0854923725128174, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 90.27547454833984, cls_loss: 380348448.0 cos_loss: 1.0854923725128174\n",
      "loss: 1.0648773908615112, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 89.5234375, cls_loss: 380013088.0 cos_loss: 1.0648773908615112\n",
      "loss: 1.0611674785614014, acc: 0.0, pred_acc: 0.765625 gate_loss: 85.8828125, cls_loss: 376088768.0 cos_loss: 1.0611674785614014\n",
      "loss: 1.066391110420227, acc: 0.0, pred_acc: 0.7890625 gate_loss: 82.58523559570312, cls_loss: 345821312.0 cos_loss: 1.066391110420227\n",
      "loss: 1.0894707441329956, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 77.58203125, cls_loss: 334117760.0 cos_loss: 1.0894707441329956\n",
      "loss: 1.081864833831787, acc: 0.0, pred_acc: 0.74609375 gate_loss: 76.9522705078125, cls_loss: 289655392.0 cos_loss: 1.081864833831787\n",
      "loss: 1.0863326787948608, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 70.78653717041016, cls_loss: 240068976.0 cos_loss: 1.0863326787948608\n",
      "loss: 1.0769424438476562, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 69.73419189453125, cls_loss: 234344528.0 cos_loss: 1.0769424438476562\n",
      "loss: 1.0762678384780884, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 66.66049194335938, cls_loss: 234602416.0 cos_loss: 1.0762678384780884\n",
      "loss: 1.1181557178497314, acc: 0.0, pred_acc: 0.77734375 gate_loss: 67.7890396118164, cls_loss: 222439376.0 cos_loss: 1.1181557178497314\n",
      "loss: 1.0730637311935425, acc: 0.0, pred_acc: 0.74609375 gate_loss: 61.87879943847656, cls_loss: 234264688.0 cos_loss: 1.0730637311935425\n",
      "loss: 1.0671606063842773, acc: 0.01171875, pred_acc: 0.76953125 gate_loss: 63.792930603027344, cls_loss: 231102928.0 cos_loss: 1.0671606063842773\n",
      "loss: 1.0967365503311157, acc: 0.01171875, pred_acc: 0.7265625 gate_loss: 59.2890625, cls_loss: 201968896.0 cos_loss: 1.0967365503311157\n",
      "loss: 1.0804603099822998, acc: 0.00390625, pred_acc: 0.75 gate_loss: 61.363277435302734, cls_loss: 210405216.0 cos_loss: 1.0804603099822998\n",
      "loss: 1.1374218463897705, acc: 0.0, pred_acc: 0.765625 gate_loss: 63.453125, cls_loss: 209141568.0 cos_loss: 1.1374218463897705\n",
      "loss: 1.0896950960159302, acc: 0.0, pred_acc: 0.71484375 gate_loss: 63.34355545043945, cls_loss: 190325472.0 cos_loss: 1.0896950960159302\n",
      "loss: 1.0792744159698486, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 64.6992416381836, cls_loss: 254446832.0 cos_loss: 1.0792744159698486\n",
      "loss: 1.061736822128296, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 67.48052978515625, cls_loss: 247101488.0 cos_loss: 1.061736822128296\n",
      "loss: 1.059999942779541, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 66.60546875, cls_loss: 255136720.0 cos_loss: 1.059999942779541\n",
      "loss: 1.0538945198059082, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 62.843753814697266, cls_loss: 270147712.0 cos_loss: 1.0538945198059082\n",
      "loss: 1.0760811567306519, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 63.11781311035156, cls_loss: 269822016.0 cos_loss: 1.0760811567306519\n",
      "loss: 1.0972626209259033, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 64.94590759277344, cls_loss: 273942208.0 cos_loss: 1.0972626209259033\n",
      "loss: 1.072965145111084, acc: 0.0, pred_acc: 0.7890625 gate_loss: 68.75848388671875, cls_loss: 308143968.0 cos_loss: 1.072965145111084\n",
      "loss: 1.1112878322601318, acc: 0.0, pred_acc: 0.734375 gate_loss: 70.07672119140625, cls_loss: 271437920.0 cos_loss: 1.1112878322601318\n",
      "loss: 1.0688750743865967, acc: 0.0, pred_acc: 0.80078125 gate_loss: 75.28125, cls_loss: 265909760.0 cos_loss: 1.0688750743865967\n",
      "loss: 1.0969815254211426, acc: 0.0078125, pred_acc: 0.73046875 gate_loss: 76.07672119140625, cls_loss: 304914656.0 cos_loss: 1.0969815254211426\n",
      "loss: 1.094186544418335, acc: 0.0, pred_acc: 0.74609375 gate_loss: 80.4375, cls_loss: 331162080.0 cos_loss: 1.094186544418335\n",
      "loss: 1.091723918914795, acc: 0.0, pred_acc: 0.79296875 gate_loss: 81.75397491455078, cls_loss: 312740640.0 cos_loss: 1.091723918914795\n",
      "loss: 1.1059073209762573, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 82.04054260253906, cls_loss: 298030848.0 cos_loss: 1.1059073209762573\n",
      "loss: 1.1164685487747192, acc: 0.0, pred_acc: 0.765625 gate_loss: 78.78385162353516, cls_loss: 271111904.0 cos_loss: 1.1164685487747192\n",
      "loss: 1.1023619174957275, acc: 0.01171875, pred_acc: 0.6875 gate_loss: 109.00480651855469, cls_loss: 436812608.0 cos_loss: 1.1023619174957275\n",
      "loss: 1.0884878635406494, acc: 0.004999999888241291, pred_acc: 0.7999999523162842 gate_loss: 129.99497985839844, cls_loss: 703931840.0 cos_loss: 1.0884878635406494\n",
      "loss: 1.141981840133667, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 138.94918823242188, cls_loss: 761756160.0 cos_loss: 1.141981840133667\n",
      "loss: 1.1561881303787231, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 141.734375, cls_loss: 822588544.0 cos_loss: 1.1561881303787231\n",
      "loss: 1.1535675525665283, acc: 0.0, pred_acc: 0.765625 gate_loss: 140.0477294921875, cls_loss: 1067057856.0 cos_loss: 1.1535675525665283\n",
      "loss: 1.168752908706665, acc: 0.0078125, pred_acc: 0.71875 gate_loss: 135.06246948242188, cls_loss: 884655360.0 cos_loss: 1.168752908706665\n",
      "loss: 1.1756905317306519, acc: 0.00390625, pred_acc: 0.73046875 gate_loss: 136.38912963867188, cls_loss: 981594304.0 cos_loss: 1.1756905317306519\n",
      "loss: 1.1092722415924072, acc: 0.0, pred_acc: 0.7890625 gate_loss: 129.0546875, cls_loss: 1085014400.0 cos_loss: 1.1092722415924072\n",
      "loss: 1.1331923007965088, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 119.36331176757812, cls_loss: 923225024.0 cos_loss: 1.1331923007965088\n",
      "loss: 1.1115835905075073, acc: 0.0, pred_acc: 0.7734375 gate_loss: 109.35546875, cls_loss: 898802496.0 cos_loss: 1.1115835905075073\n",
      "loss: 1.1089775562286377, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 99.7655258178711, cls_loss: 920983936.0 cos_loss: 1.1089775562286377\n",
      "loss: 1.0944242477416992, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 88.75791931152344, cls_loss: 805206656.0 cos_loss: 1.0944242477416992\n",
      "loss: 1.1465396881103516, acc: 0.0, pred_acc: 0.7890625 gate_loss: 77.84375, cls_loss: 669535104.0 cos_loss: 1.1465396881103516\n",
      "loss: 1.120858073234558, acc: 0.0, pred_acc: 0.82421875 gate_loss: 74.80461883544922, cls_loss: 717484800.0 cos_loss: 1.120858073234558\n",
      "loss: 1.1279735565185547, acc: 0.0, pred_acc: 0.75 gate_loss: 65.7116928100586, cls_loss: 563556544.0 cos_loss: 1.1279735565185547\n",
      "loss: 1.1024811267852783, acc: 0.0, pred_acc: 0.76953125 gate_loss: 63.59375, cls_loss: 643344832.0 cos_loss: 1.1024811267852783\n",
      "loss: 1.1665058135986328, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 63.07421875, cls_loss: 572006208.0 cos_loss: 1.1665058135986328\n",
      "loss: 1.1181470155715942, acc: 0.01171875, pred_acc: 0.7734375 gate_loss: 67.38428497314453, cls_loss: 705385728.0 cos_loss: 1.1181470155715942\n",
      "loss: 1.1089650392532349, acc: 0.0, pred_acc: 0.7265625 gate_loss: 70.07421875, cls_loss: 656843136.0 cos_loss: 1.1089650392532349\n",
      "loss: 1.0967243909835815, acc: 0.0, pred_acc: 0.76953125 gate_loss: 75.2734375, cls_loss: 984328448.0 cos_loss: 1.0967243909835815\n",
      "loss: 1.052667498588562, acc: 0.00390625, pred_acc: 0.83984375 gate_loss: 82.62108612060547, cls_loss: 1226882048.0 cos_loss: 1.052667498588562\n",
      "loss: 1.0922764539718628, acc: 0.0, pred_acc: 0.734375 gate_loss: 92.1640625, cls_loss: 1384248960.0 cos_loss: 1.0922764539718628\n",
      "loss: 1.0956400632858276, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 97.65141296386719, cls_loss: 1589586688.0 cos_loss: 1.0956400632858276\n",
      "loss: 1.0871391296386719, acc: 0.0, pred_acc: 0.76171875 gate_loss: 99.82421875, cls_loss: 1767287808.0 cos_loss: 1.0871391296386719\n",
      "loss: 1.0889264345169067, acc: 0.0, pred_acc: 0.7890625 gate_loss: 105.8984375, cls_loss: 1908142720.0 cos_loss: 1.0889264345169067\n",
      "loss: 1.0992281436920166, acc: 0.00390625, pred_acc: 0.7109375 gate_loss: 113.53515625, cls_loss: 2299840768.0 cos_loss: 1.0992281436920166\n",
      "loss: 1.1064605712890625, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 109.79296875, cls_loss: 2173137920.0 cos_loss: 1.1064605712890625\n",
      "loss: 1.0678471326828003, acc: 0.0, pred_acc: 0.765625 gate_loss: 116.546875, cls_loss: 2434199808.0 cos_loss: 1.0678471326828003\n",
      "loss: 1.1371192932128906, acc: 0.0, pred_acc: 0.765625 gate_loss: 108.4613265991211, cls_loss: 2356495872.0 cos_loss: 1.1371192932128906\n",
      "loss: 1.066900610923767, acc: 0.0, pred_acc: 0.74609375 gate_loss: 110.79296875, cls_loss: 2567907584.0 cos_loss: 1.066900610923767\n",
      "loss: 1.0838638544082642, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 106.953125, cls_loss: 2152881664.0 cos_loss: 1.0838638544082642\n",
      "loss: 1.1303576231002808, acc: 0.0, pred_acc: 0.78125 gate_loss: 106.4296875, cls_loss: 2194242816.0 cos_loss: 1.1303576231002808\n",
      "loss: 1.1012623310089111, acc: 0.0078125, pred_acc: 0.70703125 gate_loss: 106.09375, cls_loss: 2382376192.0 cos_loss: 1.1012623310089111\n",
      "loss: 1.1055443286895752, acc: 0.0, pred_acc: 0.74609375 gate_loss: 100.71875, cls_loss: 2131696512.0 cos_loss: 1.1055443286895752\n",
      "loss: 1.1040124893188477, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 95.8828125, cls_loss: 2124571776.0 cos_loss: 1.1040124893188477\n",
      "loss: 1.0943552255630493, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 100.05078125, cls_loss: 2195825408.0 cos_loss: 1.0943552255630493\n",
      "loss: 1.0774757862091064, acc: 0.0, pred_acc: 0.74609375 gate_loss: 100.44140625, cls_loss: 2439750656.0 cos_loss: 1.0774757862091064\n",
      "loss: 1.1112948656082153, acc: 0.0, pred_acc: 0.76953125 gate_loss: 97.15701293945312, cls_loss: 2313978624.0 cos_loss: 1.1112948656082153\n",
      "loss: 1.1300626993179321, acc: 0.0, pred_acc: 0.73046875 gate_loss: 98.84375, cls_loss: 2335027456.0 cos_loss: 1.1300626993179321\n",
      "loss: 1.0766788721084595, acc: 0.0, pred_acc: 0.734375 gate_loss: 99.375, cls_loss: 2448576512.0 cos_loss: 1.0766788721084595\n",
      "loss: 1.1178185939788818, acc: 0.0, pred_acc: 0.7734375 gate_loss: 95.65625, cls_loss: 2256935424.0 cos_loss: 1.1178185939788818\n",
      "loss: 1.1097252368927002, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 98.98051452636719, cls_loss: 2872757760.0 cos_loss: 1.1097252368927002\n",
      "loss: 1.14024019241333, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 101.234375, cls_loss: 2727485440.0 cos_loss: 1.14024019241333\n",
      "loss: 1.0856581926345825, acc: 0.0, pred_acc: 0.73828125 gate_loss: 99.84375, cls_loss: 2477627648.0 cos_loss: 1.0856581926345825\n",
      "loss: 1.0670100450515747, acc: 0.0, pred_acc: 0.75 gate_loss: 99.843994140625, cls_loss: 2773713920.0 cos_loss: 1.0670100450515747\n",
      "loss: 1.1109085083007812, acc: 0.0, pred_acc: 0.78125 gate_loss: 106.53516387939453, cls_loss: 2721434112.0 cos_loss: 1.1109085083007812\n",
      "loss: 1.0650429725646973, acc: 0.0, pred_acc: 0.765625 gate_loss: 103.79296875, cls_loss: 2971415552.0 cos_loss: 1.0650429725646973\n",
      "loss: 1.065085768699646, acc: 0.0, pred_acc: 0.76171875 gate_loss: 102.1953125, cls_loss: 3037330944.0 cos_loss: 1.065085768699646\n",
      "loss: 1.0884292125701904, acc: 0.0, pred_acc: 0.7734375 gate_loss: 105.40625, cls_loss: 2911579392.0 cos_loss: 1.0884292125701904\n",
      "loss: 1.1035668849945068, acc: 0.0, pred_acc: 0.81640625 gate_loss: 104.00390625, cls_loss: 2675852544.0 cos_loss: 1.1035668849945068\n",
      "loss: 1.1233879327774048, acc: 0.0, pred_acc: 0.78125 gate_loss: 107.81356048583984, cls_loss: 2603590144.0 cos_loss: 1.1233879327774048\n",
      "loss: 1.092427134513855, acc: 0.01171875, pred_acc: 0.7421875 gate_loss: 105.02734375, cls_loss: 2858100736.0 cos_loss: 1.092427134513855\n",
      "loss: 1.0506787300109863, acc: 0.0, pred_acc: 0.8046875 gate_loss: 110.32898712158203, cls_loss: 3302422784.0 cos_loss: 1.0506787300109863\n",
      "loss: 1.0581246614456177, acc: 0.0, pred_acc: 0.78515625 gate_loss: 98.12107849121094, cls_loss: 2440381184.0 cos_loss: 1.0581246614456177\n",
      "loss: 1.0998984575271606, acc: 0.0, pred_acc: 0.82421875 gate_loss: 101.8828125, cls_loss: 2333672192.0 cos_loss: 1.0998984575271606\n",
      "loss: 1.095004916191101, acc: 0.0, pred_acc: 0.7421875 gate_loss: 99.3125, cls_loss: 2306188032.0 cos_loss: 1.095004916191101\n",
      "loss: 1.120561122894287, acc: 0.0, pred_acc: 0.75390625 gate_loss: 93.64453125, cls_loss: 2003391872.0 cos_loss: 1.120561122894287\n",
      "loss: 1.0583044290542603, acc: 0.0, pred_acc: 0.76171875 gate_loss: 99.16796875, cls_loss: 2437256448.0 cos_loss: 1.0583044290542603\n",
      "loss: 1.0908197164535522, acc: 0.0, pred_acc: 0.7421875 gate_loss: 103.97660064697266, cls_loss: 2183157248.0 cos_loss: 1.0908197164535522\n",
      "loss: 1.0717945098876953, acc: 0.0, pred_acc: 0.7421875 gate_loss: 101.52734375, cls_loss: 2348598528.0 cos_loss: 1.0717945098876953\n",
      "loss: 1.0364198684692383, acc: 0.0, pred_acc: 0.796875 gate_loss: 100.24220275878906, cls_loss: 2237014784.0 cos_loss: 1.0364198684692383\n",
      "loss: 1.0694526433944702, acc: 0.0, pred_acc: 0.7890625 gate_loss: 106.2186508178711, cls_loss: 2254411264.0 cos_loss: 1.0694526433944702\n",
      "loss: 1.0790197849273682, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 100.52328491210938, cls_loss: 1964090624.0 cos_loss: 1.0790197849273682\n",
      "loss: 1.0704326629638672, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 104.58984375, cls_loss: 2412539392.0 cos_loss: 1.0704326629638672\n",
      "loss: 1.1121101379394531, acc: 0.0, pred_acc: 0.72265625 gate_loss: 101.90625, cls_loss: 1947698304.0 cos_loss: 1.1121101379394531\n",
      "loss: 1.0360814332962036, acc: 0.00390625, pred_acc: 0.73046875 gate_loss: 103.97265625, cls_loss: 2362220800.0 cos_loss: 1.0360814332962036\n",
      "loss: 1.03426194190979, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 105.55520629882812, cls_loss: 2239157504.0 cos_loss: 1.03426194190979\n",
      "loss: 1.0444285869598389, acc: 0.0, pred_acc: 0.77734375 gate_loss: 100.55461120605469, cls_loss: 1901997312.0 cos_loss: 1.0444285869598389\n",
      "loss: 1.0594890117645264, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 106.3984375, cls_loss: 2005520768.0 cos_loss: 1.0594890117645264\n",
      "loss: 1.0608638525009155, acc: 0.0, pred_acc: 0.7890625 gate_loss: 99.81249237060547, cls_loss: 1709417472.0 cos_loss: 1.0608638525009155\n",
      "loss: 1.0494946241378784, acc: 0.00390625, pred_acc: 0.75 gate_loss: 97.93359375, cls_loss: 1817322240.0 cos_loss: 1.0494946241378784\n",
      "loss: 1.0544769763946533, acc: 0.0, pred_acc: 0.76171875 gate_loss: 98.76953125, cls_loss: 1587812224.0 cos_loss: 1.0544769763946533\n",
      "loss: 1.0748374462127686, acc: 0.0, pred_acc: 0.78515625 gate_loss: 98.41728973388672, cls_loss: 1424396672.0 cos_loss: 1.0748374462127686\n",
      "loss: 1.0627944469451904, acc: 0.0, pred_acc: 0.79296875 gate_loss: 80.96939086914062, cls_loss: 1250987520.0 cos_loss: 1.0627944469451904\n",
      "loss: 1.0888237953186035, acc: 0.0, pred_acc: 0.74609375 gate_loss: 74.48016357421875, cls_loss: 879544448.0 cos_loss: 1.0888237953186035\n",
      "loss: 1.0707067251205444, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 59.3515625, cls_loss: 688744576.0 cos_loss: 1.0707067251205444\n",
      "loss: 1.0828843116760254, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 58.030941009521484, cls_loss: 712431680.0 cos_loss: 1.0828843116760254\n",
      "loss: 1.1170251369476318, acc: 0.00390625, pred_acc: 0.7109375 gate_loss: 51.76953125, cls_loss: 538639424.0 cos_loss: 1.1170251369476318\n",
      "loss: 1.0884495973587036, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 52.8746452331543, cls_loss: 553807488.0 cos_loss: 1.0884495973587036\n",
      "loss: 1.133432388305664, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 54.014251708984375, cls_loss: 528679776.0 cos_loss: 1.133432388305664\n",
      "loss: 1.0586318969726562, acc: 0.0078125, pred_acc: 0.7890625 gate_loss: 55.125, cls_loss: 567838784.0 cos_loss: 1.0586318969726562\n",
      "loss: 1.0886422395706177, acc: 0.0, pred_acc: 0.71875 gate_loss: 58.171878814697266, cls_loss: 570183104.0 cos_loss: 1.0886422395706177\n",
      "loss: 1.0842480659484863, acc: 0.0, pred_acc: 0.73828125 gate_loss: 60.505043029785156, cls_loss: 599026496.0 cos_loss: 1.0842480659484863\n",
      "loss: 1.096517562866211, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 65.57421875, cls_loss: 683337024.0 cos_loss: 1.096517562866211\n",
      "loss: 1.0483144521713257, acc: 0.0, pred_acc: 0.75390625 gate_loss: 68.99217987060547, cls_loss: 640548928.0 cos_loss: 1.0483144521713257\n",
      "loss: 1.0442748069763184, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 68.625, cls_loss: 760577792.0 cos_loss: 1.0442748069763184\n",
      "loss: 1.0496102571487427, acc: 0.0, pred_acc: 0.78125 gate_loss: 74.03515625, cls_loss: 914224512.0 cos_loss: 1.0496102571487427\n",
      "loss: 1.0498509407043457, acc: 0.0078125, pred_acc: 0.8046875 gate_loss: 77.81640625, cls_loss: 888996288.0 cos_loss: 1.0498509407043457\n",
      "loss: 1.0781772136688232, acc: 0.0, pred_acc: 0.8046875 gate_loss: 81.78231048583984, cls_loss: 984891712.0 cos_loss: 1.0781772136688232\n",
      "loss: 1.081115484237671, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 77.796142578125, cls_loss: 901548928.0 cos_loss: 1.081115484237671\n",
      "loss: 1.0340156555175781, acc: 0.0, pred_acc: 0.76953125 gate_loss: 85.8125, cls_loss: 1030896000.0 cos_loss: 1.0340156555175781\n",
      "loss: 1.0769625902175903, acc: 0.0, pred_acc: 0.7734375 gate_loss: 83.08595275878906, cls_loss: 1023477376.0 cos_loss: 1.0769625902175903\n",
      "loss: 1.045758843421936, acc: 0.0078125, pred_acc: 0.8046875 gate_loss: 82.515625, cls_loss: 867259776.0 cos_loss: 1.045758843421936\n",
      "loss: 1.0194276571273804, acc: 0.0, pred_acc: 0.765625 gate_loss: 80.0859375, cls_loss: 963876800.0 cos_loss: 1.0194276571273804\n",
      "loss: 1.0388399362564087, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 78.8671875, cls_loss: 880424640.0 cos_loss: 1.0388399362564087\n",
      "loss: 1.0656459331512451, acc: 0.0, pred_acc: 0.765625 gate_loss: 80.05469512939453, cls_loss: 960153664.0 cos_loss: 1.0656459331512451\n",
      "loss: 1.0484583377838135, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 79.59765625, cls_loss: 911181312.0 cos_loss: 1.0484583377838135\n",
      "loss: 1.0365512371063232, acc: 0.00390625, pred_acc: 0.80859375 gate_loss: 74.94921875, cls_loss: 881148544.0 cos_loss: 1.0365512371063232\n",
      "loss: 1.0273418426513672, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 75.48538208007812, cls_loss: 813283072.0 cos_loss: 1.0273418426513672\n",
      "loss: 1.0127543210983276, acc: 0.00390625, pred_acc: 0.80859375 gate_loss: 77.4053955078125, cls_loss: 829753408.0 cos_loss: 1.0127543210983276\n",
      "loss: 1.0330212116241455, acc: 0.0, pred_acc: 0.7734375 gate_loss: 89.10931396484375, cls_loss: 1051553024.0 cos_loss: 1.0330212116241455\n",
      "loss: 1.0371289253234863, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 98.49993896484375, cls_loss: 1356049664.0 cos_loss: 1.0371289253234863\n",
      "loss: 1.0637726783752441, acc: 0.0, pred_acc: 0.78515625 gate_loss: 106.85546875, cls_loss: 1532929408.0 cos_loss: 1.0637726783752441\n",
      "loss: 1.0449455976486206, acc: 0.0078125, pred_acc: 0.72265625 gate_loss: 108.10551452636719, cls_loss: 1721403392.0 cos_loss: 1.0449455976486206\n",
      "loss: 1.0515704154968262, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 109.8046875, cls_loss: 1712579328.0 cos_loss: 1.0515704154968262\n",
      "loss: 1.0751190185546875, acc: 0.00390625, pred_acc: 0.72265625 gate_loss: 108.53089904785156, cls_loss: 1665430656.0 cos_loss: 1.0751190185546875\n",
      "loss: 1.0589786767959595, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 99.3671875, cls_loss: 1537588736.0 cos_loss: 1.0589786767959595\n",
      "loss: 1.0561716556549072, acc: 0.01171875, pred_acc: 0.78515625 gate_loss: 93.7734375, cls_loss: 1307570048.0 cos_loss: 1.0561716556549072\n",
      "loss: 1.0343271493911743, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 88.98731231689453, cls_loss: 1279461376.0 cos_loss: 1.0343271493911743\n",
      "loss: 1.0359032154083252, acc: 0.0, pred_acc: 0.7421875 gate_loss: 77.5390625, cls_loss: 1042557504.0 cos_loss: 1.0359032154083252\n",
      "loss: 1.0942177772521973, acc: 0.0, pred_acc: 0.79296875 gate_loss: 72.6017074584961, cls_loss: 1026994304.0 cos_loss: 1.0942177772521973\n",
      "loss: 1.0496265888214111, acc: 0.01171875, pred_acc: 0.765625 gate_loss: 68.375, cls_loss: 932426240.0 cos_loss: 1.0496265888214111\n",
      "loss: 1.0821576118469238, acc: 0.0, pred_acc: 0.7421875 gate_loss: 59.328125, cls_loss: 847505280.0 cos_loss: 1.0821576118469238\n",
      "loss: 1.086904525756836, acc: 0.0, pred_acc: 0.734375 gate_loss: 59.63671875, cls_loss: 805121152.0 cos_loss: 1.086904525756836\n",
      "loss: 1.0607963800430298, acc: 0.0, pred_acc: 0.76171875 gate_loss: 60.5234375, cls_loss: 830550656.0 cos_loss: 1.0607963800430298\n",
      "loss: 1.0400224924087524, acc: 0.0078125, pred_acc: 0.80078125 gate_loss: 58.85546875, cls_loss: 878404544.0 cos_loss: 1.0400224924087524\n",
      "loss: 1.1269216537475586, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 58.9921875, cls_loss: 895161600.0 cos_loss: 1.1269216537475586\n",
      "loss: 1.0549285411834717, acc: 0.0, pred_acc: 0.81640625 gate_loss: 60.375, cls_loss: 862111616.0 cos_loss: 1.0549285411834717\n",
      "loss: 1.0848636627197266, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 65.39344787597656, cls_loss: 946244736.0 cos_loss: 1.0848636627197266\n",
      "loss: 1.0633946657180786, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 69.6812744140625, cls_loss: 996278400.0 cos_loss: 1.0633946657180786\n",
      "loss: 1.0601545572280884, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 68.31640625, cls_loss: 1187430784.0 cos_loss: 1.0601545572280884\n",
      "loss: 1.0798394680023193, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 73.5, cls_loss: 1307661056.0 cos_loss: 1.0798394680023193\n",
      "loss: 1.0707736015319824, acc: 0.0, pred_acc: 0.82421875 gate_loss: 73.16421508789062, cls_loss: 1102769664.0 cos_loss: 1.0707736015319824\n",
      "loss: 1.0579452514648438, acc: 0.0, pred_acc: 0.75390625 gate_loss: 71.19140625, cls_loss: 1326140928.0 cos_loss: 1.0579452514648438\n",
      "loss: 1.051032304763794, acc: 0.0, pred_acc: 0.7734375 gate_loss: 70.625, cls_loss: 1071865600.0 cos_loss: 1.051032304763794\n",
      "loss: 1.009682536125183, acc: 0.0, pred_acc: 0.7265625 gate_loss: 73.82421875, cls_loss: 1450252800.0 cos_loss: 1.009682536125183\n",
      "loss: 1.052022933959961, acc: 0.0, pred_acc: 0.7421875 gate_loss: 72.8203125, cls_loss: 1292600448.0 cos_loss: 1.052022933959961\n",
      "loss: 1.06108820438385, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 68.859375, cls_loss: 1220749184.0 cos_loss: 1.06108820438385\n",
      "loss: 1.0438487529754639, acc: 0.0078125, pred_acc: 0.73046875 gate_loss: 67.50938415527344, cls_loss: 1087248640.0 cos_loss: 1.0438487529754639\n",
      "loss: 1.0273383855819702, acc: 0.0, pred_acc: 0.77734375 gate_loss: 64.38671875, cls_loss: 1145608448.0 cos_loss: 1.0273383855819702\n",
      "loss: 1.0631353855133057, acc: 0.00390625, pred_acc: 0.73828125 gate_loss: 62.64453125, cls_loss: 930213760.0 cos_loss: 1.0631353855133057\n",
      "loss: 1.01179838180542, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 62.48046875, cls_loss: 1132947328.0 cos_loss: 1.01179838180542\n",
      "loss: 1.0647876262664795, acc: 0.0078125, pred_acc: 0.703125 gate_loss: 62.312583923339844, cls_loss: 994401280.0 cos_loss: 1.0647876262664795\n",
      "loss: 1.0427440404891968, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 58.56640625, cls_loss: 1027472256.0 cos_loss: 1.0427440404891968\n",
      "loss: 1.0444142818450928, acc: 0.0078125, pred_acc: 0.71875 gate_loss: 61.515625, cls_loss: 1137992704.0 cos_loss: 1.0444142818450928\n",
      "loss: 1.036054015159607, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 63.42578125, cls_loss: 1165609216.0 cos_loss: 1.036054015159607\n",
      "loss: 1.0510272979736328, acc: 0.00390625, pred_acc: 0.73828125 gate_loss: 63.464820861816406, cls_loss: 1097401472.0 cos_loss: 1.0510272979736328\n",
      "loss: 1.026519536972046, acc: 0.0, pred_acc: 0.78125 gate_loss: 66.98046875, cls_loss: 1313261952.0 cos_loss: 1.026519536972046\n",
      "loss: 1.0625286102294922, acc: 0.0078125, pred_acc: 0.7265625 gate_loss: 69.1875, cls_loss: 1200637952.0 cos_loss: 1.0625286102294922\n",
      "loss: 1.0229007005691528, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 73.1015625, cls_loss: 1212476160.0 cos_loss: 1.0229007005691528\n",
      "loss: 1.0540956258773804, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 72.53125, cls_loss: 1361617280.0 cos_loss: 1.0540956258773804\n",
      "loss: 1.0284066200256348, acc: 0.0, pred_acc: 0.7890625 gate_loss: 77.5240707397461, cls_loss: 1392575232.0 cos_loss: 1.0284066200256348\n",
      "loss: 1.020725131034851, acc: 0.00390625, pred_acc: 0.6953125 gate_loss: 73.19921875, cls_loss: 1322375040.0 cos_loss: 1.020725131034851\n",
      "loss: 1.0188459157943726, acc: 0.0078125, pred_acc: 0.8203125 gate_loss: 75.92578125, cls_loss: 1322430848.0 cos_loss: 1.0188459157943726\n",
      "loss: 1.026552438735962, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 75.44139862060547, cls_loss: 1386020096.0 cos_loss: 1.026552438735962\n",
      "loss: 1.0531872510910034, acc: 0.0, pred_acc: 0.7734375 gate_loss: 75.51171875, cls_loss: 1349317888.0 cos_loss: 1.0531872510910034\n",
      "loss: 1.0163408517837524, acc: 0.0078125, pred_acc: 0.796875 gate_loss: 80.23046875, cls_loss: 1501892096.0 cos_loss: 1.0163408517837524\n",
      "loss: 1.0500997304916382, acc: 0.0, pred_acc: 0.76953125 gate_loss: 80.39459228515625, cls_loss: 1314817024.0 cos_loss: 1.0500997304916382\n",
      "loss: 1.0284671783447266, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 76.969482421875, cls_loss: 1344920576.0 cos_loss: 1.0284671783447266\n",
      "loss: 1.023139476776123, acc: 0.01171875, pred_acc: 0.73046875 gate_loss: 77.65234375, cls_loss: 1286431232.0 cos_loss: 1.023139476776123\n",
      "loss: 1.0615710020065308, acc: 0.0, pred_acc: 0.75 gate_loss: 76.26564025878906, cls_loss: 1287205632.0 cos_loss: 1.0615710020065308\n",
      "loss: 1.0185638666152954, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 74.61328125, cls_loss: 1339034112.0 cos_loss: 1.0185638666152954\n",
      "loss: 1.0233033895492554, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 74.890625, cls_loss: 1363847936.0 cos_loss: 1.0233033895492554\n",
      "loss: 1.05479097366333, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 76.92578125, cls_loss: 1309521152.0 cos_loss: 1.05479097366333\n",
      "loss: 1.0002553462982178, acc: 0.00390625, pred_acc: 0.8046875 gate_loss: 78.06253051757812, cls_loss: 1363241216.0 cos_loss: 1.0002553462982178\n",
      "loss: 1.0238038301467896, acc: 0.0, pred_acc: 0.77734375 gate_loss: 77.38671875, cls_loss: 1457411072.0 cos_loss: 1.0238038301467896\n",
      "loss: 1.023789644241333, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 79.26953125, cls_loss: 1382499584.0 cos_loss: 1.023789644241333\n",
      "loss: 1.0564831495285034, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 83.0558853149414, cls_loss: 1357098496.0 cos_loss: 1.0564831495285034\n",
      "loss: 1.0442966222763062, acc: 0.0, pred_acc: 0.76953125 gate_loss: 83.015625, cls_loss: 1279967104.0 cos_loss: 1.0442966222763062\n",
      "loss: 1.0191810131072998, acc: 0.01953125, pred_acc: 0.7890625 gate_loss: 80.59957885742188, cls_loss: 1378209664.0 cos_loss: 1.0191810131072998\n",
      "loss: 1.0209141969680786, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 96.80859375, cls_loss: 1687496448.0 cos_loss: 1.0209141969680786\n",
      "loss: 1.0170637369155884, acc: 0.0, pred_acc: 0.75 gate_loss: 108.83984375, cls_loss: 2154557440.0 cos_loss: 1.0170637369155884\n",
      "loss: 1.0409165620803833, acc: 0.0, pred_acc: 0.7890625 gate_loss: 123.41015625, cls_loss: 2375811840.0 cos_loss: 1.0409165620803833\n",
      "loss: 1.0320111513137817, acc: 0.0, pred_acc: 0.7734375 gate_loss: 133.359375, cls_loss: 2603090432.0 cos_loss: 1.0320111513137817\n",
      "loss: 0.9807248115539551, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 137.35548400878906, cls_loss: 2944864512.0 cos_loss: 0.9807248115539551\n",
      "loss: 1.0439908504486084, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 134.98828125, cls_loss: 2844760320.0 cos_loss: 1.0439908504486084\n",
      "loss: 1.034099817276001, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 134.21487426757812, cls_loss: 2884547072.0 cos_loss: 1.034099817276001\n",
      "loss: 1.029022216796875, acc: 0.0, pred_acc: 0.76171875 gate_loss: 129.69784545898438, cls_loss: 2791658496.0 cos_loss: 1.029022216796875\n",
      "loss: 1.006869912147522, acc: 0.00390625, pred_acc: 0.80859375 gate_loss: 125.35182189941406, cls_loss: 3445081600.0 cos_loss: 1.006869912147522\n",
      "loss: 1.0364954471588135, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 120.47782897949219, cls_loss: 3164670208.0 cos_loss: 1.0364954471588135\n",
      "loss: 1.0246505737304688, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 114.69804382324219, cls_loss: 2908237056.0 cos_loss: 1.0246505737304688\n",
      "loss: 1.0315433740615845, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 111.11543273925781, cls_loss: 2705516800.0 cos_loss: 1.0315433740615845\n",
      "loss: 1.0092802047729492, acc: 0.0, pred_acc: 0.7421875 gate_loss: 109.2109375, cls_loss: 2714353152.0 cos_loss: 1.0092802047729492\n",
      "loss: 1.0345163345336914, acc: 0.0, pred_acc: 0.7265625 gate_loss: 108.23828125, cls_loss: 2498369536.0 cos_loss: 1.0345163345336914\n",
      "loss: 1.0235940217971802, acc: 0.0, pred_acc: 0.75390625 gate_loss: 107.07037353515625, cls_loss: 2276946688.0 cos_loss: 1.0235940217971802\n",
      "loss: 1.0076708793640137, acc: 0.0, pred_acc: 0.8125 gate_loss: 106.33984375, cls_loss: 2500207360.0 cos_loss: 1.0076708793640137\n",
      "loss: 1.0191805362701416, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 104.328125, cls_loss: 2457432832.0 cos_loss: 1.0191805362701416\n",
      "loss: 1.0568376779556274, acc: 0.009999999776482582, pred_acc: 0.7849999666213989 gate_loss: 102.76499938964844, cls_loss: 1992237696.0 cos_loss: 1.0568376779556274\n",
      "loss: 1.0108208656311035, acc: 0.0, pred_acc: 0.71484375 gate_loss: 99.19140625, cls_loss: 1962822784.0 cos_loss: 1.0108208656311035\n",
      "loss: 1.0052076578140259, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 96.70703125, cls_loss: 1940131200.0 cos_loss: 1.0052076578140259\n",
      "loss: 1.0098013877868652, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 92.76953125, cls_loss: 1599366528.0 cos_loss: 1.0098013877868652\n",
      "loss: 1.0705450773239136, acc: 0.00390625, pred_acc: 0.73046875 gate_loss: 86.625, cls_loss: 1531710720.0 cos_loss: 1.0705450773239136\n",
      "loss: 1.0374562740325928, acc: 0.0, pred_acc: 0.734375 gate_loss: 83.49222564697266, cls_loss: 1517913728.0 cos_loss: 1.0374562740325928\n",
      "loss: 1.0295062065124512, acc: 0.00390625, pred_acc: 0.79296875 gate_loss: 81.64453125, cls_loss: 1412387840.0 cos_loss: 1.0295062065124512\n",
      "loss: 1.0583956241607666, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 79.95703125, cls_loss: 1553920896.0 cos_loss: 1.0583956241607666\n",
      "loss: 1.0312296152114868, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 74.88734436035156, cls_loss: 1462602112.0 cos_loss: 1.0312296152114868\n",
      "loss: 1.0524992942810059, acc: 0.00390625, pred_acc: 0.71484375 gate_loss: 77.66777038574219, cls_loss: 1383353600.0 cos_loss: 1.0524992942810059\n",
      "loss: 1.0522280931472778, acc: 0.0, pred_acc: 0.7421875 gate_loss: 79.61328125, cls_loss: 1497369216.0 cos_loss: 1.0522280931472778\n",
      "loss: 1.0328985452651978, acc: 0.0, pred_acc: 0.73828125 gate_loss: 83.76348876953125, cls_loss: 1620135552.0 cos_loss: 1.0328985452651978\n",
      "loss: 1.03383207321167, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 88.63284301757812, cls_loss: 1506056064.0 cos_loss: 1.03383207321167\n",
      "loss: 1.0425596237182617, acc: 0.01171875, pred_acc: 0.7734375 gate_loss: 89.75, cls_loss: 1669817984.0 cos_loss: 1.0425596237182617\n",
      "loss: 1.028040885925293, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 89.60153198242188, cls_loss: 1559700096.0 cos_loss: 1.028040885925293\n",
      "loss: 1.021450161933899, acc: 0.0078125, pred_acc: 0.796875 gate_loss: 90.98828125, cls_loss: 1702899328.0 cos_loss: 1.021450161933899\n",
      "loss: 1.0171418190002441, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 92.171875, cls_loss: 1658103552.0 cos_loss: 1.0171418190002441\n",
      "loss: 1.0316588878631592, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 89.88671875, cls_loss: 1709563392.0 cos_loss: 1.0316588878631592\n",
      "loss: 1.0124119520187378, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 90.17610931396484, cls_loss: 1767072768.0 cos_loss: 1.0124119520187378\n",
      "loss: 1.0230056047439575, acc: 0.00390625, pred_acc: 0.80859375 gate_loss: 87.33998107910156, cls_loss: 1651344512.0 cos_loss: 1.0230056047439575\n",
      "loss: 1.0300264358520508, acc: 0.0, pred_acc: 0.796875 gate_loss: 85.8515625, cls_loss: 1603298560.0 cos_loss: 1.0300264358520508\n",
      "loss: 1.016961693763733, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 84.2109375, cls_loss: 1496576640.0 cos_loss: 1.016961693763733\n",
      "loss: 1.029345154762268, acc: 0.0, pred_acc: 0.75390625 gate_loss: 82.55078125, cls_loss: 1599898496.0 cos_loss: 1.029345154762268\n",
      "loss: 1.01102614402771, acc: 0.01171875, pred_acc: 0.76953125 gate_loss: 82.49961853027344, cls_loss: 1469920256.0 cos_loss: 1.01102614402771\n",
      "loss: 0.9993111491203308, acc: 0.0, pred_acc: 0.76171875 gate_loss: 80.42967224121094, cls_loss: 1590445696.0 cos_loss: 0.9993111491203308\n",
      "loss: 1.0324745178222656, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 81.3633041381836, cls_loss: 1572351488.0 cos_loss: 1.0324745178222656\n",
      "loss: 1.0407785177230835, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 81.29296875, cls_loss: 1481129856.0 cos_loss: 1.0407785177230835\n",
      "loss: 1.0303692817687988, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 81.94140625, cls_loss: 1429490816.0 cos_loss: 1.0303692817687988\n",
      "loss: 0.9944095015525818, acc: 0.00390625, pred_acc: 0.80859375 gate_loss: 83.25, cls_loss: 1577583616.0 cos_loss: 0.9944095015525818\n",
      "loss: 0.9821939468383789, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 83.89479064941406, cls_loss: 1413398400.0 cos_loss: 0.9821939468383789\n",
      "loss: 1.0036979913711548, acc: 0.0078125, pred_acc: 0.73828125 gate_loss: 83.62419128417969, cls_loss: 1553394304.0 cos_loss: 1.0036979913711548\n",
      "loss: 1.005908727645874, acc: 0.0, pred_acc: 0.7890625 gate_loss: 86.1640625, cls_loss: 1505140992.0 cos_loss: 1.005908727645874\n",
      "loss: 1.021693229675293, acc: 0.0, pred_acc: 0.796875 gate_loss: 88.45069122314453, cls_loss: 1440784000.0 cos_loss: 1.021693229675293\n",
      "loss: 1.0429641008377075, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 88.77734375, cls_loss: 1575151872.0 cos_loss: 1.0429641008377075\n",
      "loss: 1.013808012008667, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 86.72264862060547, cls_loss: 1485802240.0 cos_loss: 1.013808012008667\n",
      "loss: 1.0010254383087158, acc: 0.0, pred_acc: 0.77734375 gate_loss: 87.62109375, cls_loss: 1414511872.0 cos_loss: 1.0010254383087158\n",
      "loss: 1.0211446285247803, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 86.47286987304688, cls_loss: 1517134464.0 cos_loss: 1.0211446285247803\n",
      "loss: 0.9967696666717529, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 84.39058685302734, cls_loss: 1416405888.0 cos_loss: 0.9967696666717529\n",
      "loss: 0.9930235147476196, acc: 0.0, pred_acc: 0.71875 gate_loss: 87.73828125, cls_loss: 1412223488.0 cos_loss: 0.9930235147476196\n",
      "loss: 0.986987292766571, acc: 0.015625, pred_acc: 0.76953125 gate_loss: 87.7734375, cls_loss: 1482585600.0 cos_loss: 0.986987292766571\n",
      "loss: 0.9909636378288269, acc: 0.0, pred_acc: 0.79296875 gate_loss: 88.53907775878906, cls_loss: 1439343616.0 cos_loss: 0.9909636378288269\n",
      "loss: 1.020821213722229, acc: 0.0, pred_acc: 0.74609375 gate_loss: 88.671875, cls_loss: 1548870912.0 cos_loss: 1.020821213722229\n",
      "loss: 0.9861756563186646, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 92.39848327636719, cls_loss: 1604004736.0 cos_loss: 0.9861756563186646\n",
      "loss: 0.9569957256317139, acc: 0.0078125, pred_acc: 0.7890625 gate_loss: 93.0859375, cls_loss: 1534188032.0 cos_loss: 0.9569957256317139\n",
      "loss: 1.0170081853866577, acc: 0.0, pred_acc: 0.71875 gate_loss: 93.078125, cls_loss: 1381623040.0 cos_loss: 1.0170081853866577\n",
      "loss: 1.0094492435455322, acc: 0.0, pred_acc: 0.78515625 gate_loss: 94.92273712158203, cls_loss: 1674587136.0 cos_loss: 1.0094492435455322\n",
      "loss: 0.990960419178009, acc: 0.0, pred_acc: 0.7734375 gate_loss: 94.12890625, cls_loss: 1481444864.0 cos_loss: 0.990960419178009\n",
      "loss: 0.9621520042419434, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 94.234375, cls_loss: 1597504896.0 cos_loss: 0.9621520042419434\n",
      "loss: 1.0024759769439697, acc: 0.0, pred_acc: 0.76171875 gate_loss: 94.12655639648438, cls_loss: 1420256384.0 cos_loss: 1.0024759769439697\n",
      "loss: 0.9870107173919678, acc: 0.0, pred_acc: 0.75390625 gate_loss: 92.9609375, cls_loss: 1374479104.0 cos_loss: 0.9870107173919678\n",
      "loss: 0.9980934858322144, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 97.80078125, cls_loss: 1412977536.0 cos_loss: 0.9980934858322144\n",
      "loss: 0.986947774887085, acc: 0.0078125, pred_acc: 0.70703125 gate_loss: 95.98828125, cls_loss: 1559401472.0 cos_loss: 0.986947774887085\n",
      "loss: 0.9608213901519775, acc: 0.0, pred_acc: 0.7578125 gate_loss: 97.03907775878906, cls_loss: 1894348544.0 cos_loss: 0.9608213901519775\n",
      "loss: 0.9835359454154968, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 99.29296875, cls_loss: 1682316672.0 cos_loss: 0.9835359454154968\n",
      "loss: 0.9794653058052063, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 98.18363952636719, cls_loss: 1629422848.0 cos_loss: 0.9794653058052063\n",
      "loss: 1.004191517829895, acc: 0.01171875, pred_acc: 0.78515625 gate_loss: 100.23828125, cls_loss: 1533222784.0 cos_loss: 1.004191517829895\n",
      "loss: 0.9826741814613342, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 100.58984375, cls_loss: 1441587584.0 cos_loss: 0.9826741814613342\n",
      "loss: 0.9932650923728943, acc: 0.015625, pred_acc: 0.76953125 gate_loss: 100.41388702392578, cls_loss: 1616833664.0 cos_loss: 0.9932650923728943\n",
      "loss: 1.00128173828125, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 101.87461853027344, cls_loss: 1491908352.0 cos_loss: 1.00128173828125\n",
      "loss: 0.9842827320098877, acc: 0.01171875, pred_acc: 0.80859375 gate_loss: 102.72266387939453, cls_loss: 1609333120.0 cos_loss: 0.9842827320098877\n",
      "loss: 0.9913418292999268, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 99.06834411621094, cls_loss: 1512823936.0 cos_loss: 0.9913418292999268\n",
      "loss: 0.9990934133529663, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 97.43359375, cls_loss: 1575095424.0 cos_loss: 0.9990934133529663\n",
      "loss: 0.9673746824264526, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 98.82421875, cls_loss: 1613931904.0 cos_loss: 0.9673746824264526\n",
      "loss: 0.9960960745811462, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 97.3984375, cls_loss: 1458532736.0 cos_loss: 0.9960960745811462\n",
      "loss: 1.0184071063995361, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 95.62109375, cls_loss: 1450516480.0 cos_loss: 1.0184071063995361\n",
      "loss: 0.9843043684959412, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 96.1015625, cls_loss: 1533118592.0 cos_loss: 0.9843043684959412\n",
      "loss: 0.9768655300140381, acc: 0.0078125, pred_acc: 0.75 gate_loss: 95.35185241699219, cls_loss: 1564563072.0 cos_loss: 0.9768655300140381\n",
      "loss: 1.0235435962677002, acc: 0.00390625, pred_acc: 0.7265625 gate_loss: 96.58984375, cls_loss: 1268670464.0 cos_loss: 1.0235435962677002\n",
      "loss: 0.9505916833877563, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 98.39453125, cls_loss: 1454700800.0 cos_loss: 0.9505916833877563\n",
      "loss: 1.0020577907562256, acc: 0.01171875, pred_acc: 0.78125 gate_loss: 97.16790771484375, cls_loss: 1535536768.0 cos_loss: 1.0020577907562256\n",
      "loss: 1.0035924911499023, acc: 0.01171875, pred_acc: 0.76953125 gate_loss: 96.5, cls_loss: 1536619648.0 cos_loss: 1.0035924911499023\n",
      "loss: 0.9912521839141846, acc: 0.00390625, pred_acc: 0.8125 gate_loss: 95.5390625, cls_loss: 1562393472.0 cos_loss: 0.9912521839141846\n",
      "loss: 0.9403216242790222, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 95.72265625, cls_loss: 1435887872.0 cos_loss: 0.9403216242790222\n",
      "loss: 0.9840972423553467, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 99.0703125, cls_loss: 1455640576.0 cos_loss: 0.9840972423553467\n",
      "loss: 0.9841773509979248, acc: 0.00390625, pred_acc: 0.8203125 gate_loss: 97.0390625, cls_loss: 1673163904.0 cos_loss: 0.9841773509979248\n",
      "loss: 0.9906982183456421, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 98.27734375, cls_loss: 1482636800.0 cos_loss: 0.9906982183456421\n",
      "loss: 0.9915103316307068, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 97.37109375, cls_loss: 1415930496.0 cos_loss: 0.9915103316307068\n",
      "loss: 0.9803096055984497, acc: 0.01171875, pred_acc: 0.79296875 gate_loss: 98.68359375, cls_loss: 1486294656.0 cos_loss: 0.9803096055984497\n",
      "loss: 1.0050663948059082, acc: 0.01171875, pred_acc: 0.7421875 gate_loss: 100.09367370605469, cls_loss: 1464935296.0 cos_loss: 1.0050663948059082\n",
      "loss: 0.9479570388793945, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 99.96875, cls_loss: 1451996544.0 cos_loss: 0.9479570388793945\n",
      "loss: 0.9944521188735962, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 100.76171875, cls_loss: 1620580352.0 cos_loss: 0.9944521188735962\n",
      "loss: 0.980270266532898, acc: 0.015625, pred_acc: 0.7578125 gate_loss: 100.03125, cls_loss: 1358857344.0 cos_loss: 0.980270266532898\n",
      "loss: 0.9919071197509766, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 101.66020965576172, cls_loss: 1512163072.0 cos_loss: 0.9919071197509766\n",
      "loss: 0.9911500811576843, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 102.76351165771484, cls_loss: 1658702208.0 cos_loss: 0.9911500811576843\n",
      "loss: 0.9899152517318726, acc: 0.0, pred_acc: 0.72265625 gate_loss: 101.28125, cls_loss: 1475953792.0 cos_loss: 0.9899152517318726\n",
      "loss: 0.9706991910934448, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 102.5078125, cls_loss: 1408804864.0 cos_loss: 0.9706991910934448\n",
      "loss: 0.9509814977645874, acc: 0.00390625, pred_acc: 0.73828125 gate_loss: 102.80657958984375, cls_loss: 1512205952.0 cos_loss: 0.9509814977645874\n",
      "loss: 0.9834491014480591, acc: 0.0, pred_acc: 0.7734375 gate_loss: 103.34765625, cls_loss: 1427438080.0 cos_loss: 0.9834491014480591\n",
      "loss: 0.9602280259132385, acc: 0.0078125, pred_acc: 0.80078125 gate_loss: 101.84765625, cls_loss: 1297940736.0 cos_loss: 0.9602280259132385\n",
      "loss: 0.9813944101333618, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 100.79296875, cls_loss: 1541203584.0 cos_loss: 0.9813944101333618\n",
      "loss: 0.9578356742858887, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 102.04296875, cls_loss: 1474607360.0 cos_loss: 0.9578356742858887\n",
      "loss: 0.985916256904602, acc: 0.0, pred_acc: 0.76171875 gate_loss: 102.71875, cls_loss: 1444898432.0 cos_loss: 0.985916256904602\n",
      "loss: 0.9953747391700745, acc: 0.01171875, pred_acc: 0.71875 gate_loss: 101.10459899902344, cls_loss: 1346551168.0 cos_loss: 0.9953747391700745\n",
      "loss: 0.9592052698135376, acc: 0.00390625, pred_acc: 0.79296875 gate_loss: 105.484375, cls_loss: 1634842368.0 cos_loss: 0.9592052698135376\n",
      "loss: 0.9903336763381958, acc: 0.0, pred_acc: 0.8046875 gate_loss: 111.53943634033203, cls_loss: 1747014400.0 cos_loss: 0.9903336763381958\n",
      "loss: 0.9703059196472168, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 113.80859375, cls_loss: 2042456064.0 cos_loss: 0.9703059196472168\n",
      "loss: 1.0083043575286865, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 114.22265625, cls_loss: 2047608704.0 cos_loss: 1.0083043575286865\n",
      "loss: 1.0109199285507202, acc: 0.0, pred_acc: 0.72265625 gate_loss: 111.9453125, cls_loss: 2002397696.0 cos_loss: 1.0109199285507202\n",
      "loss: 0.9682477712631226, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 111.96484375, cls_loss: 2118915200.0 cos_loss: 0.9682477712631226\n",
      "loss: 1.0149600505828857, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 108.4375, cls_loss: 1723396736.0 cos_loss: 1.0149600505828857\n",
      "loss: 1.0036250352859497, acc: 0.0, pred_acc: 0.7421875 gate_loss: 106.6022720336914, cls_loss: 1804403200.0 cos_loss: 1.0036250352859497\n",
      "loss: 1.00198233127594, acc: 0.0, pred_acc: 0.734375 gate_loss: 99.91014862060547, cls_loss: 1636999680.0 cos_loss: 1.00198233127594\n",
      "loss: 0.9943326711654663, acc: 0.00390625, pred_acc: 0.8125 gate_loss: 96.56640625, cls_loss: 1671761280.0 cos_loss: 0.9943326711654663\n",
      "loss: 1.0198075771331787, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 96.59705352783203, cls_loss: 1528076032.0 cos_loss: 1.0198075771331787\n",
      "loss: 0.9883754849433899, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 92.54296875, cls_loss: 1568434816.0 cos_loss: 0.9883754849433899\n",
      "loss: 0.9819434881210327, acc: 0.0, pred_acc: 0.75 gate_loss: 93.42578125, cls_loss: 1608750464.0 cos_loss: 0.9819434881210327\n",
      "loss: 1.0315076112747192, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 92.96875, cls_loss: 1460240640.0 cos_loss: 1.0315076112747192\n",
      "loss: 0.9977046251296997, acc: 0.0, pred_acc: 0.80078125 gate_loss: 93.609375, cls_loss: 1854919424.0 cos_loss: 0.9977046251296997\n",
      "loss: 1.01178777217865, acc: 0.0, pred_acc: 0.75 gate_loss: 89.23307800292969, cls_loss: 1585389696.0 cos_loss: 1.01178777217865\n",
      "loss: 1.0187793970108032, acc: 0.0, pred_acc: 0.76953125 gate_loss: 91.8828125, cls_loss: 1659689984.0 cos_loss: 1.0187793970108032\n",
      "loss: 1.0260062217712402, acc: 0.0, pred_acc: 0.77734375 gate_loss: 92.48046875, cls_loss: 1471219712.0 cos_loss: 1.0260062217712402\n",
      "loss: 0.992098331451416, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 95.25390625, cls_loss: 1726919936.0 cos_loss: 0.992098331451416\n",
      "loss: 1.026369571685791, acc: 0.0, pred_acc: 0.7578125 gate_loss: 94.5858154296875, cls_loss: 1626083840.0 cos_loss: 1.026369571685791\n",
      "loss: 1.0000267028808594, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 93.17182922363281, cls_loss: 1729992064.0 cos_loss: 1.0000267028808594\n",
      "loss: 0.9677408933639526, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 97.1328125, cls_loss: 1575543168.0 cos_loss: 0.9677408933639526\n",
      "loss: 0.9957544803619385, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 95.9609375, cls_loss: 1602273536.0 cos_loss: 0.9957544803619385\n",
      "loss: 1.021687388420105, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 96.15625, cls_loss: 1603324416.0 cos_loss: 1.021687388420105\n",
      "loss: 1.010310411453247, acc: 0.0, pred_acc: 0.78125 gate_loss: 100.6953125, cls_loss: 1736238976.0 cos_loss: 1.010310411453247\n",
      "loss: 0.9988512396812439, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 102.41403198242188, cls_loss: 1639342976.0 cos_loss: 0.9988512396812439\n",
      "loss: 0.9814666509628296, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 103.6171875, cls_loss: 1713800576.0 cos_loss: 0.9814666509628296\n",
      "loss: 0.9842413663864136, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 106.55859375, cls_loss: 1625065344.0 cos_loss: 0.9842413663864136\n",
      "loss: 0.9702814817428589, acc: 0.0, pred_acc: 0.78125 gate_loss: 107.1640625, cls_loss: 1690302080.0 cos_loss: 0.9702814817428589\n",
      "loss: 0.9928219318389893, acc: 0.00390625, pred_acc: 0.80859375 gate_loss: 108.26171875, cls_loss: 1618633216.0 cos_loss: 0.9928219318389893\n",
      "loss: 1.0131553411483765, acc: 0.0078125, pred_acc: 0.75 gate_loss: 105.83203125, cls_loss: 1701550336.0 cos_loss: 1.0131553411483765\n",
      "loss: 0.9763889312744141, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 106.33984375, cls_loss: 1792173184.0 cos_loss: 0.9763889312744141\n",
      "loss: 0.9695247411727905, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 104.43359375, cls_loss: 1582897920.0 cos_loss: 0.9695247411727905\n",
      "loss: 0.9949042797088623, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 104.79296875, cls_loss: 1566007168.0 cos_loss: 0.9949042797088623\n",
      "loss: 0.9999105930328369, acc: 0.0078125, pred_acc: 0.78125 gate_loss: 101.26953125, cls_loss: 1519598080.0 cos_loss: 0.9999105930328369\n",
      "loss: 0.9731688499450684, acc: 0.01171875, pred_acc: 0.79296875 gate_loss: 102.42573547363281, cls_loss: 1502363520.0 cos_loss: 0.9731688499450684\n",
      "loss: 0.968245267868042, acc: 0.0, pred_acc: 0.76171875 gate_loss: 99.41796875, cls_loss: 1524957184.0 cos_loss: 0.968245267868042\n",
      "loss: 0.9793649315834045, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 101.1640625, cls_loss: 1457830016.0 cos_loss: 0.9793649315834045\n",
      "loss: 0.9666591882705688, acc: 0.0, pred_acc: 0.78515625 gate_loss: 102.734375, cls_loss: 1466816128.0 cos_loss: 0.9666591882705688\n",
      "loss: 0.9877221584320068, acc: 0.0, pred_acc: 0.76953125 gate_loss: 103.9765625, cls_loss: 1356761600.0 cos_loss: 0.9877221584320068\n",
      "loss: 0.9889017939567566, acc: 0.01171875, pred_acc: 0.7421875 gate_loss: 102.71484375, cls_loss: 1406560512.0 cos_loss: 0.9889017939567566\n",
      "loss: 0.9723094701766968, acc: 0.01171875, pred_acc: 0.76953125 gate_loss: 103.87499237060547, cls_loss: 1414693888.0 cos_loss: 0.9723094701766968\n",
      "loss: 0.9963376522064209, acc: 0.0078125, pred_acc: 0.80859375 gate_loss: 105.6875228881836, cls_loss: 1433246208.0 cos_loss: 0.9963376522064209\n",
      "loss: 0.9704394340515137, acc: 0.0, pred_acc: 0.7578125 gate_loss: 106.16471099853516, cls_loss: 1360701568.0 cos_loss: 0.9704394340515137\n",
      "loss: 0.992251455783844, acc: 0.00390625, pred_acc: 0.75 gate_loss: 105.63684844970703, cls_loss: 1324629632.0 cos_loss: 0.992251455783844\n",
      "loss: 0.9756399393081665, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 104.96875, cls_loss: 1443481728.0 cos_loss: 0.9756399393081665\n",
      "loss: 0.9767141938209534, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 103.83148956298828, cls_loss: 1505393408.0 cos_loss: 0.9767141938209534\n",
      "loss: 0.9687814116477966, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 104.38864135742188, cls_loss: 1301311104.0 cos_loss: 0.9687814116477966\n",
      "loss: 0.9780707359313965, acc: 0.0, pred_acc: 0.7734375 gate_loss: 103.95828247070312, cls_loss: 1415171072.0 cos_loss: 0.9780707359313965\n",
      "loss: 0.9777171611785889, acc: 0.0, pred_acc: 0.7421875 gate_loss: 102.73047637939453, cls_loss: 1235916160.0 cos_loss: 0.9777171611785889\n",
      "loss: 0.9700067639350891, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 101.8125, cls_loss: 1281546496.0 cos_loss: 0.9700067639350891\n",
      "loss: 0.9818019866943359, acc: 0.0, pred_acc: 0.75390625 gate_loss: 102.79273986816406, cls_loss: 1327030656.0 cos_loss: 0.9818019866943359\n",
      "loss: 1.002264380455017, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 103.640625, cls_loss: 1455646464.0 cos_loss: 1.002264380455017\n",
      "loss: 0.962356686592102, acc: 0.0, pred_acc: 0.76953125 gate_loss: 103.11328125, cls_loss: 1305763072.0 cos_loss: 0.962356686592102\n",
      "loss: 0.9737379550933838, acc: 0.0078125, pred_acc: 0.73046875 gate_loss: 104.58592987060547, cls_loss: 1375865088.0 cos_loss: 0.9737379550933838\n",
      "loss: 0.9606508612632751, acc: 0.0078125, pred_acc: 0.82421875 gate_loss: 104.1256103515625, cls_loss: 1283590144.0 cos_loss: 0.9606508612632751\n",
      "loss: 1.0263340473175049, acc: 0.0078125, pred_acc: 0.73828125 gate_loss: 103.76546478271484, cls_loss: 1213046656.0 cos_loss: 1.0263340473175049\n",
      "loss: 0.9656035900115967, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 102.33592987060547, cls_loss: 1297267456.0 cos_loss: 0.9656035900115967\n",
      "loss: 0.9731638431549072, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 103.17578125, cls_loss: 1327476352.0 cos_loss: 0.9731638431549072\n",
      "loss: 0.9860038757324219, acc: 0.01171875, pred_acc: 0.76953125 gate_loss: 100.67578125, cls_loss: 1110190336.0 cos_loss: 0.9860038757324219\n",
      "loss: 0.9745357632637024, acc: 0.015625, pred_acc: 0.71875 gate_loss: 98.25, cls_loss: 1065905600.0 cos_loss: 0.9745357632637024\n",
      "loss: 0.9732314944267273, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 102.234375, cls_loss: 1233976576.0 cos_loss: 0.9732314944267273\n",
      "loss: 0.9242286086082458, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 99.89448547363281, cls_loss: 1112107392.0 cos_loss: 0.9242286086082458\n",
      "loss: 0.9772672653198242, acc: 0.00390625, pred_acc: 0.72265625 gate_loss: 106.77597045898438, cls_loss: 1172953088.0 cos_loss: 0.9772672653198242\n",
      "loss: 0.9593926668167114, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 112.80078125, cls_loss: 1322718592.0 cos_loss: 0.9593926668167114\n",
      "loss: 0.9861317276954651, acc: 0.00390625, pred_acc: 0.82421875 gate_loss: 117.31640625, cls_loss: 1338873088.0 cos_loss: 0.9861317276954651\n",
      "loss: 0.9658238887786865, acc: 0.0, pred_acc: 0.7421875 gate_loss: 118.88533020019531, cls_loss: 1299845888.0 cos_loss: 0.9658238887786865\n",
      "loss: 0.9470381736755371, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 123.77734375, cls_loss: 1574674688.0 cos_loss: 0.9470381736755371\n",
      "loss: 0.9647189378738403, acc: 0.0, pred_acc: 0.78125 gate_loss: 127.8125, cls_loss: 1370473984.0 cos_loss: 0.9647189378738403\n",
      "loss: 0.9753316640853882, acc: 0.00390625, pred_acc: 0.73046875 gate_loss: 129.8328857421875, cls_loss: 1462139648.0 cos_loss: 0.9753316640853882\n",
      "loss: 0.9728766679763794, acc: 0.0, pred_acc: 0.7265625 gate_loss: 137.7109375, cls_loss: 1585411072.0 cos_loss: 0.9728766679763794\n",
      "loss: 0.9901685118675232, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 140.18359375, cls_loss: 1649460480.0 cos_loss: 0.9901685118675232\n",
      "loss: 1.0000969171524048, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 143.82801818847656, cls_loss: 1830922624.0 cos_loss: 1.0000969171524048\n",
      "loss: 0.9768277406692505, acc: 0.0078125, pred_acc: 0.8046875 gate_loss: 148.859375, cls_loss: 1940033536.0 cos_loss: 0.9768277406692505\n",
      "loss: 0.9641589522361755, acc: 0.015625, pred_acc: 0.80078125 gate_loss: 147.1640625, cls_loss: 1840156416.0 cos_loss: 0.9641589522361755\n",
      "loss: 0.953887939453125, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 147.97265625, cls_loss: 2278701056.0 cos_loss: 0.953887939453125\n",
      "loss: 0.980841338634491, acc: 0.0, pred_acc: 0.73046875 gate_loss: 147.74269104003906, cls_loss: 2137288192.0 cos_loss: 0.980841338634491\n",
      "loss: 1.0120235681533813, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 148.23046875, cls_loss: 2116314112.0 cos_loss: 1.0120235681533813\n",
      "loss: 0.9364162683486938, acc: 0.01953125, pred_acc: 0.7734375 gate_loss: 148.09765625, cls_loss: 2402563328.0 cos_loss: 0.9364162683486938\n",
      "loss: 0.9562676548957825, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 144.5859375, cls_loss: 2174493440.0 cos_loss: 0.9562676548957825\n",
      "loss: 0.9602565765380859, acc: 0.015625, pred_acc: 0.7265625 gate_loss: 141.56640625, cls_loss: 2130426112.0 cos_loss: 0.9602565765380859\n",
      "loss: 0.9633915424346924, acc: 0.01171875, pred_acc: 0.77734375 gate_loss: 140.41796875, cls_loss: 2106560512.0 cos_loss: 0.9633915424346924\n",
      "loss: 0.9810237884521484, acc: 0.0078125, pred_acc: 0.75 gate_loss: 136.25, cls_loss: 2185572096.0 cos_loss: 0.9810237884521484\n",
      "loss: 1.0112271308898926, acc: 0.0, pred_acc: 0.7649999856948853 gate_loss: 134.71499633789062, cls_loss: 2114162816.0 cos_loss: 1.0112271308898926\n",
      "loss: 0.9325761795043945, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 133.42578125, cls_loss: 2114130048.0 cos_loss: 0.9325761795043945\n",
      "loss: 1.021706223487854, acc: 0.0, pred_acc: 0.765625 gate_loss: 129.06640625, cls_loss: 1917846144.0 cos_loss: 1.021706223487854\n",
      "loss: 0.9589036703109741, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 127.83622741699219, cls_loss: 2134852096.0 cos_loss: 0.9589036703109741\n",
      "loss: 0.9830800294876099, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 125.2421875, cls_loss: 1853641600.0 cos_loss: 0.9830800294876099\n",
      "loss: 0.9648305177688599, acc: 0.0, pred_acc: 0.80078125 gate_loss: 127.984375, cls_loss: 2183559424.0 cos_loss: 0.9648305177688599\n",
      "loss: 0.9811652898788452, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 127.046875, cls_loss: 1797493120.0 cos_loss: 0.9811652898788452\n",
      "loss: 0.992393434047699, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 126.7578125, cls_loss: 1956086400.0 cos_loss: 0.992393434047699\n",
      "loss: 0.9626355171203613, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 124.65625, cls_loss: 1812525056.0 cos_loss: 0.9626355171203613\n",
      "loss: 0.9773358106613159, acc: 0.01171875, pred_acc: 0.78125 gate_loss: 125.12554931640625, cls_loss: 1964079360.0 cos_loss: 0.9773358106613159\n",
      "loss: 0.9572912454605103, acc: 0.0, pred_acc: 0.7421875 gate_loss: 123.59375, cls_loss: 1948793344.0 cos_loss: 0.9572912454605103\n",
      "loss: 0.9280678629875183, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 123.4453125, cls_loss: 1970974976.0 cos_loss: 0.9280678629875183\n",
      "loss: 0.9508570432662964, acc: 0.01171875, pred_acc: 0.75390625 gate_loss: 121.35546875, cls_loss: 1844398592.0 cos_loss: 0.9508570432662964\n",
      "loss: 0.9778121709823608, acc: 0.0078125, pred_acc: 0.75 gate_loss: 122.52769470214844, cls_loss: 1850612480.0 cos_loss: 0.9778121709823608\n",
      "loss: 0.9736851453781128, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 117.04705810546875, cls_loss: 1790218240.0 cos_loss: 0.9736851453781128\n",
      "loss: 0.9866186380386353, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 115.2481460571289, cls_loss: 1686918016.0 cos_loss: 0.9866186380386353\n",
      "loss: 0.9958510994911194, acc: 0.0078125, pred_acc: 0.7890625 gate_loss: 116.28515625, cls_loss: 1813085824.0 cos_loss: 0.9958510994911194\n",
      "loss: 0.9732586145401001, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 112.89453125, cls_loss: 1562709504.0 cos_loss: 0.9732586145401001\n",
      "loss: 0.9758310317993164, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 114.640625, cls_loss: 1617263104.0 cos_loss: 0.9758310317993164\n",
      "loss: 0.9788352847099304, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 112.328125, cls_loss: 1648050816.0 cos_loss: 0.9788352847099304\n",
      "loss: 0.9963817000389099, acc: 0.01171875, pred_acc: 0.765625 gate_loss: 112.51953125, cls_loss: 1573336832.0 cos_loss: 0.9963817000389099\n",
      "loss: 0.9866279363632202, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 112.37701416015625, cls_loss: 1549907456.0 cos_loss: 0.9866279363632202\n",
      "loss: 0.9926407337188721, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 111.59375, cls_loss: 1662401152.0 cos_loss: 0.9926407337188721\n",
      "loss: 0.9528424143791199, acc: 0.0, pred_acc: 0.75390625 gate_loss: 112.32421875, cls_loss: 1669992192.0 cos_loss: 0.9528424143791199\n",
      "loss: 0.9674807786941528, acc: 0.0, pred_acc: 0.73828125 gate_loss: 111.06642150878906, cls_loss: 1665263488.0 cos_loss: 0.9674807786941528\n",
      "loss: 0.9761266708374023, acc: 0.00390625, pred_acc: 0.75 gate_loss: 110.96875, cls_loss: 1472514816.0 cos_loss: 0.9761266708374023\n",
      "loss: 0.9794192910194397, acc: 0.00390625, pred_acc: 0.7109375 gate_loss: 111.28125, cls_loss: 1415792384.0 cos_loss: 0.9794192910194397\n",
      "loss: 0.9479274749755859, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 112.78118896484375, cls_loss: 1553458688.0 cos_loss: 0.9479274749755859\n",
      "loss: 0.9827539920806885, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 110.94499969482422, cls_loss: 1451982592.0 cos_loss: 0.9827539920806885\n",
      "loss: 0.9708751440048218, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 114.46875, cls_loss: 1258826240.0 cos_loss: 0.9708751440048218\n",
      "loss: 0.9785412549972534, acc: 0.00390625, pred_acc: 0.8203125 gate_loss: 114.203125, cls_loss: 1474633344.0 cos_loss: 0.9785412549972534\n",
      "loss: 0.9585687518119812, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 113.80078125, cls_loss: 1350760448.0 cos_loss: 0.9585687518119812\n",
      "loss: 0.9599711894989014, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 114.51556396484375, cls_loss: 1444102528.0 cos_loss: 0.9599711894989014\n",
      "loss: 0.9483687877655029, acc: 0.01171875, pred_acc: 0.734375 gate_loss: 114.93719482421875, cls_loss: 1451417984.0 cos_loss: 0.9483687877655029\n",
      "loss: 0.9658593535423279, acc: 0.0, pred_acc: 0.78125 gate_loss: 117.23190307617188, cls_loss: 1361137664.0 cos_loss: 0.9658593535423279\n",
      "loss: 0.9696986675262451, acc: 0.00390625, pred_acc: 0.73828125 gate_loss: 114.51953125, cls_loss: 1296284544.0 cos_loss: 0.9696986675262451\n",
      "loss: 0.9519482254981995, acc: 0.0, pred_acc: 0.80078125 gate_loss: 114.48149871826172, cls_loss: 1328536832.0 cos_loss: 0.9519482254981995\n",
      "loss: 0.9465286731719971, acc: 0.0, pred_acc: 0.80859375 gate_loss: 120.1953125, cls_loss: 1391044864.0 cos_loss: 0.9465286731719971\n",
      "loss: 0.985047459602356, acc: 0.0, pred_acc: 0.765625 gate_loss: 123.11329650878906, cls_loss: 1428781696.0 cos_loss: 0.985047459602356\n",
      "loss: 0.9945464730262756, acc: 0.0, pred_acc: 0.73046875 gate_loss: 124.6796875, cls_loss: 1361935104.0 cos_loss: 0.9945464730262756\n",
      "loss: 0.9702144861221313, acc: 0.0, pred_acc: 0.7734375 gate_loss: 128.30078125, cls_loss: 1424827904.0 cos_loss: 0.9702144861221313\n",
      "loss: 0.9676942825317383, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 130.76953125, cls_loss: 1441823872.0 cos_loss: 0.9676942825317383\n",
      "loss: 0.965742826461792, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 132.984375, cls_loss: 1452292736.0 cos_loss: 0.965742826461792\n",
      "loss: 0.9589863419532776, acc: 0.0078125, pred_acc: 0.73828125 gate_loss: 132.29296875, cls_loss: 1595862400.0 cos_loss: 0.9589863419532776\n",
      "loss: 0.9485510587692261, acc: 0.015625, pred_acc: 0.78515625 gate_loss: 130.96875, cls_loss: 1566610560.0 cos_loss: 0.9485510587692261\n",
      "loss: 0.9750140905380249, acc: 0.0, pred_acc: 0.7421875 gate_loss: 129.0859375, cls_loss: 1412786176.0 cos_loss: 0.9750140905380249\n",
      "loss: 0.9550095796585083, acc: 0.015625, pred_acc: 0.79296875 gate_loss: 126.1640625, cls_loss: 1563699584.0 cos_loss: 0.9550095796585083\n",
      "loss: 0.9613712430000305, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 125.89453125, cls_loss: 1417293184.0 cos_loss: 0.9613712430000305\n",
      "loss: 0.9764496088027954, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 120.83182525634766, cls_loss: 1168657920.0 cos_loss: 0.9764496088027954\n",
      "loss: 0.9700788259506226, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 120.5390625, cls_loss: 1299190912.0 cos_loss: 0.9700788259506226\n",
      "loss: 0.9677519798278809, acc: 0.01171875, pred_acc: 0.7734375 gate_loss: 120.71792602539062, cls_loss: 1150516352.0 cos_loss: 0.9677519798278809\n",
      "loss: 0.9710820317268372, acc: 0.0078125, pred_acc: 0.75 gate_loss: 120.17189025878906, cls_loss: 1153708160.0 cos_loss: 0.9710820317268372\n",
      "loss: 0.938294529914856, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 124.69920349121094, cls_loss: 1357380352.0 cos_loss: 0.938294529914856\n",
      "loss: 0.9768593311309814, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 127.1953125, cls_loss: 1371866240.0 cos_loss: 0.9768593311309814\n",
      "loss: 0.9333546161651611, acc: 0.00390625, pred_acc: 0.703125 gate_loss: 130.68359375, cls_loss: 1636395776.0 cos_loss: 0.9333546161651611\n",
      "loss: 0.9554490447044373, acc: 0.015625, pred_acc: 0.78515625 gate_loss: 129.75390625, cls_loss: 1764834304.0 cos_loss: 0.9554490447044373\n",
      "loss: 0.9632291793823242, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 128.23150634765625, cls_loss: 1481614848.0 cos_loss: 0.9632291793823242\n",
      "loss: 0.9753984212875366, acc: 0.0, pred_acc: 0.75 gate_loss: 123.0, cls_loss: 1363240448.0 cos_loss: 0.9753984212875366\n",
      "loss: 0.9557833671569824, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 123.38671875, cls_loss: 1433312256.0 cos_loss: 0.9557833671569824\n",
      "loss: 0.9743025302886963, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 118.666259765625, cls_loss: 1108621184.0 cos_loss: 0.9743025302886963\n",
      "loss: 0.9618691205978394, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 112.234375, cls_loss: 1190795008.0 cos_loss: 0.9618691205978394\n",
      "loss: 0.9385569095611572, acc: 0.01171875, pred_acc: 0.75 gate_loss: 106.87890625, cls_loss: 1121434112.0 cos_loss: 0.9385569095611572\n",
      "loss: 0.9900757074356079, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 103.26171875, cls_loss: 1035993472.0 cos_loss: 0.9900757074356079\n",
      "loss: 0.9463763236999512, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 101.76171875, cls_loss: 907753344.0 cos_loss: 0.9463763236999512\n",
      "loss: 0.9505235552787781, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 99.65625, cls_loss: 1063126080.0 cos_loss: 0.9505235552787781\n",
      "loss: 0.9572234749794006, acc: 0.0, pred_acc: 0.74609375 gate_loss: 96.99217987060547, cls_loss: 935727232.0 cos_loss: 0.9572234749794006\n",
      "loss: 0.9830555319786072, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 96.08203125, cls_loss: 1011450944.0 cos_loss: 0.9830555319786072\n",
      "loss: 0.9465671181678772, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 97.51953125, cls_loss: 1096507648.0 cos_loss: 0.9465671181678772\n",
      "loss: 0.9779857993125916, acc: 0.0, pred_acc: 0.71875 gate_loss: 97.7734375, cls_loss: 1052520064.0 cos_loss: 0.9779857993125916\n",
      "loss: 0.9546451568603516, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 97.45304107666016, cls_loss: 1107324800.0 cos_loss: 0.9546451568603516\n",
      "loss: 0.9559926986694336, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 98.671875, cls_loss: 1012012032.0 cos_loss: 0.9559926986694336\n",
      "loss: 0.9677326679229736, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 97.00390625, cls_loss: 956487744.0 cos_loss: 0.9677326679229736\n",
      "loss: 0.9708276987075806, acc: 0.0, pred_acc: 0.69921875 gate_loss: 97.54296875, cls_loss: 906401280.0 cos_loss: 0.9708276987075806\n",
      "loss: 0.9711986780166626, acc: 0.0, pred_acc: 0.74609375 gate_loss: 95.54878234863281, cls_loss: 806098496.0 cos_loss: 0.9711986780166626\n",
      "loss: 0.9802924394607544, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 100.390625, cls_loss: 901863680.0 cos_loss: 0.9802924394607544\n",
      "loss: 0.958922266960144, acc: 0.01171875, pred_acc: 0.70703125 gate_loss: 101.328125, cls_loss: 831651648.0 cos_loss: 0.958922266960144\n",
      "loss: 0.954491913318634, acc: 0.0234375, pred_acc: 0.77734375 gate_loss: 102.69119262695312, cls_loss: 790745152.0 cos_loss: 0.954491913318634\n",
      "loss: 0.9463380575180054, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 105.9453125, cls_loss: 905934016.0 cos_loss: 0.9463380575180054\n",
      "loss: 0.979442298412323, acc: 0.0078125, pred_acc: 0.73046875 gate_loss: 106.59375, cls_loss: 827308288.0 cos_loss: 0.979442298412323\n",
      "loss: 0.9327086806297302, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 109.78125, cls_loss: 871124736.0 cos_loss: 0.9327086806297302\n",
      "loss: 0.9811780452728271, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 110.765625, cls_loss: 840732672.0 cos_loss: 0.9811780452728271\n",
      "loss: 0.9422546625137329, acc: 0.01171875, pred_acc: 0.77734375 gate_loss: 110.5703125, cls_loss: 865560960.0 cos_loss: 0.9422546625137329\n",
      "loss: 0.9442505836486816, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 110.58984375, cls_loss: 876720832.0 cos_loss: 0.9442505836486816\n",
      "loss: 0.9456331133842468, acc: 0.0, pred_acc: 0.80859375 gate_loss: 109.8671875, cls_loss: 812310272.0 cos_loss: 0.9456331133842468\n",
      "loss: 0.936408281326294, acc: 0.01171875, pred_acc: 0.7578125 gate_loss: 109.2890625, cls_loss: 847780224.0 cos_loss: 0.936408281326294\n",
      "loss: 0.9462440013885498, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 108.04296875, cls_loss: 745879680.0 cos_loss: 0.9462440013885498\n",
      "loss: 0.9566059708595276, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 106.06640625, cls_loss: 728874368.0 cos_loss: 0.9566059708595276\n",
      "loss: 0.9709728956222534, acc: 0.0078125, pred_acc: 0.71875 gate_loss: 105.1328125, cls_loss: 680593280.0 cos_loss: 0.9709728956222534\n",
      "loss: 0.9764835834503174, acc: 0.01171875, pred_acc: 0.796875 gate_loss: 104.6171875, cls_loss: 639201984.0 cos_loss: 0.9764835834503174\n",
      "loss: 0.9739375114440918, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 104.18359375, cls_loss: 634560000.0 cos_loss: 0.9739375114440918\n",
      "loss: 0.952333927154541, acc: 0.00390625, pred_acc: 0.8515625 gate_loss: 102.20700073242188, cls_loss: 629656640.0 cos_loss: 0.952333927154541\n",
      "loss: 0.9700509309768677, acc: 0.015625, pred_acc: 0.78515625 gate_loss: 104.50390625, cls_loss: 599062848.0 cos_loss: 0.9700509309768677\n",
      "loss: 0.9643422365188599, acc: 0.015625, pred_acc: 0.8046875 gate_loss: 103.89453125, cls_loss: 594915840.0 cos_loss: 0.9643422365188599\n",
      "loss: 0.9358145594596863, acc: 0.0078125, pred_acc: 0.73828125 gate_loss: 104.8515625, cls_loss: 623176448.0 cos_loss: 0.9358145594596863\n",
      "loss: 0.9396728277206421, acc: 0.0078125, pred_acc: 0.75 gate_loss: 103.94921875, cls_loss: 696305344.0 cos_loss: 0.9396728277206421\n",
      "loss: 0.9436671733856201, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 105.83984375, cls_loss: 550526272.0 cos_loss: 0.9436671733856201\n",
      "loss: 0.949729323387146, acc: 0.0, pred_acc: 0.76953125 gate_loss: 106.98887634277344, cls_loss: 584837248.0 cos_loss: 0.949729323387146\n",
      "loss: 0.9783316850662231, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 106.95703125, cls_loss: 560292288.0 cos_loss: 0.9783316850662231\n",
      "loss: 0.9336884021759033, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 107.67578125, cls_loss: 551356928.0 cos_loss: 0.9336884021759033\n",
      "loss: 0.964574933052063, acc: 0.01953125, pred_acc: 0.765625 gate_loss: 106.45703125, cls_loss: 597807808.0 cos_loss: 0.964574933052063\n",
      "loss: 0.9292072653770447, acc: 0.01171875, pred_acc: 0.74609375 gate_loss: 105.80854034423828, cls_loss: 506047008.0 cos_loss: 0.9292072653770447\n",
      "loss: 0.956332266330719, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 104.68498229980469, cls_loss: 508955616.0 cos_loss: 0.956332266330719\n",
      "loss: 0.9135101437568665, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 104.07421875, cls_loss: 537046464.0 cos_loss: 0.9135101437568665\n",
      "loss: 0.9558244943618774, acc: 0.015625, pred_acc: 0.76171875 gate_loss: 103.05493927001953, cls_loss: 491168192.0 cos_loss: 0.9558244943618774\n",
      "loss: 0.9455327391624451, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 101.9140625, cls_loss: 532205504.0 cos_loss: 0.9455327391624451\n",
      "loss: 0.9607166051864624, acc: 0.0078125, pred_acc: 0.75 gate_loss: 102.76953125, cls_loss: 485670400.0 cos_loss: 0.9607166051864624\n",
      "loss: 0.9508007764816284, acc: 0.01171875, pred_acc: 0.7265625 gate_loss: 100.984375, cls_loss: 562656064.0 cos_loss: 0.9508007764816284\n",
      "loss: 0.9465241432189941, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 98.06255340576172, cls_loss: 480214336.0 cos_loss: 0.9465241432189941\n",
      "loss: 0.9226531982421875, acc: 0.0, pred_acc: 0.73828125 gate_loss: 99.2344970703125, cls_loss: 458309120.0 cos_loss: 0.9226531982421875\n",
      "loss: 0.9545753002166748, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 97.4453125, cls_loss: 410089440.0 cos_loss: 0.9545753002166748\n",
      "loss: 0.9086041450500488, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 98.43940734863281, cls_loss: 464893824.0 cos_loss: 0.9086041450500488\n",
      "loss: 0.950089693069458, acc: 0.01953125, pred_acc: 0.7734375 gate_loss: 98.0078125, cls_loss: 464258496.0 cos_loss: 0.950089693069458\n",
      "loss: 0.9349664449691772, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 98.8114013671875, cls_loss: 498449632.0 cos_loss: 0.9349664449691772\n",
      "loss: 0.9552087783813477, acc: 0.0, pred_acc: 0.73046875 gate_loss: 98.72265625, cls_loss: 504381088.0 cos_loss: 0.9552087783813477\n",
      "loss: 0.9349620342254639, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 98.30859375, cls_loss: 528666304.0 cos_loss: 0.9349620342254639\n",
      "loss: 0.9548420906066895, acc: 0.01953125, pred_acc: 0.734375 gate_loss: 98.73826599121094, cls_loss: 478031360.0 cos_loss: 0.9548420906066895\n",
      "loss: 0.9452451467514038, acc: 0.0234375, pred_acc: 0.734375 gate_loss: 97.62890625, cls_loss: 455394976.0 cos_loss: 0.9452451467514038\n",
      "loss: 0.9407187104225159, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 98.2421646118164, cls_loss: 459501280.0 cos_loss: 0.9407187104225159\n",
      "loss: 0.9299617409706116, acc: 0.015625, pred_acc: 0.7578125 gate_loss: 98.2578125, cls_loss: 510927360.0 cos_loss: 0.9299617409706116\n",
      "loss: 0.9369527101516724, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 98.9609375, cls_loss: 469083616.0 cos_loss: 0.9369527101516724\n",
      "loss: 0.9749313592910767, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 97.34375, cls_loss: 444694976.0 cos_loss: 0.9749313592910767\n",
      "loss: 0.9463843703269958, acc: 0.01171875, pred_acc: 0.7890625 gate_loss: 99.29904174804688, cls_loss: 473962528.0 cos_loss: 0.9463843703269958\n",
      "loss: 0.9538936614990234, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 103.2265625, cls_loss: 482669184.0 cos_loss: 0.9538936614990234\n",
      "loss: 0.9435592889785767, acc: 0.0, pred_acc: 0.71484375 gate_loss: 107.48046875, cls_loss: 520634816.0 cos_loss: 0.9435592889785767\n",
      "loss: 0.9136185646057129, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 109.69140625, cls_loss: 471368672.0 cos_loss: 0.9136185646057129\n",
      "loss: 0.9510408043861389, acc: 0.00390625, pred_acc: 0.79296875 gate_loss: 111.1953125, cls_loss: 540801472.0 cos_loss: 0.9510408043861389\n",
      "loss: 0.9382548928260803, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 112.0546875, cls_loss: 568614400.0 cos_loss: 0.9382548928260803\n",
      "loss: 0.9352598786354065, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 112.21482849121094, cls_loss: 559339904.0 cos_loss: 0.9352598786354065\n",
      "loss: 0.9620461463928223, acc: 0.0, pred_acc: 0.78125 gate_loss: 112.855224609375, cls_loss: 564667264.0 cos_loss: 0.9620461463928223\n",
      "loss: 0.9300327301025391, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 113.734375, cls_loss: 536499744.0 cos_loss: 0.9300327301025391\n",
      "loss: 0.9589346647262573, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 114.01171875, cls_loss: 549090304.0 cos_loss: 0.9589346647262573\n",
      "loss: 0.9455738067626953, acc: 0.01953125, pred_acc: 0.7265625 gate_loss: 111.84375, cls_loss: 512587040.0 cos_loss: 0.9455738067626953\n",
      "loss: 0.9238715171813965, acc: 0.0, pred_acc: 0.7734375 gate_loss: 113.125, cls_loss: 500503648.0 cos_loss: 0.9238715171813965\n",
      "loss: 0.9354218244552612, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 110.83984375, cls_loss: 485698048.0 cos_loss: 0.9354218244552612\n",
      "loss: 0.9304969310760498, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 111.4375, cls_loss: 446203936.0 cos_loss: 0.9304969310760498\n",
      "loss: 0.9339102506637573, acc: 0.00390625, pred_acc: 0.81640625 gate_loss: 110.72265625, cls_loss: 459598432.0 cos_loss: 0.9339102506637573\n",
      "loss: 0.9333406686782837, acc: 0.0078125, pred_acc: 0.78125 gate_loss: 109.86556243896484, cls_loss: 465448000.0 cos_loss: 0.9333406686782837\n",
      "loss: 0.9232310652732849, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 110.75210571289062, cls_loss: 473028000.0 cos_loss: 0.9232310652732849\n",
      "loss: 0.9533985257148743, acc: 0.015625, pred_acc: 0.75390625 gate_loss: 109.0, cls_loss: 421275168.0 cos_loss: 0.9533985257148743\n",
      "loss: 0.927324652671814, acc: 0.0, pred_acc: 0.80859375 gate_loss: 107.5079116821289, cls_loss: 551439296.0 cos_loss: 0.927324652671814\n",
      "loss: 0.9175702929496765, acc: 0.015625, pred_acc: 0.8125 gate_loss: 104.73828125, cls_loss: 477412000.0 cos_loss: 0.9175702929496765\n",
      "loss: 0.9606689214706421, acc: 0.0, pred_acc: 0.796875 gate_loss: 105.3239974975586, cls_loss: 403586400.0 cos_loss: 0.9606689214706421\n",
      "loss: 0.9383136034011841, acc: 0.01171875, pred_acc: 0.77734375 gate_loss: 104.7265625, cls_loss: 407912352.0 cos_loss: 0.9383136034011841\n",
      "loss: 0.9442691802978516, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 105.65913391113281, cls_loss: 397052864.0 cos_loss: 0.9442691802978516\n",
      "loss: 0.907492458820343, acc: 0.0, pred_acc: 0.8203125 gate_loss: 106.75390625, cls_loss: 475747488.0 cos_loss: 0.907492458820343\n",
      "loss: 0.9008116722106934, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 108.1903305053711, cls_loss: 451179968.0 cos_loss: 0.9008116722106934\n",
      "loss: 0.9462247490882874, acc: 0.01171875, pred_acc: 0.74609375 gate_loss: 118.49609375, cls_loss: 463418624.0 cos_loss: 0.9462247490882874\n",
      "loss: 0.9480828046798706, acc: 0.01171875, pred_acc: 0.7421875 gate_loss: 127.8515625, cls_loss: 504489856.0 cos_loss: 0.9480828046798706\n",
      "loss: 0.9288415908813477, acc: 0.00390625, pred_acc: 0.75 gate_loss: 132.44921875, cls_loss: 592762176.0 cos_loss: 0.9288415908813477\n",
      "loss: 0.9177964329719543, acc: 0.0, pred_acc: 0.765625 gate_loss: 138.64659118652344, cls_loss: 533900448.0 cos_loss: 0.9177964329719543\n",
      "loss: 0.9574963450431824, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 140.04296875, cls_loss: 525095456.0 cos_loss: 0.9574963450431824\n",
      "loss: 0.950603723526001, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 140.35546875, cls_loss: 458068928.0 cos_loss: 0.950603723526001\n",
      "loss: 0.9197859764099121, acc: 0.00390625, pred_acc: 0.765625 gate_loss: 134.0, cls_loss: 402944736.0 cos_loss: 0.9197859764099121\n",
      "loss: 0.9199663400650024, acc: 0.0078125, pred_acc: 0.6953125 gate_loss: 127.96875, cls_loss: 391150944.0 cos_loss: 0.9199663400650024\n",
      "loss: 0.9771087169647217, acc: 0.01171875, pred_acc: 0.78125 gate_loss: 119.19387817382812, cls_loss: 316729216.0 cos_loss: 0.9771087169647217\n",
      "loss: 0.942608118057251, acc: 0.01171875, pred_acc: 0.80078125 gate_loss: 113.0234375, cls_loss: 310109184.0 cos_loss: 0.942608118057251\n",
      "loss: 0.9510067105293274, acc: 0.01953125, pred_acc: 0.703125 gate_loss: 109.79296875, cls_loss: 366824064.0 cos_loss: 0.9510067105293274\n",
      "loss: 0.9764188528060913, acc: 0.00390625, pred_acc: 0.69921875 gate_loss: 102.44137573242188, cls_loss: 276181472.0 cos_loss: 0.9764188528060913\n",
      "loss: 0.9414783716201782, acc: 0.015625, pred_acc: 0.76953125 gate_loss: 99.19140625, cls_loss: 281945984.0 cos_loss: 0.9414783716201782\n",
      "loss: 0.9446250200271606, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 94.88874816894531, cls_loss: 281914880.0 cos_loss: 0.9446250200271606\n",
      "loss: 0.9418137073516846, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 95.91796875, cls_loss: 315119072.0 cos_loss: 0.9418137073516846\n",
      "loss: 0.9567186832427979, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 94.33984375, cls_loss: 419434080.0 cos_loss: 0.9567186832427979\n",
      "loss: 0.9699860215187073, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 95.62922668457031, cls_loss: 505159168.0 cos_loss: 0.9699860215187073\n",
      "loss: 0.9542986154556274, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 94.55865478515625, cls_loss: 593113088.0 cos_loss: 0.9542986154556274\n",
      "loss: 0.9450505375862122, acc: 0.015625, pred_acc: 0.76953125 gate_loss: 96.73046875, cls_loss: 678484928.0 cos_loss: 0.9450505375862122\n",
      "loss: 0.9732718467712402, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 99.78939819335938, cls_loss: 703388864.0 cos_loss: 0.9732718467712402\n",
      "loss: 0.9580273628234863, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 92.9453125, cls_loss: 807508800.0 cos_loss: 0.9580273628234863\n",
      "loss: 0.9625424146652222, acc: 0.0, pred_acc: 0.72265625 gate_loss: 88.87890625, cls_loss: 742949888.0 cos_loss: 0.9625424146652222\n",
      "loss: 0.9599591493606567, acc: 0.01171875, pred_acc: 0.8125 gate_loss: 84.6015625, cls_loss: 848885760.0 cos_loss: 0.9599591493606567\n",
      "loss: 1.0257288217544556, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 84.64453125, cls_loss: 830425216.0 cos_loss: 1.0257288217544556\n",
      "loss: 0.9592247009277344, acc: 0.015625, pred_acc: 0.77734375 gate_loss: 77.734375, cls_loss: 872061056.0 cos_loss: 0.9592247009277344\n",
      "loss: 0.9699863195419312, acc: 0.0078125, pred_acc: 0.7890625 gate_loss: 76.453125, cls_loss: 865044864.0 cos_loss: 0.9699863195419312\n",
      "loss: 0.9826736450195312, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 73.46484375, cls_loss: 944080256.0 cos_loss: 0.9826736450195312\n",
      "loss: 0.9920421838760376, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 72.484375, cls_loss: 945091584.0 cos_loss: 0.9920421838760376\n",
      "loss: 0.9852858781814575, acc: 0.0, pred_acc: 0.75390625 gate_loss: 68.93359375, cls_loss: 791184768.0 cos_loss: 0.9852858781814575\n",
      "loss: 0.9841755628585815, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 66.703125, cls_loss: 935494208.0 cos_loss: 0.9841755628585815\n",
      "loss: 0.9259909391403198, acc: 0.004999999888241291, pred_acc: 0.7849999666213989 gate_loss: 68.30451965332031, cls_loss: 945684096.0 cos_loss: 0.9259909391403198\n",
      "loss: 0.9491063356399536, acc: 0.0078125, pred_acc: 0.80078125 gate_loss: 93.0859375, cls_loss: 1884886656.0 cos_loss: 0.9491063356399536\n",
      "loss: 0.9900778532028198, acc: 0.01171875, pred_acc: 0.79296875 gate_loss: 108.24221801757812, cls_loss: 2980545536.0 cos_loss: 0.9900778532028198\n",
      "loss: 0.9741305708885193, acc: 0.0, pred_acc: 0.75 gate_loss: 119.64453125, cls_loss: 5292996608.0 cos_loss: 0.9741305708885193\n",
      "loss: 1.0491046905517578, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 121.828125, cls_loss: 6355533824.0 cos_loss: 1.0491046905517578\n",
      "loss: 1.064021348953247, acc: 0.01171875, pred_acc: 0.75 gate_loss: 121.59375, cls_loss: 10219900928.0 cos_loss: 1.064021348953247\n",
      "loss: 1.0559972524642944, acc: 0.00390625, pred_acc: 0.75 gate_loss: 117.875, cls_loss: 9098772480.0 cos_loss: 1.0559972524642944\n",
      "loss: 1.040343165397644, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 109.046875, cls_loss: 11265547264.0 cos_loss: 1.040343165397644\n",
      "loss: 1.0378236770629883, acc: 0.0, pred_acc: 0.734375 gate_loss: 98.63671875, cls_loss: 13686723584.0 cos_loss: 1.0378236770629883\n",
      "loss: 1.0514752864837646, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 87.15232849121094, cls_loss: 12883436544.0 cos_loss: 1.0514752864837646\n",
      "loss: 1.0582871437072754, acc: 0.0078125, pred_acc: 0.8046875 gate_loss: 72.70703125, cls_loss: 10335782912.0 cos_loss: 1.0582871437072754\n",
      "loss: 1.0704095363616943, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 59.734375, cls_loss: 10348818432.0 cos_loss: 1.0704095363616943\n",
      "loss: 1.0630582571029663, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 49.94921875, cls_loss: 12479214592.0 cos_loss: 1.0630582571029663\n",
      "loss: 1.0831191539764404, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 41.55078125, cls_loss: 15968454656.0 cos_loss: 1.0831191539764404\n",
      "loss: 1.1236296892166138, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 36.30078125, cls_loss: 20569853952.0 cos_loss: 1.1236296892166138\n",
      "loss: 1.1241779327392578, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 29.484375, cls_loss: 14517004288.0 cos_loss: 1.1241779327392578\n",
      "loss: 1.127118468284607, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 27.453125, cls_loss: 18484412416.0 cos_loss: 1.127118468284607\n",
      "loss: 1.1373869180679321, acc: 0.00390625, pred_acc: 0.79296875 gate_loss: 25.6328125, cls_loss: 15447819264.0 cos_loss: 1.1373869180679321\n",
      "loss: 1.1600888967514038, acc: 0.00390625, pred_acc: 0.81640625 gate_loss: 21.21484375, cls_loss: 12885585920.0 cos_loss: 1.1600888967514038\n",
      "loss: 1.1734037399291992, acc: 0.0078125, pred_acc: 0.74609375 gate_loss: 18.5, cls_loss: 19733139456.0 cos_loss: 1.1734037399291992\n",
      "loss: 1.140576720237732, acc: 0.0, pred_acc: 0.76953125 gate_loss: 17.78125, cls_loss: 15668963328.0 cos_loss: 1.140576720237732\n",
      "loss: 1.1332769393920898, acc: 0.0, pred_acc: 0.78125 gate_loss: 19.8515625, cls_loss: 23971612672.0 cos_loss: 1.1332769393920898\n",
      "loss: 1.1314756870269775, acc: 0.0, pred_acc: 0.7578125 gate_loss: 19.671875, cls_loss: 16985582592.0 cos_loss: 1.1314756870269775\n",
      "loss: 1.1177351474761963, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 16.859375, cls_loss: 17256607744.0 cos_loss: 1.1177351474761963\n",
      "loss: 1.1675832271575928, acc: 0.0, pred_acc: 0.765625 gate_loss: 17.1015625, cls_loss: 23895887872.0 cos_loss: 1.1675832271575928\n",
      "loss: 1.1022508144378662, acc: 0.0, pred_acc: 0.7421875 gate_loss: 18.9140625, cls_loss: 28244404224.0 cos_loss: 1.1022508144378662\n",
      "loss: 1.1121599674224854, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 17.76171875, cls_loss: 26895001600.0 cos_loss: 1.1121599674224854\n",
      "loss: 1.0767427682876587, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 22.44140625, cls_loss: 36608679936.0 cos_loss: 1.0767427682876587\n",
      "loss: 1.085458755493164, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 24.359375, cls_loss: 43806359552.0 cos_loss: 1.085458755493164\n",
      "loss: 1.0820810794830322, acc: 0.0, pred_acc: 0.7890625 gate_loss: 25.00390625, cls_loss: 48577449984.0 cos_loss: 1.0820810794830322\n",
      "loss: 1.0654840469360352, acc: 0.0, pred_acc: 0.7578125 gate_loss: 25.53125, cls_loss: 52736442368.0 cos_loss: 1.0654840469360352\n",
      "loss: 1.0794049501419067, acc: 0.0, pred_acc: 0.75390625 gate_loss: 27.421875, cls_loss: 60768366592.0 cos_loss: 1.0794049501419067\n",
      "loss: 1.0815402269363403, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 29.73046875, cls_loss: 78642176000.0 cos_loss: 1.0815402269363403\n",
      "loss: 1.1038646697998047, acc: 0.0, pred_acc: 0.7265625 gate_loss: 32.27734375, cls_loss: 81291124736.0 cos_loss: 1.1038646697998047\n",
      "loss: 1.070578694343567, acc: 0.0, pred_acc: 0.73828125 gate_loss: 34.48828125, cls_loss: 100441563136.0 cos_loss: 1.070578694343567\n",
      "loss: 1.1111106872558594, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 38.625, cls_loss: 107004723200.0 cos_loss: 1.1111106872558594\n",
      "loss: 1.1114716529846191, acc: 0.0, pred_acc: 0.73828125 gate_loss: 39.7890625, cls_loss: 98639577088.0 cos_loss: 1.1114716529846191\n",
      "loss: 1.0666579008102417, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 39.1484375, cls_loss: 127678783488.0 cos_loss: 1.0666579008102417\n",
      "loss: 1.0990357398986816, acc: 0.0, pred_acc: 0.76953125 gate_loss: 43.08984375, cls_loss: 123940200448.0 cos_loss: 1.0990357398986816\n",
      "loss: 1.081606149673462, acc: 0.0, pred_acc: 0.76171875 gate_loss: 40.3515625, cls_loss: 139782815744.0 cos_loss: 1.081606149673462\n",
      "loss: 1.0775518417358398, acc: 0.0, pred_acc: 0.7578125 gate_loss: 43.0, cls_loss: 133878423552.0 cos_loss: 1.0775518417358398\n",
      "loss: 1.0504908561706543, acc: 0.00390625, pred_acc: 0.76171875 gate_loss: 44.46484375, cls_loss: 154527465472.0 cos_loss: 1.0504908561706543\n",
      "loss: 1.0960849523544312, acc: 0.0, pred_acc: 0.7421875 gate_loss: 44.875, cls_loss: 134743269376.0 cos_loss: 1.0960849523544312\n",
      "loss: 1.0468313694000244, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 46.15625, cls_loss: 136562688000.0 cos_loss: 1.0468313694000244\n",
      "loss: 1.0972926616668701, acc: 0.0, pred_acc: 0.77734375 gate_loss: 48.6796875, cls_loss: 150596993024.0 cos_loss: 1.0972926616668701\n",
      "loss: 1.060670256614685, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 47.88671875, cls_loss: 135999758336.0 cos_loss: 1.060670256614685\n",
      "loss: 1.0793263912200928, acc: 0.0, pred_acc: 0.78515625 gate_loss: 50.3828125, cls_loss: 135707140096.0 cos_loss: 1.0793263912200928\n",
      "loss: 1.0901545286178589, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 50.5078125, cls_loss: 127461203968.0 cos_loss: 1.0901545286178589\n",
      "loss: 1.0648560523986816, acc: 0.00390625, pred_acc: 0.75 gate_loss: 49.7109375, cls_loss: 120985829376.0 cos_loss: 1.0648560523986816\n",
      "loss: 1.0591468811035156, acc: 0.01171875, pred_acc: 0.78515625 gate_loss: 51.16796875, cls_loss: 117645352960.0 cos_loss: 1.0591468811035156\n",
      "loss: 1.0691449642181396, acc: 0.0, pred_acc: 0.79296875 gate_loss: 46.33984375, cls_loss: 97095811072.0 cos_loss: 1.0691449642181396\n",
      "loss: 1.0553078651428223, acc: 0.0, pred_acc: 0.76953125 gate_loss: 49.02734375, cls_loss: 97414217728.0 cos_loss: 1.0553078651428223\n",
      "loss: 1.1050283908843994, acc: 0.0, pred_acc: 0.77734375 gate_loss: 46.59765625, cls_loss: 78355693568.0 cos_loss: 1.1050283908843994\n",
      "loss: 1.0337105989456177, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 45.80975341796875, cls_loss: 86048464896.0 cos_loss: 1.0337105989456177\n",
      "loss: 1.1063809394836426, acc: 0.0, pred_acc: 0.7578125 gate_loss: 46.7503662109375, cls_loss: 68195962880.0 cos_loss: 1.1063809394836426\n",
      "loss: 1.1008647680282593, acc: 0.00390625, pred_acc: 0.76953125 gate_loss: 49.44921875, cls_loss: 81601798144.0 cos_loss: 1.1008647680282593\n",
      "loss: 1.0945146083831787, acc: 0.0, pred_acc: 0.79296875 gate_loss: 45.765625, cls_loss: 67797151744.0 cos_loss: 1.0945146083831787\n",
      "loss: 1.0505574941635132, acc: 0.01171875, pred_acc: 0.7578125 gate_loss: 44.98046875, cls_loss: 66510557184.0 cos_loss: 1.0505574941635132\n",
      "loss: 1.0256762504577637, acc: 0.0, pred_acc: 0.73828125 gate_loss: 44.421875, cls_loss: 64738955264.0 cos_loss: 1.0256762504577637\n",
      "loss: 1.0256869792938232, acc: 0.0, pred_acc: 0.78125 gate_loss: 47.05859375, cls_loss: 59954397184.0 cos_loss: 1.0256869792938232\n",
      "loss: 1.0671331882476807, acc: 0.00390625, pred_acc: 0.73046875 gate_loss: 43.109375, cls_loss: 48871354368.0 cos_loss: 1.0671331882476807\n",
      "loss: 1.0474339723587036, acc: 0.0, pred_acc: 0.7734375 gate_loss: 41.3671875, cls_loss: 44190887936.0 cos_loss: 1.0474339723587036\n",
      "loss: 1.0517818927764893, acc: 0.0, pred_acc: 0.76171875 gate_loss: 41.2109375, cls_loss: 42695651328.0 cos_loss: 1.0517818927764893\n",
      "loss: 1.0352431535720825, acc: 0.0, pred_acc: 0.765625 gate_loss: 42.48046875, cls_loss: 39774547968.0 cos_loss: 1.0352431535720825\n",
      "loss: 1.049008846282959, acc: 0.0, pred_acc: 0.78125 gate_loss: 38.36328125, cls_loss: 33077024768.0 cos_loss: 1.049008846282959\n",
      "loss: 1.0480904579162598, acc: 0.00390625, pred_acc: 0.7421875 gate_loss: 39.421875, cls_loss: 32925038592.0 cos_loss: 1.0480904579162598\n",
      "loss: 1.069930076599121, acc: 0.00390625, pred_acc: 0.75 gate_loss: 38.65625, cls_loss: 27606325248.0 cos_loss: 1.069930076599121\n",
      "loss: 1.0246928930282593, acc: 0.00390625, pred_acc: 0.72265625 gate_loss: 37.0859375, cls_loss: 27329445888.0 cos_loss: 1.0246928930282593\n",
      "loss: 1.0548005104064941, acc: 0.0078125, pred_acc: 0.75390625 gate_loss: 39.5078125, cls_loss: 23974166528.0 cos_loss: 1.0548005104064941\n",
      "loss: 1.051695466041565, acc: 0.01171875, pred_acc: 0.765625 gate_loss: 37.546875, cls_loss: 22001137664.0 cos_loss: 1.051695466041565\n",
      "loss: 1.020855188369751, acc: 0.0, pred_acc: 0.76953125 gate_loss: 39.0390625, cls_loss: 21651707904.0 cos_loss: 1.020855188369751\n",
      "loss: 1.0279731750488281, acc: 0.0, pred_acc: 0.78515625 gate_loss: 40.6796875, cls_loss: 22771544064.0 cos_loss: 1.0279731750488281\n",
      "loss: 1.0214558839797974, acc: 0.0, pred_acc: 0.7578125 gate_loss: 41.51171875, cls_loss: 20295139328.0 cos_loss: 1.0214558839797974\n",
      "loss: 1.0361202955245972, acc: 0.0, pred_acc: 0.78125 gate_loss: 45.359375, cls_loss: 20284725248.0 cos_loss: 1.0361202955245972\n",
      "loss: 1.0183384418487549, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 46.38671875, cls_loss: 22861268992.0 cos_loss: 1.0183384418487549\n",
      "loss: 1.0454275608062744, acc: 0.00390625, pred_acc: 0.7265625 gate_loss: 49.2265625, cls_loss: 24399138816.0 cos_loss: 1.0454275608062744\n",
      "loss: 1.0430312156677246, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 51.53515625, cls_loss: 23569356800.0 cos_loss: 1.0430312156677246\n",
      "loss: 1.020385980606079, acc: 0.0, pred_acc: 0.75 gate_loss: 52.72265625, cls_loss: 23421069312.0 cos_loss: 1.020385980606079\n",
      "loss: 1.0015939474105835, acc: 0.0, pred_acc: 0.765625 gate_loss: 56.11248016357422, cls_loss: 26583429120.0 cos_loss: 1.0015939474105835\n",
      "loss: 0.9771438837051392, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 70.6875, cls_loss: 27213168640.0 cos_loss: 0.9771438837051392\n",
      "loss: 1.0060415267944336, acc: 0.01171875, pred_acc: 0.7734375 gate_loss: 86.765625, cls_loss: 33426432000.0 cos_loss: 1.0060415267944336\n",
      "loss: 0.9732240438461304, acc: 0.0, pred_acc: 0.78125 gate_loss: 97.96875, cls_loss: 36425183232.0 cos_loss: 0.9732240438461304\n",
      "loss: 0.9786903858184814, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 107.6328125, cls_loss: 40772603904.0 cos_loss: 0.9786903858184814\n",
      "loss: 0.9684277772903442, acc: 0.0, pred_acc: 0.78125 gate_loss: 112.64453125, cls_loss: 46664896512.0 cos_loss: 0.9684277772903442\n",
      "loss: 0.976372480392456, acc: 0.0, pred_acc: 0.76171875 gate_loss: 117.6484375, cls_loss: 52571504640.0 cos_loss: 0.976372480392456\n",
      "loss: 0.986515998840332, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 117.83206939697266, cls_loss: 52796469248.0 cos_loss: 0.986515998840332\n",
      "loss: 0.989280104637146, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 116.56640625, cls_loss: 48045015040.0 cos_loss: 0.989280104637146\n",
      "loss: 0.9720743298530579, acc: 0.00390625, pred_acc: 0.8046875 gate_loss: 116.22401428222656, cls_loss: 50630078464.0 cos_loss: 0.9720743298530579\n",
      "loss: 0.9986428022384644, acc: 0.015625, pred_acc: 0.74609375 gate_loss: 112.796875, cls_loss: 49423851520.0 cos_loss: 0.9986428022384644\n",
      "loss: 0.978969931602478, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 109.82421875, cls_loss: 47411474432.0 cos_loss: 0.978969931602478\n",
      "loss: 0.9829372763633728, acc: 0.0, pred_acc: 0.765625 gate_loss: 105.30859375, cls_loss: 44160589824.0 cos_loss: 0.9829372763633728\n",
      "loss: 0.9781733155250549, acc: 0.00390625, pred_acc: 0.72265625 gate_loss: 101.69921875, cls_loss: 39147077632.0 cos_loss: 0.9781733155250549\n",
      "loss: 0.9832435846328735, acc: 0.0, pred_acc: 0.76953125 gate_loss: 99.015625, cls_loss: 41803378688.0 cos_loss: 0.9832435846328735\n",
      "loss: 0.9780675768852234, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 95.57421875, cls_loss: 41034960896.0 cos_loss: 0.9780675768852234\n",
      "loss: 0.9411336779594421, acc: 0.00390625, pred_acc: 0.8046875 gate_loss: 90.71875, cls_loss: 43166793728.0 cos_loss: 0.9411336779594421\n",
      "loss: 0.9819298386573792, acc: 0.0078125, pred_acc: 0.7578125 gate_loss: 90.70703125, cls_loss: 40241729536.0 cos_loss: 0.9819298386573792\n",
      "loss: 0.9632033109664917, acc: 0.0078125, pred_acc: 0.79296875 gate_loss: 88.8984375, cls_loss: 37903122432.0 cos_loss: 0.9632033109664917\n",
      "loss: 0.9870859384536743, acc: 0.00390625, pred_acc: 0.6953125 gate_loss: 86.72265625, cls_loss: 40273727488.0 cos_loss: 0.9870859384536743\n",
      "loss: 0.968445897102356, acc: 0.01171875, pred_acc: 0.734375 gate_loss: 85.67578125, cls_loss: 38242603008.0 cos_loss: 0.968445897102356\n",
      "loss: 0.958293080329895, acc: 0.0078125, pred_acc: 0.7734375 gate_loss: 85.21484375, cls_loss: 35464585216.0 cos_loss: 0.958293080329895\n",
      "loss: 0.9852306842803955, acc: 0.0, pred_acc: 0.75 gate_loss: 86.00390625, cls_loss: 34512224256.0 cos_loss: 0.9852306842803955\n",
      "loss: 0.9684174060821533, acc: 0.01171875, pred_acc: 0.76953125 gate_loss: 86.03125, cls_loss: 36392615936.0 cos_loss: 0.9684174060821533\n",
      "loss: 0.9604453444480896, acc: 0.00390625, pred_acc: 0.80078125 gate_loss: 86.609375, cls_loss: 38900686848.0 cos_loss: 0.9604453444480896\n",
      "loss: 0.9739587903022766, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 86.203125, cls_loss: 35156905984.0 cos_loss: 0.9739587903022766\n",
      "loss: 0.9808543920516968, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 87.17578125, cls_loss: 33889193984.0 cos_loss: 0.9808543920516968\n",
      "loss: 0.9631212949752808, acc: 0.01171875, pred_acc: 0.75390625 gate_loss: 88.578125, cls_loss: 35129360384.0 cos_loss: 0.9631212949752808\n",
      "loss: 0.961405873298645, acc: 0.0078125, pred_acc: 0.71484375 gate_loss: 90.26953125, cls_loss: 32173006848.0 cos_loss: 0.961405873298645\n",
      "loss: 0.969173789024353, acc: 0.0078125, pred_acc: 0.765625 gate_loss: 90.19140625, cls_loss: 38683885568.0 cos_loss: 0.969173789024353\n",
      "loss: 0.9425404071807861, acc: 0.0078125, pred_acc: 0.796875 gate_loss: 93.75, cls_loss: 36976828416.0 cos_loss: 0.9425404071807861\n",
      "loss: 0.9590058326721191, acc: 0.01171875, pred_acc: 0.80859375 gate_loss: 94.890625, cls_loss: 41027575808.0 cos_loss: 0.9590058326721191\n",
      "loss: 0.9473127722740173, acc: 0.0, pred_acc: 0.765625 gate_loss: 96.27734375, cls_loss: 42412064768.0 cos_loss: 0.9473127722740173\n",
      "loss: 0.9530960917472839, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 98.70703125, cls_loss: 40292528128.0 cos_loss: 0.9530960917472839\n",
      "loss: 0.9684898257255554, acc: 0.0078125, pred_acc: 0.73046875 gate_loss: 99.20703125, cls_loss: 37899481088.0 cos_loss: 0.9684898257255554\n",
      "loss: 0.9508485794067383, acc: 0.00390625, pred_acc: 0.7578125 gate_loss: 99.05078125, cls_loss: 42263535616.0 cos_loss: 0.9508485794067383\n",
      "loss: 0.9563426971435547, acc: 0.00390625, pred_acc: 0.77734375 gate_loss: 101.7109375, cls_loss: 39686270976.0 cos_loss: 0.9563426971435547\n",
      "loss: 0.9600754380226135, acc: 0.0078125, pred_acc: 0.76171875 gate_loss: 102.20703125, cls_loss: 41213411328.0 cos_loss: 0.9600754380226135\n",
      "loss: 0.9366332292556763, acc: 0.01171875, pred_acc: 0.75390625 gate_loss: 101.765625, cls_loss: 42572464128.0 cos_loss: 0.9366332292556763\n",
      "loss: 0.9144560694694519, acc: 0.01171875, pred_acc: 0.7265625 gate_loss: 101.81269836425781, cls_loss: 41596370944.0 cos_loss: 0.9144560694694519\n",
      "loss: 0.9663377404212952, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 100.5390625, cls_loss: 40762077184.0 cos_loss: 0.9663377404212952\n",
      "loss: 0.9658114314079285, acc: 0.01171875, pred_acc: 0.796875 gate_loss: 100.71484375, cls_loss: 36605394944.0 cos_loss: 0.9658114314079285\n",
      "loss: 0.9477605819702148, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 100.73046875, cls_loss: 39077879808.0 cos_loss: 0.9477605819702148\n",
      "loss: 0.9523554444313049, acc: 0.0, pred_acc: 0.71484375 gate_loss: 100.57421875, cls_loss: 34672164864.0 cos_loss: 0.9523554444313049\n",
      "loss: 0.947464108467102, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 100.40625, cls_loss: 39206010880.0 cos_loss: 0.947464108467102\n",
      "loss: 0.9188687205314636, acc: 0.0, pred_acc: 0.76953125 gate_loss: 99.9921875, cls_loss: 43365908480.0 cos_loss: 0.9188687205314636\n",
      "loss: 0.9625635147094727, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 98.55078125, cls_loss: 36447105024.0 cos_loss: 0.9625635147094727\n",
      "loss: 0.9415577054023743, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 98.3984375, cls_loss: 40171548672.0 cos_loss: 0.9415577054023743\n",
      "loss: 0.9407157301902771, acc: 0.00390625, pred_acc: 0.734375 gate_loss: 98.078125, cls_loss: 36868116480.0 cos_loss: 0.9407157301902771\n",
      "loss: 0.9395355582237244, acc: 0.01171875, pred_acc: 0.7734375 gate_loss: 98.390625, cls_loss: 35444170752.0 cos_loss: 0.9395355582237244\n",
      "loss: 0.9264861345291138, acc: 0.0, pred_acc: 0.78515625 gate_loss: 97.68359375, cls_loss: 37605167104.0 cos_loss: 0.9264861345291138\n",
      "loss: 0.9498552083969116, acc: 0.015625, pred_acc: 0.765625 gate_loss: 98.3671875, cls_loss: 37020282880.0 cos_loss: 0.9498552083969116\n",
      "loss: 0.9575711488723755, acc: 0.01953125, pred_acc: 0.78125 gate_loss: 98.86328125, cls_loss: 39260430336.0 cos_loss: 0.9575711488723755\n",
      "loss: 0.9637134075164795, acc: 0.0078125, pred_acc: 0.71875 gate_loss: 99.1171875, cls_loss: 35137212416.0 cos_loss: 0.9637134075164795\n",
      "loss: 0.9431329965591431, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 99.0703125, cls_loss: 36296040448.0 cos_loss: 0.9431329965591431\n",
      "loss: 0.9478381872177124, acc: 0.00390625, pred_acc: 0.72265625 gate_loss: 100.0859375, cls_loss: 34497744896.0 cos_loss: 0.9478381872177124\n",
      "loss: 0.9575161933898926, acc: 0.0234375, pred_acc: 0.75390625 gate_loss: 100.74609375, cls_loss: 37314637824.0 cos_loss: 0.9575161933898926\n",
      "loss: 0.9515461921691895, acc: 0.00390625, pred_acc: 0.7890625 gate_loss: 101.2109375, cls_loss: 34616573952.0 cos_loss: 0.9515461921691895\n",
      "loss: 0.9245644807815552, acc: 0.0, pred_acc: 0.796875 gate_loss: 103.625, cls_loss: 36615315456.0 cos_loss: 0.9245644807815552\n",
      "loss: 0.9470531940460205, acc: 0.0, pred_acc: 0.7578125 gate_loss: 103.71484375, cls_loss: 38222225408.0 cos_loss: 0.9470531940460205\n",
      "loss: 0.9573295712471008, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 103.6484375, cls_loss: 35015520256.0 cos_loss: 0.9573295712471008\n",
      "loss: 0.9784097075462341, acc: 0.0078125, pred_acc: 0.78125 gate_loss: 103.8515625, cls_loss: 36051767296.0 cos_loss: 0.9784097075462341\n",
      "loss: 0.9565969109535217, acc: 0.00390625, pred_acc: 0.75 gate_loss: 104.25, cls_loss: 40007036928.0 cos_loss: 0.9565969109535217\n",
      "loss: 0.9567508101463318, acc: 0.00390625, pred_acc: 0.7734375 gate_loss: 104.5546875, cls_loss: 35485827072.0 cos_loss: 0.9567508101463318\n",
      "loss: 0.9416229724884033, acc: 0.01953125, pred_acc: 0.78125 gate_loss: 104.60546875, cls_loss: 35763732480.0 cos_loss: 0.9416229724884033\n",
      "loss: 0.9689608812332153, acc: 0.00390625, pred_acc: 0.796875 gate_loss: 104.57781982421875, cls_loss: 37164961792.0 cos_loss: 0.9689608812332153\n",
      "loss: 0.9438188076019287, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 105.2578125, cls_loss: 37297979392.0 cos_loss: 0.9438188076019287\n",
      "loss: 0.9557026624679565, acc: 0.01171875, pred_acc: 0.7421875 gate_loss: 105.28125, cls_loss: 32175384576.0 cos_loss: 0.9557026624679565\n",
      "loss: 0.9484114646911621, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 105.60546875, cls_loss: 32676474880.0 cos_loss: 0.9484114646911621\n",
      "loss: 0.9582073092460632, acc: 0.015625, pred_acc: 0.7734375 gate_loss: 103.921875, cls_loss: 33949102080.0 cos_loss: 0.9582073092460632\n",
      "loss: 0.9412386417388916, acc: 0.0078125, pred_acc: 0.78515625 gate_loss: 104.9765625, cls_loss: 35187302400.0 cos_loss: 0.9412386417388916\n",
      "loss: 0.9535481929779053, acc: 0.0078125, pred_acc: 0.7265625 gate_loss: 104.02734375, cls_loss: 35038179328.0 cos_loss: 0.9535481929779053\n",
      "loss: 0.9293422698974609, acc: 0.01171875, pred_acc: 0.78125 gate_loss: 104.37109375, cls_loss: 36603142144.0 cos_loss: 0.9293422698974609\n",
      "loss: 0.9745894074440002, acc: 0.00390625, pred_acc: 0.73828125 gate_loss: 104.0546875, cls_loss: 34140819456.0 cos_loss: 0.9745894074440002\n",
      "loss: 0.9146196842193604, acc: 0.0, pred_acc: 0.7421875 gate_loss: 105.23828125, cls_loss: 35863629824.0 cos_loss: 0.9146196842193604\n",
      "loss: 0.9546576738357544, acc: 0.00390625, pred_acc: 0.75390625 gate_loss: 103.8515625, cls_loss: 35863609344.0 cos_loss: 0.9546576738357544\n",
      "loss: 0.9440808296203613, acc: 0.0078125, pred_acc: 0.734375 gate_loss: 104.890625, cls_loss: 34567819264.0 cos_loss: 0.9440808296203613\n",
      "loss: 0.941773533821106, acc: 0.015625, pred_acc: 0.76171875 gate_loss: 104.0859375, cls_loss: 33590310912.0 cos_loss: 0.941773533821106\n",
      "loss: 0.9402998089790344, acc: 0.01171875, pred_acc: 0.734375 gate_loss: 104.515625, cls_loss: 32103680000.0 cos_loss: 0.9402998089790344\n",
      "loss: 0.9718886613845825, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 105.05859375, cls_loss: 28917446656.0 cos_loss: 0.9718886613845825\n",
      "loss: 0.9556078910827637, acc: 0.0078125, pred_acc: 0.76953125 gate_loss: 105.375, cls_loss: 32996298752.0 cos_loss: 0.9556078910827637\n",
      "loss: 0.9462801218032837, acc: 0.0078125, pred_acc: 0.77734375 gate_loss: 105.8671875, cls_loss: 32692455424.0 cos_loss: 0.9462801218032837\n",
      "loss: 0.9429022073745728, acc: 0.01171875, pred_acc: 0.7578125 gate_loss: 104.375, cls_loss: 31609720832.0 cos_loss: 0.9429022073745728\n",
      "loss: 0.9409647583961487, acc: 0.015625, pred_acc: 0.7421875 gate_loss: 105.2204360961914, cls_loss: 34109661184.0 cos_loss: 0.9409647583961487\n",
      "loss: 0.9586176872253418, acc: 0.01171875, pred_acc: 0.75390625 gate_loss: 105.73442077636719, cls_loss: 33041760256.0 cos_loss: 0.9586176872253418\n",
      "loss: 0.9441784024238586, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 104.6640625, cls_loss: 31116232704.0 cos_loss: 0.9441784024238586\n",
      "loss: 0.9286087155342102, acc: 0.00390625, pred_acc: 0.78125 gate_loss: 105.15234375, cls_loss: 33295159296.0 cos_loss: 0.9286087155342102\n",
      "loss: 0.9395040273666382, acc: 0.0078125, pred_acc: 0.7421875 gate_loss: 104.98828125, cls_loss: 31895861248.0 cos_loss: 0.9395040273666382\n",
      "loss: 0.9484140872955322, acc: 0.01171875, pred_acc: 0.76171875 gate_loss: 105.1171875, cls_loss: 30813925376.0 cos_loss: 0.9484140872955322\n",
      "loss: 0.9619271159172058, acc: 0.015625, pred_acc: 0.765625 gate_loss: 105.75822448730469, cls_loss: 32142327808.0 cos_loss: 0.9619271159172058\n",
      "loss: 0.9587849378585815, acc: 0.01171875, pred_acc: 0.75 gate_loss: 96.53515625, cls_loss: 34833309696.0 cos_loss: 0.9587849378585815\n",
      "loss: 0.969659686088562, acc: 0.015625, pred_acc: 0.79296875 gate_loss: 89.89453125, cls_loss: 36942073856.0 cos_loss: 0.969659686088562\n",
      "loss: 0.9565401077270508, acc: 0.0, pred_acc: 0.7890625 gate_loss: 85.109375, cls_loss: 40727678976.0 cos_loss: 0.9565401077270508\n",
      "loss: 0.9745245575904846, acc: 0.0, pred_acc: 0.78125 gate_loss: 80.046875, cls_loss: 43252559872.0 cos_loss: 0.9745245575904846\n",
      "loss: 1.0009820461273193, acc: 0.0, pred_acc: 0.7890625 gate_loss: 76.0546875, cls_loss: 44766597120.0 cos_loss: 1.0009820461273193\n",
      "loss: 0.9871136546134949, acc: 0.00390625, pred_acc: 0.78515625 gate_loss: 73.43359375, cls_loss: 37628575744.0 cos_loss: 0.9871136546134949\n",
      "loss: 1.0041126012802124, acc: 0.0, pred_acc: 0.7265625 gate_loss: 69.890625, cls_loss: 42215682048.0 cos_loss: 1.0041126012802124\n",
      "loss: 0.9942773580551147, acc: 0.00390625, pred_acc: 0.74609375 gate_loss: 66.89453125, cls_loss: 44021035008.0 cos_loss: 0.9942773580551147\n",
      "loss: 1.0112582445144653, acc: 0.0, pred_acc: 0.7249999642372131 gate_loss: 68.20999908447266, cls_loss: 46966779904.0 cos_loss: 1.0112582445144653\n"
     ]
    }
   ],
   "source": [
    "hidden_size = config.hidden_size\n",
    "\n",
    "# set pred_model to evaluation mode, becasue we don't train it\n",
    "# set the extractor_model to training mode.\n",
    "pred_model.eval()\n",
    "\n",
    "model = SurrogateInterpretation(query_vector_size=hidden_size, classifier=pred_model.classifier, num_labels=1000) \n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Freeze parameters of the pred model \n",
    "def freeze_params(m: nn.Module):\n",
    "    for name, param in m.named_parameters():\n",
    "        param.requires_grad = False\n",
    "freeze_params(pred_model)\n",
    "\n",
    "def get_pseudo_logits(pred_model, inputs):\n",
    "    # obtain the cls token output of the last hidden state\n",
    "    pred_outputs = pred_model(pixel_values=inputs, output_hidden_states=True)\n",
    "    query_vector = pred_outputs.hidden_states[-1][:,0,:] # (N, d)\n",
    "    pseudo_logits = pred_outputs.logits\n",
    "    return pseudo_logits, query_vector\n",
    "\n",
    "pseudo_logits, query_vector = get_pseudo_logits(pred_model, inputs['pixel_values'])\n",
    "\n",
    "outputs = model(pixel_values=inputs['pixel_values'], \n",
    "                query_vector=query_vector, \n",
    "                pseudo_probs=pseudo_logits, \n",
    "                labels=None)\n",
    "# print(\"attention_output shape: \", outputs['attention_output'].shape)\n",
    "# print(\"attention_weights shape: \", outputs['attention_weights'].shape)\n",
    "# print(\"feature_maps shape: \", outputs['feature_maps'].shape)\n",
    "print(\"query_vector shape: \", outputs['query_vector'].shape)\n",
    "\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "def evaluation():\n",
    "    with torch.no_grad():\n",
    "        for idx, data in enumerate(val_dataloader):\n",
    "            pixel_values = data['pixel_values'].to(device)\n",
    "            label = data['labels'].to(device)\n",
    "            pseudo_logits, query_vector = get_pseudo_logits(pred_model, pixel_values)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values, \n",
    "                            query_vector=query_vector, \n",
    "                            pseudo_probs=pseudo_logits, \n",
    "                            labels=label)\n",
    "        \n",
    "            loss = outputs['loss']\n",
    "            print(f\"loss: {loss.item()}, acc: {outputs['acc'].item()}, pred_acc: {outputs['pred_acc'].item()}\")\n",
    "            break\n",
    "                #f\"cossim_loss: {outputs['cossim_loss'].item()}, cls_loss: {outputs['cls_loss'].item()}\")\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for idx, data in enumerate(train_dataloader):\n",
    "        pixel_values = data['pixel_values'].to(device)\n",
    "        label = data['labels'].to(device)\n",
    "        pseudo_logits, query_vector = get_pseudo_logits(pred_model, pixel_values)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, \n",
    "                        query_vector=query_vector, \n",
    "                        pseudo_probs=pseudo_logits, \n",
    "                        labels=label)\n",
    "    \n",
    "        loss = outputs['loss']\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"loss: {loss.item()}, acc: {outputs['acc'].item()}, pred_acc: {outputs['pred_acc'].item()}\"\n",
    "              f\" gate_loss: {outputs['gate_loss'].item()}, cls_loss: {outputs['cls_loss'].item()}\"\n",
    "              f\" cos_loss: {outputs['cos_loss'].item()}\")\n",
    "        # if (idx + 1) % 5 == 0:\n",
    "        #     print(\"evaluation\")\n",
    "        #     model.eval()\n",
    "        #     evaluation()\n",
    "        #     model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/gated_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_embedding.weight\n",
      "True\n",
      "patch_embedding.bias\n",
      "True\n",
      "transform_func.input_layer.weight\n",
      "True\n",
      "transform_func.input_layer.bias\n",
      "True\n",
      "transform_func.layers.0.0.weight\n",
      "True\n",
      "transform_func.layers.0.0.bias\n",
      "True\n",
      "transform_func.layers.0.3.weight\n",
      "True\n",
      "transform_func.layers.0.3.bias\n",
      "True\n",
      "transform_func.layers.0.5.weight\n",
      "True\n",
      "transform_func.layers.0.5.bias\n",
      "True\n",
      "transform_func.layers.1.0.weight\n",
      "True\n",
      "transform_func.layers.1.0.bias\n",
      "True\n",
      "transform_func.layers.1.3.weight\n",
      "True\n",
      "transform_func.layers.1.3.bias\n",
      "True\n",
      "transform_func.layers.1.5.weight\n",
      "True\n",
      "transform_func.layers.1.5.bias\n",
      "True\n",
      "transform_func.layers.2.0.weight\n",
      "True\n",
      "transform_func.layers.2.0.bias\n",
      "True\n",
      "transform_func.layers.2.3.weight\n",
      "True\n",
      "transform_func.layers.2.3.bias\n",
      "True\n",
      "transform_func.layers.2.5.weight\n",
      "True\n",
      "transform_func.layers.2.5.bias\n",
      "True\n",
      "transform_func.layers.3.0.weight\n",
      "True\n",
      "transform_func.layers.3.0.bias\n",
      "True\n",
      "transform_func.layers.3.3.weight\n",
      "True\n",
      "transform_func.layers.3.3.bias\n",
      "True\n",
      "transform_func.layers.3.5.weight\n",
      "True\n",
      "transform_func.layers.3.5.bias\n",
      "True\n",
      "transform_func.layers.4.0.weight\n",
      "True\n",
      "transform_func.layers.4.0.bias\n",
      "True\n",
      "transform_func.layers.4.3.weight\n",
      "True\n",
      "transform_func.layers.4.3.bias\n",
      "True\n",
      "transform_func.layers.4.5.weight\n",
      "True\n",
      "transform_func.layers.4.5.bias\n",
      "True\n",
      "transform_func.output_layer.weight\n",
      "True\n",
      "transform_func.output_layer.bias\n",
      "True\n",
      "attention.query.weight\n",
      "True\n",
      "attention.query.bias\n",
      "True\n",
      "attention.key.weight\n",
      "True\n",
      "attention.key.bias\n",
      "True\n",
      "classifier.weight\n",
      "False\n",
      "classifier.bias\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    # print(param.grad)\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassifierOutput(loss=tensor(1.0113, device='cuda:0', grad_fn=<NegBackward0>), logits=tensor([[-2.7440e-01,  8.2152e-01, -8.3646e-02,  4.1588e-01,  5.6233e-01,\n",
       "          1.8593e-01, -5.7729e-01, -4.6004e-01, -5.3389e-01,  2.4016e-01,\n",
       "         -3.1957e-01, -5.9910e-01, -6.6402e-01, -4.9756e-01, -6.2448e-01,\n",
       "         -1.3501e+00, -1.0016e-01, -6.2170e-01,  1.1087e-01, -1.1060e+00,\n",
       "         -2.0846e-01,  3.1697e-01, -9.3152e-01, -3.0693e-01, -1.0124e+00,\n",
       "         -1.8751e-01,  5.8825e-01, -3.6161e-01, -7.4696e-01,  7.4135e-01,\n",
       "         -3.6653e-01, -2.7586e-01,  3.6596e-01, -1.1206e+00, -8.8843e-02,\n",
       "         -1.1328e+00,  1.5458e-01, -1.0399e+00,  1.0136e+00, -1.0395e+00,\n",
       "         -2.4214e+00,  5.1124e-01,  4.9458e-01, -7.4005e-01, -1.5815e+00,\n",
       "         -3.2451e-01, -2.0448e+00, -4.8128e-01, -6.3616e-01, -1.1355e+00,\n",
       "         -1.0902e+00, -4.5296e-02, -6.4045e-01, -2.3987e-01,  1.3110e-01,\n",
       "         -1.2664e+00, -4.7160e-01, -4.3717e-01, -9.5664e-01, -5.9685e-01,\n",
       "          5.0885e-01, -8.4836e-02,  2.6987e-01, -1.5009e-03, -5.3329e-01,\n",
       "          1.8374e-02,  1.3334e+00,  2.4888e-01,  1.0063e+00,  7.6835e-01,\n",
       "         -1.5822e+00, -1.1380e-01, -3.2713e-01, -2.5521e-02, -3.2676e-01,\n",
       "         -3.1566e-01,  3.4420e-01, -6.7578e-01,  5.6247e-02,  1.0190e+00,\n",
       "          5.4855e-01, -1.2385e+00,  2.1572e-03,  1.1123e+00,  1.0889e-01,\n",
       "         -9.7254e-02, -2.5954e-02,  5.7346e-01, -1.5262e-01, -1.8867e+00,\n",
       "         -2.8063e-01,  3.5609e-01,  6.1713e-01,  1.6875e+00, -4.3178e-01,\n",
       "          8.9129e-03,  1.1920e+00, -3.2913e-01, -7.3960e-01, -7.0972e-01,\n",
       "          4.8708e-01, -9.3546e-02,  2.6652e-01,  5.2837e-01,  1.3479e+00,\n",
       "          1.1221e-02,  4.6141e-01, -5.7131e-01, -1.6571e-01,  1.1242e-01,\n",
       "         -2.3517e-01, -4.8880e-01, -9.9192e-01, -5.0105e-01, -4.8489e-01,\n",
       "         -4.9375e-01, -3.3806e-01,  2.4019e-01, -1.9119e-01, -1.1011e+00,\n",
       "         -1.7069e+00, -2.2294e-01, -6.8953e-01, -4.9889e-01, -4.8608e-01,\n",
       "         -7.6868e-01,  8.1537e-01, -1.1233e+00, -2.1399e-01, -6.4167e-01,\n",
       "         -1.9564e-03, -2.9136e-01,  1.1972e-01,  1.0402e-01,  3.5878e-01,\n",
       "         -3.7325e-01,  1.0803e+00,  7.0626e-01,  1.6979e+00, -1.1570e+00,\n",
       "         -5.6738e-02, -3.8451e-01, -6.8758e-01, -1.0975e+00, -1.3078e+00,\n",
       "         -4.1956e-01, -8.2479e-01, -2.4859e-01, -7.0805e-01, -4.2823e-01,\n",
       "          4.9027e-01,  2.4361e+00, -6.9993e-01, -1.3069e-01,  2.4626e-01,\n",
       "         -1.1333e-01, -2.9626e-01,  2.5771e-01,  9.4812e-01, -2.9462e-01,\n",
       "          2.7271e-01,  1.2025e-01, -1.8200e-01,  4.1967e-01, -6.1763e-01,\n",
       "         -6.5493e-01, -5.3568e-01, -1.6535e+00, -2.3745e-01,  1.6642e-01,\n",
       "          4.8847e-01,  6.6036e-01,  1.1190e+00,  1.1518e+00,  5.8662e-01,\n",
       "         -1.3715e+00, -3.2914e-01, -1.7763e-01,  1.6187e+00, -5.6090e-01,\n",
       "         -2.8725e-01, -9.0181e-01, -1.0113e+00, -6.3814e-01,  5.2166e-01,\n",
       "          7.4613e-01,  1.0333e+00,  8.1840e-01, -1.3876e+00, -5.9455e-01,\n",
       "         -1.0704e+00,  7.6960e-02, -7.0641e-02,  1.1513e+00, -9.4308e-01,\n",
       "          1.0859e+00,  1.4987e-02,  3.6260e-01,  1.4790e+00, -8.0545e-01,\n",
       "         -1.2812e+00,  4.6480e-01, -1.4895e-01, -1.0958e-01,  6.5870e-01,\n",
       "          8.0927e-01,  1.6042e+00,  1.2460e+00,  1.4121e+00,  9.3363e-01,\n",
       "          8.8407e-01,  6.6707e-01,  1.1530e-01,  6.6987e-02, -3.6389e-01,\n",
       "         -1.1113e-01, -7.0843e-01, -2.1488e-01, -7.7222e-02,  5.7772e-01,\n",
       "         -6.1893e-01, -5.0870e-01,  6.6297e-01,  2.5727e-01,  1.1874e+00,\n",
       "         -1.0786e+00, -2.1505e-01, -6.0127e-01, -7.2902e-01, -8.8006e-01,\n",
       "         -3.6280e-01, -1.0738e-01,  7.0196e-01, -2.6081e-01, -5.2853e-01,\n",
       "          6.4915e-01,  6.4392e-01,  6.2249e-01, -7.7752e-01,  2.3403e-01,\n",
       "          4.0907e-01, -4.9212e-01, -4.0451e-01,  2.2779e+00,  2.5964e-01,\n",
       "          7.3747e-01,  2.0521e+00, -1.2726e+00,  1.2519e+00,  1.8160e-01,\n",
       "          9.5821e-01,  1.0097e+00, -3.6869e-01, -7.0831e-02,  2.4287e+00,\n",
       "          3.1362e-01, -8.8791e-02,  3.6186e-01,  1.3932e-01,  9.4902e-01,\n",
       "         -2.8978e-01,  4.5243e-01,  2.4066e-01,  2.7516e-01,  6.4583e-01,\n",
       "          2.2476e-01,  2.8123e-01, -2.7115e-02,  2.1226e+00, -9.7692e-01,\n",
       "         -9.3125e-01, -1.3479e-01,  1.5560e-01, -9.5713e-02,  5.0977e-02,\n",
       "          3.4425e-01,  1.7494e+00,  3.0856e-01,  6.7257e-01,  1.1297e-01,\n",
       "          1.1354e+00,  9.2246e+00,  8.2434e+00,  3.4349e+00,  5.1892e+00,\n",
       "          1.2419e+01,  2.7142e+00,  6.7615e+00,  2.7834e+00,  3.3213e+00,\n",
       "          2.7445e+00,  1.1721e+00,  2.9049e+00,  3.2908e+00, -1.1714e+00,\n",
       "         -3.9413e-01, -1.0572e+00, -1.1997e+00,  1.3443e+00,  1.0496e+00,\n",
       "         -1.9585e-01, -9.2810e-01,  8.0214e-01,  1.1943e+00,  1.5536e-01,\n",
       "         -2.4694e-01,  4.1648e-01,  4.3922e-01,  5.6566e-01, -5.9879e-01,\n",
       "         -9.4627e-01, -3.9202e-01, -4.1245e-01, -1.9356e+00,  7.1511e-02,\n",
       "         -1.8446e+00, -7.2554e-01, -1.2953e-01, -3.8618e-01, -5.7988e-01,\n",
       "         -4.6145e-01, -1.1922e+00, -1.0444e+00, -1.4972e+00, -9.3877e-01,\n",
       "         -1.5967e+00, -4.8206e-01, -9.9910e-01, -1.0700e+00,  2.8532e-01,\n",
       "          7.9193e-01,  7.4121e-01,  1.9572e+00,  4.9881e-01, -2.4233e-01,\n",
       "         -6.0462e-01, -1.7086e+00,  4.2006e-01, -9.2753e-02, -5.5907e-02,\n",
       "          1.1660e+00,  1.3959e+00,  2.5617e-01,  2.0618e-01, -2.0619e-01,\n",
       "          5.0448e-01, -4.5773e-01, -6.0199e-02, -4.4010e-01, -7.7120e-01,\n",
       "         -4.7521e-01,  4.2124e-01, -1.6700e-01, -2.0272e-01,  2.1957e-01,\n",
       "          3.0896e-01,  1.4704e+00,  2.0342e+00,  1.7751e+00,  2.7294e+00,\n",
       "          8.3779e-01,  1.2457e+00,  7.6050e-01,  1.0273e+00, -6.9039e-01,\n",
       "         -1.0521e+00, -9.8197e-01, -1.5549e+00, -2.7766e-01, -5.6514e-01,\n",
       "         -4.6554e-01,  3.2677e-02, -2.8894e-01, -3.7544e-01,  4.9535e-01,\n",
       "         -1.6795e+00,  8.2198e-01,  3.3524e-01, -7.8522e-02, -6.3497e-01,\n",
       "          2.7987e-01, -1.2589e+00, -5.0363e-01,  5.8114e-01, -1.1996e-01,\n",
       "          1.0035e+00, -2.2806e-01, -4.5122e-01, -1.0766e+00, -8.7707e-01,\n",
       "         -5.9994e-01, -3.3739e-01, -2.1972e-01, -5.7738e-01,  4.8167e-01,\n",
       "         -3.0150e-02,  1.1394e+00, -2.1640e-01, -1.3076e+00,  3.0939e-01,\n",
       "         -6.2128e-01, -9.3001e-01, -1.2858e+00, -4.3142e-01,  3.2152e-01,\n",
       "          1.0875e-01,  2.9716e-01,  2.7964e-01,  4.8177e-01, -1.0744e+00,\n",
       "         -1.1445e+00,  3.2509e-01,  5.6577e-01,  4.0458e-01,  1.7592e+00,\n",
       "         -2.3754e-01, -2.8476e-02, -3.6820e-03,  7.6423e-02,  5.7744e-01,\n",
       "         -8.6129e-01, -7.1558e-01, -6.3676e-01, -8.3527e-01, -7.7784e-01,\n",
       "         -1.0800e-01, -3.5608e-01, -6.9455e-01,  3.2584e-01, -5.1246e-01,\n",
       "         -8.0428e-01,  4.2397e-01, -3.5443e-01, -4.4501e-02, -1.1915e+00,\n",
       "          8.2980e-01,  7.8841e-01,  1.1845e-01, -4.5100e-01, -6.4679e-01,\n",
       "         -1.0271e+00, -3.7493e-01,  2.8910e-01, -5.2395e-01,  4.9606e-01,\n",
       "         -5.8948e-01,  6.5076e-01,  2.5874e-01, -1.7141e+00, -3.6961e-01,\n",
       "         -5.5960e-01,  1.6648e-01,  6.4772e-01, -3.8954e-02,  1.2290e-01,\n",
       "          2.1829e-01, -2.6131e-01,  1.8974e+00, -7.0874e-01,  8.1532e-01,\n",
       "          8.4909e-01, -6.4389e-01,  5.1968e-01,  1.6919e-01,  3.4676e-01,\n",
       "          8.2633e-01,  9.9245e-01,  4.9263e-01, -1.9911e-02,  4.1965e-01,\n",
       "         -1.3451e+00, -4.0570e-01, -3.5356e-01,  6.2517e-01, -1.3734e-01,\n",
       "          2.4840e-01,  5.3367e-01,  1.1360e+00,  7.0188e-01, -6.5715e-01,\n",
       "          7.3630e-01, -5.2197e-01, -1.1020e-01,  1.8150e-01,  2.6709e-01,\n",
       "          6.8624e-01, -2.3975e-01,  9.8127e-01, -1.2459e-01, -7.5877e-01,\n",
       "          7.3448e-02, -9.9351e-01, -4.2777e-01, -4.1875e-01, -8.8321e-01,\n",
       "         -4.3163e-01, -3.0265e-01,  1.4455e-03,  7.3518e-01,  4.8785e-01,\n",
       "         -1.7422e-01,  5.2704e-01, -6.6004e-01,  2.0918e-01,  1.1516e-01,\n",
       "         -5.2116e-01,  2.0090e-01, -6.7433e-01,  9.1537e-01, -1.1738e+00,\n",
       "         -7.6295e-01,  6.6664e-01, -4.1686e-01, -1.4499e+00,  1.3901e-01,\n",
       "          9.9322e-01,  9.8053e-02, -1.0718e+00,  8.1044e-01, -6.7834e-02,\n",
       "         -6.7398e-01, -5.1529e-01,  2.0844e-01,  1.6544e+00, -8.4659e-01,\n",
       "         -7.9887e-01,  1.2859e-01,  4.9094e-01, -9.8153e-01,  2.6284e-01,\n",
       "         -5.0355e-01, -1.5533e-01,  2.8213e-02, -8.8739e-01, -1.5765e-02,\n",
       "         -7.4631e-01, -8.1062e-01, -3.0902e-01, -5.4260e-01,  1.5043e+00,\n",
       "         -7.3558e-01, -9.4519e-01,  1.5595e-01,  7.0973e-01, -6.7472e-01,\n",
       "         -1.2607e+00, -1.2296e-01, -1.0845e+00, -2.1875e-01, -2.9757e-01,\n",
       "         -8.3845e-01,  1.4300e-01, -5.0874e-01,  2.5716e-02, -7.6290e-01,\n",
       "         -3.3885e-01, -4.2422e-02, -1.5976e-01,  3.0160e-01,  1.3273e+00,\n",
       "         -1.6781e+00, -5.4436e-01, -2.8526e-01, -7.8079e-01, -1.6894e-01,\n",
       "          5.0532e-01, -7.5372e-02,  1.6244e-01,  1.1134e+00, -1.1903e+00,\n",
       "         -2.8913e-01, -5.6730e-01, -4.0905e-01,  6.1022e-01, -6.1974e-01,\n",
       "          6.4292e-01,  2.1886e-01, -1.1264e+00,  3.5416e-01,  3.1475e-01,\n",
       "         -1.8573e-01, -4.6855e-01,  1.1824e-01, -3.0977e-01, -4.0062e-01,\n",
       "          2.5505e-01,  1.0325e+00, -2.6545e-01,  5.2354e-01, -6.4195e-01,\n",
       "         -2.7811e-02, -1.9681e-01, -3.7175e-01,  2.0471e-01,  1.8113e-01,\n",
       "          4.4919e-01,  7.6956e-01,  3.3565e-01,  5.6310e-01, -1.9201e+00,\n",
       "         -3.2162e-01,  3.0347e-01, -1.1089e+00, -2.0784e-01, -1.0610e+00,\n",
       "          2.9501e-01, -8.4815e-01, -7.9827e-01,  6.1276e-01, -4.0942e-03,\n",
       "         -1.9935e-01, -1.2166e-01,  6.2084e-01,  4.5649e-01, -8.8974e-01,\n",
       "          5.8859e-01, -4.2699e-01,  1.3895e-01, -1.0995e+00, -6.1191e-01,\n",
       "          2.1028e+00,  6.3405e-01,  1.1876e+00,  3.3910e-01,  5.1135e-03,\n",
       "         -2.2754e-01, -1.6974e+00,  1.4149e+00, -7.1520e-01, -5.4555e-01,\n",
       "         -3.7761e-01,  8.6435e-01,  3.9194e-01, -1.1095e+00, -4.6969e-01,\n",
       "          1.7947e-01,  4.2453e-01, -8.4034e-01,  7.4392e-01,  2.9486e-01,\n",
       "         -4.0502e-01, -3.8891e-01, -1.1372e+00,  3.3015e-01,  8.8616e-02,\n",
       "         -2.0107e-01,  1.4688e+00, -1.5054e-01, -1.9651e-01,  8.5750e-02,\n",
       "         -4.8125e-01,  3.6400e-01, -1.2768e-01, -1.2785e+00,  3.7882e-01,\n",
       "          5.1384e-01,  2.2562e+00,  7.1396e-01,  1.9604e-01,  1.3043e-01,\n",
       "          5.4616e-02, -7.1471e-01, -6.2496e-01, -5.8214e-01,  6.4207e-01,\n",
       "          1.6644e+00, -6.5639e-01, -6.0604e-01, -3.6229e-01, -8.5739e-01,\n",
       "          1.8551e+00,  3.1923e-02,  1.3245e+00,  1.4893e+00,  4.1012e-01,\n",
       "         -9.1091e-01,  1.7001e+00,  5.8547e-01,  1.1650e+00,  2.1780e-01,\n",
       "          2.2610e+00,  1.4491e+00, -4.0709e-01, -6.3869e-01, -3.0787e-01,\n",
       "          2.0166e+00, -1.5269e-01,  1.5882e-01,  2.7698e-01,  5.5204e-02,\n",
       "          1.2022e-01,  4.2927e-01, -2.4668e-01, -7.7653e-01, -5.5620e-01,\n",
       "         -7.4234e-01,  7.3248e-02,  1.7739e+00,  5.2387e-01, -5.3746e-01,\n",
       "          4.5963e-01,  2.8235e-01, -1.8619e+00, -2.8060e-01,  6.9280e-01,\n",
       "          1.2346e-01, -8.2240e-01, -5.0642e-02, -7.0809e-01,  5.3008e-01,\n",
       "          3.6089e-02, -5.1863e-01, -1.2513e+00,  9.5009e-01, -9.4893e-01,\n",
       "          3.8106e-01, -4.1751e-02,  1.3699e-01, -1.0686e+00, -1.5132e+00,\n",
       "          5.1402e-01,  1.3833e+00,  1.7781e+00, -2.6448e-01, -1.5462e+00,\n",
       "         -7.0315e-02, -1.6882e+00,  1.8423e-01,  1.5900e+00, -8.0075e-01,\n",
       "         -1.2254e+00,  5.7594e-01, -2.2487e-02, -4.6676e-01, -1.2282e+00,\n",
       "         -1.2785e+00,  4.9975e-01, -1.3482e+00, -1.6023e-01, -2.6754e-01,\n",
       "         -5.9867e-01,  9.2814e-01,  1.8817e+00, -6.8047e-01,  1.1702e+00,\n",
       "         -5.3678e-01, -4.9709e-01, -5.8847e-01,  1.4025e-01, -1.2399e+00,\n",
       "          2.5401e+00,  9.6136e-02, -8.4394e-01,  2.2136e+00, -5.3243e-01,\n",
       "          2.7421e-02,  4.5158e-01, -6.7256e-01,  9.0890e-02, -5.2418e-01,\n",
       "          5.0862e-01,  2.1475e+00, -3.4157e-01, -6.3769e-01, -1.0648e-01,\n",
       "         -4.5618e-01,  1.1507e+00,  1.1930e+00, -1.0939e+00,  7.5884e-01,\n",
       "          8.4331e-03, -7.9318e-01, -2.8177e-01, -7.5870e-01,  4.4149e-02,\n",
       "         -2.1433e-01, -2.4516e-01,  6.2996e-01, -7.9924e-01, -2.0618e-01,\n",
       "         -1.3106e+00, -5.4182e-01,  1.0733e+00,  4.6978e-01,  5.0547e-01,\n",
       "          3.1910e+00,  8.3487e-02, -6.1829e-01, -2.8447e-01,  3.3360e-02,\n",
       "          4.6518e-01, -1.7266e-01,  2.1146e-01,  5.5168e-01, -3.7053e-02,\n",
       "          6.0374e-02,  6.3373e-01,  2.5814e+00, -4.9609e-01,  8.4855e-01,\n",
       "          2.9119e-01,  4.1263e-01,  8.7382e-01, -2.6697e-01, -2.3373e-01,\n",
       "         -2.2684e+00, -1.3204e-01,  8.4527e-02,  2.5386e-01, -6.0919e-02,\n",
       "         -2.0888e-01,  2.7876e+00, -4.6881e-01, -1.3111e-01, -7.6445e-01,\n",
       "         -9.0823e-02, -9.0411e-01,  9.9204e-01,  1.8669e-01,  1.2859e+00,\n",
       "         -8.3082e-01, -1.6369e+00, -1.0891e+00,  4.8475e-01, -4.1585e-01,\n",
       "         -2.5464e-01, -3.6219e-01,  3.0436e-01,  1.2003e-01, -3.0959e-01,\n",
       "          1.6867e+00,  2.0983e+00, -6.8361e-01, -4.2406e-01, -4.1992e-01,\n",
       "         -5.3525e-01, -5.4705e-01, -1.7306e-01,  1.9723e+00, -1.3187e-01,\n",
       "          2.7744e-01, -2.9025e-01,  8.0483e-01, -1.2614e+00,  3.3269e-01,\n",
       "          1.4268e+00, -4.5754e-01,  2.5705e-01,  1.4156e-01, -1.2762e-01,\n",
       "          1.1515e+00,  1.5074e+00,  7.6816e-02, -5.2184e-01,  1.8905e+00,\n",
       "         -1.0434e+00, -3.7242e-01,  5.4028e-01, -5.1568e-01,  3.7434e-01,\n",
       "         -4.9443e-01,  4.9512e-01, -1.7179e-01, -6.5369e-01,  1.7693e-01,\n",
       "          9.6347e-01,  7.1897e-02,  1.2054e+00, -5.0967e-01,  5.1655e-02,\n",
       "         -1.0479e+00,  2.9729e-02,  9.2197e-01, -5.8221e-01, -1.3241e+00,\n",
       "         -3.4358e-01,  5.7025e-01, -4.7002e-01, -6.3321e-01,  1.3735e+00,\n",
       "          1.1043e-01,  4.8302e-01,  9.3428e-01, -2.5793e-01, -1.1571e+00,\n",
       "          1.8266e+00,  3.4023e-01, -1.2013e+00, -7.6217e-01, -3.8434e-01,\n",
       "         -4.8746e-01, -9.8153e-01, -6.2929e-01, -3.5940e-01,  1.1546e+00,\n",
       "          2.9395e-01,  6.1523e-01,  1.3506e-01,  1.5891e+00,  5.2941e-01,\n",
       "         -6.0901e-01, -7.7012e-01,  4.4061e-01,  7.6849e-01,  9.6495e-01,\n",
       "          9.3406e-02,  3.6738e-01, -5.7888e-01, -8.2980e-03, -5.2764e-01,\n",
       "         -9.4816e-01,  2.5897e-02,  1.1908e-01, -2.7109e-02, -1.0012e+00,\n",
       "          5.4562e-02, -4.1429e-01, -2.9267e-01,  2.2967e+00, -1.8628e-01,\n",
       "          1.6241e-01, -9.7087e-01, -1.9991e-01, -5.2037e-02, -5.5518e-01,\n",
       "         -6.2326e-01,  4.0608e-01, -1.4164e+00, -5.7858e-01,  4.7241e-01,\n",
       "          2.4964e-02, -3.0017e-01, -3.1323e-01, -1.5171e+00, -6.2090e-01,\n",
       "         -6.7453e-01, -1.0476e+00,  8.4142e-02, -3.7253e-01,  1.2036e-01,\n",
       "         -1.4899e+00, -1.6975e+00, -1.2126e+00, -8.3006e-01,  1.0227e-01,\n",
       "         -8.8374e-01, -5.9596e-02, -7.1446e-01, -4.0916e-01, -2.5237e-01,\n",
       "         -1.3012e+00, -7.1904e-01, -3.6575e-01, -1.1435e+00,  3.7217e-01,\n",
       "         -8.3983e-01, -1.2421e+00, -2.0016e+00,  6.3994e-01, -2.9642e-01,\n",
       "         -1.2733e+00, -1.0175e+00, -8.1577e-02, -1.2890e+00, -7.5889e-01,\n",
       "         -5.4226e-01, -2.2517e-01, -6.6734e-01,  4.2125e-01, -5.5033e-02,\n",
       "         -9.3223e-01,  5.6551e-01,  1.2528e-01, -2.2980e-01, -4.6202e-01,\n",
       "         -6.4327e-01, -8.6787e-01, -1.1124e-01, -6.4735e-02, -8.1441e-02,\n",
       "          7.5747e-02, -4.4919e-01,  4.4366e-01,  3.5033e-01, -6.1740e-01,\n",
       "         -1.0161e+00, -8.0745e-01, -9.0065e-01, -1.4300e+00,  3.9205e-01,\n",
       "         -9.0833e-01, -3.3837e-01, -1.4402e+00,  1.4775e-01, -7.8797e-01,\n",
       "         -8.5843e-01,  2.9447e-01, -6.6196e-01, -9.0431e-01,  6.1098e-02]],\n",
       "       device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=(tensor([[[ 0.0195,  0.0298, -0.5341,  ...,  0.0042, -0.0201, -0.8158],\n",
       "         [ 0.2138, -0.1478,  0.1984,  ...,  0.0329,  0.3637, -1.3785],\n",
       "         [-0.2235, -0.0821,  0.0874,  ...,  0.0082,  0.3486, -1.3180],\n",
       "         ...,\n",
       "         [ 0.0058, -0.1374, -0.1287,  ...,  0.3021,  0.2901, -1.1500],\n",
       "         [-0.1565, -0.1544, -0.1163,  ...,  0.3314,  0.4554, -1.4122],\n",
       "         [ 0.5006, -0.2616,  0.1262,  ...,  0.7968,  0.8747, -1.2230]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-0.2124, -0.0457, -0.8402,  ...,  0.5455, -0.5121, -1.0968],\n",
       "         [ 0.4190,  0.0777, -0.5895,  ...,  0.5957, -1.0995, -2.4486],\n",
       "         [-0.1068, -0.2157, -0.1509,  ..., -0.4899, -0.1915, -2.5036],\n",
       "         ...,\n",
       "         [-0.5737,  0.5071, -0.6048,  ..., -0.5760,  0.6478, -1.6868],\n",
       "         [-0.4868,  0.1148, -0.3595,  ..., -0.3790,  0.6645, -2.3384],\n",
       "         [ 0.1854, -1.6476, -0.5062,  ..., -0.4795, -0.1468, -1.8889]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-0.2761, -0.4033, -0.7963,  ...,  0.2752, -1.0528, -0.5781],\n",
       "         [ 1.5400,  1.4223,  0.0906,  ...,  0.9550, -1.1697, -3.1143],\n",
       "         [ 0.4640,  0.8933,  0.9232,  ...,  0.1983, -0.0662, -3.0330],\n",
       "         ...,\n",
       "         [-0.4534,  1.0381, -0.0457,  ..., -0.0686, -0.2092, -2.0791],\n",
       "         [-0.5195,  0.5077, -0.5636,  ..., -0.7087,  1.2084, -3.8062],\n",
       "         [ 0.9887,  0.3152,  0.1719,  ..., -1.6340, -0.6598, -4.0821]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-0.3007, -0.5376, -0.9212,  ...,  0.1993, -2.0268, -0.5863],\n",
       "         [ 1.9392,  2.3376,  0.3727,  ...,  0.7454, -0.1254, -4.4001],\n",
       "         [ 0.6801,  2.6173,  1.6476,  ...,  0.2119, -0.0270, -3.5595],\n",
       "         ...,\n",
       "         [-0.5270,  2.1541, -0.3111,  ...,  0.5598,  0.9799, -3.9075],\n",
       "         [ 0.0141,  1.9925, -1.2025,  ..., -0.0915,  1.9368, -4.9829],\n",
       "         [-0.2755,  1.7407, -0.9251,  ..., -1.8785,  1.2611, -6.1649]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-0.4751, -0.4499, -1.2042,  ..., -0.5672, -2.0493, -0.9314],\n",
       "         [ 2.0370,  1.8590, -0.7882,  ..., -0.2339,  0.8552, -2.5985],\n",
       "         [ 0.4810,  2.6398,  0.7231,  ..., -0.3161,  0.8075, -2.7663],\n",
       "         ...,\n",
       "         [-0.4381,  2.5207, -0.7904,  ...,  1.0805,  1.9877, -3.9398],\n",
       "         [ 0.2506,  2.5132, -0.9123,  ...,  0.3466,  2.8406, -4.7041],\n",
       "         [-0.9340,  2.4373, -0.2003,  ..., -2.0934,  2.4075, -5.7180]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-1.1685, -0.4912, -1.3312,  ..., -1.5129, -2.0163, -0.5976],\n",
       "         [ 1.1335,  1.9771,  0.4724,  ..., -2.5017,  1.7339, -1.7357],\n",
       "         [-0.0474,  2.0293,  1.0217,  ..., -1.9734,  1.1254, -2.5960],\n",
       "         ...,\n",
       "         [ 0.3051,  1.6259, -0.8599,  ...,  0.4260,  3.0206, -2.6297],\n",
       "         [ 0.2728,  2.4435, -0.0953,  ..., -0.8083,  2.6860, -3.1276],\n",
       "         [-0.5686,  2.6536,  1.2584,  ..., -2.7115,  3.3662, -4.4926]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-0.5140, -0.6983, -1.5340,  ..., -1.7549, -2.8290, -0.7964],\n",
       "         [ 1.6464,  1.1990, -0.2683,  ..., -1.9245,  1.3940,  0.6899],\n",
       "         [ 0.7737,  1.0775,  0.0833,  ..., -0.6061,  1.7000,  0.8381],\n",
       "         ...,\n",
       "         [ 0.4487,  1.3127, -0.6607,  ...,  0.2428,  3.2458, -0.9033],\n",
       "         [ 0.4967,  1.4425, -0.9581,  ..., -0.1039,  3.0748, -1.5839],\n",
       "         [-0.4397,  1.6098,  0.6942,  ..., -1.4329,  3.9631, -2.5964]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[-0.1976, -0.6608, -1.1343,  ..., -1.7295, -2.6185, -0.8010],\n",
       "         [ 0.7162,  2.6044,  0.8336,  ..., -2.0067,  1.9470,  0.4423],\n",
       "         [ 0.3043,  0.8395,  0.5694,  ..., -1.2838,  1.9416,  0.4362],\n",
       "         ...,\n",
       "         [-0.2426,  0.5387, -0.6731,  ...,  0.1151,  3.5290, -0.4571],\n",
       "         [-0.4437,  1.8318, -0.5789,  ..., -0.4804,  3.4456, -0.9438],\n",
       "         [-1.1308,  2.1756,  0.1669,  ..., -0.8936,  4.6887, -1.8094]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 0.2532, -1.1113, -1.5334,  ..., -1.6843, -2.7075, -0.7832],\n",
       "         [-1.0966,  1.6897,  0.4956,  ..., -0.2258,  2.2369, -0.6098],\n",
       "         [-0.8735, -0.4397,  0.6467,  ..., -0.4006,  1.8309, -0.3495],\n",
       "         ...,\n",
       "         [-0.8501,  0.1590, -0.5801,  ...,  0.4554,  4.3038, -0.0767],\n",
       "         [-0.8393,  0.7544, -1.2609,  ...,  0.7398,  2.2978, -0.2451],\n",
       "         [-0.6061, -0.2115, -1.0183,  ..., -0.2893,  3.2665, -1.0743]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 0.9018, -1.3797, -0.6686,  ..., -2.1363, -2.1963, -0.5122],\n",
       "         [-1.8541,  0.8188,  3.7359,  ...,  1.1323,  2.2382,  0.5651],\n",
       "         [-3.0059, -1.1863,  2.9868,  ...,  1.2030,  0.1342,  0.8282],\n",
       "         ...,\n",
       "         [-1.0896, -0.6526,  1.2889,  ...,  1.1651,  3.7784,  1.1872],\n",
       "         [-1.7410, -0.8175,  0.4806,  ...,  1.8485,  2.8602,  0.5383],\n",
       "         [-0.7905, -2.2642, -0.3578,  ...,  1.0525,  2.4466,  0.1072]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 1.4124, -0.4791,  0.0480,  ..., -2.5246, -3.1832, -0.8853],\n",
       "         [-2.9368,  1.3741,  8.9134,  ...,  2.3798,  5.0741, -0.6617],\n",
       "         [-4.6047, -0.0397,  6.2723,  ...,  1.8187,  2.0917, -1.0256],\n",
       "         ...,\n",
       "         [-2.9491, -0.6105,  1.5836,  ...,  1.8301,  5.2807,  0.3466],\n",
       "         [-2.0328, -0.1687,  2.3245,  ...,  3.7218,  2.6245, -0.8157],\n",
       "         [-2.2364,  0.2472,  0.5054,  ...,  2.4849,  3.0477, -1.2961]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 0.5117,  0.7665,  1.4652,  ..., -3.4655, -4.6141, -0.9444],\n",
       "         [-5.2808,  7.1169, 18.5072,  ...,  0.1161,  9.4307,  2.1374],\n",
       "         [-2.6534,  2.8153, 11.0677,  ...,  0.9530,  5.0181, -0.0640],\n",
       "         ...,\n",
       "         [-3.9073, -0.7957,  3.4582,  ...,  1.4311,  4.4995, -1.1074],\n",
       "         [-2.9001,  1.3238,  4.7782,  ...,  2.6614,  3.3344, -0.2597],\n",
       "         [-4.7881,  2.2973,  6.1972,  ..., -4.9104,  6.1232,  1.2157]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>), tensor([[[ 2.3126,  5.5116, 11.7884,  ..., -9.5746, -5.9893,  1.3186],\n",
       "         [-3.2050, 11.7367, 27.7955,  ..., -6.0934,  6.4021, 10.2500],\n",
       "         [ 2.4797,  7.6280, 21.8379,  ..., -6.5983,  2.8061,  5.1101],\n",
       "         ...,\n",
       "         [-1.5127,  4.1776,  8.8029,  ..., -2.9645,  4.4368,  3.0759],\n",
       "         [ 0.2478,  5.6034, 11.9160,  ..., -0.5723,  2.0423,  3.3845],\n",
       "         [-2.7351,  5.8127, 14.5410,  ..., -8.4604,  3.0837,  8.8164]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)), attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 28, 28])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_feature_map['cnn_output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x38b19fad0>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "idx = 1\n",
    "plt.imshow(output_feature_map['cnn_output'][0, idx].detach().cpu().numpy())\n",
    "# Add a color bar\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x38b61b350>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(output_feature_map['cnn_output'][0].sum(0).detach().cpu().numpy())\n",
    "# Add a color bar\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " resnet18(weights=ResNet18_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m196\u001b[39m, \u001b[38;5;241m768\u001b[39m)\n\u001b[1;32m      2\u001b[0m A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": [
    "A = torch.randn(1, 196, 768)\n",
    "A = A.to(device)\n",
    "torch.linalg.pinv(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpinv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn't support float64. Please use float32 instead."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
